{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":10654057,"sourceType":"datasetVersion","datasetId":6597483},{"sourceId":10699180,"sourceType":"datasetVersion","datasetId":6630257},{"sourceId":10700119,"sourceType":"datasetVersion","datasetId":6630845},{"sourceId":10700156,"sourceType":"datasetVersion","datasetId":6630878},{"sourceId":10700193,"sourceType":"datasetVersion","datasetId":6630913}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required module\nimport os\nimport subprocess\n\n\ntry:\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from PIL import Image\n    import glob\n    from tqdm import tqdm\n    import seaborn as sns\n    import torch\n    import torchvision.models as models\n    import torchvision.transforms as transforms\n    from sklearn.metrics.pairwise import cosine_similarity\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch_geometric.nn import GCNConv\n    from scipy.stats import rankdata\n    from torch.utils.data import Dataset\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from collections import defaultdict\n    import random\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.cluster import KMeans\n    from torch_geometric.utils import add_self_loops\n    from sklearn.metrics import roc_auc_score\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.metrics import precision_score, recall_score, f1_score, ndcg_score\n    from itertools import chain\n    from sklearn.manifold import TSNE\n    import networkx as nx\n    from torchvision import models, transforms\n    import torch\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import gc\n    import glob\n    import pickle\n    import zipfile \nexcept ImportError:\n    print(\"Installing dependencies...\")\n\n    # Install system-level dependencies\n    subprocess.run([\"apt-get\", \"update\"])\n    subprocess.run([\"apt-get\", \"install\", \"-y\", \"libcairo2\", \"libcairo2-dev\"])\n\n    # Install Python packages\n    subprocess.run([\"pip\", \"install\", \"--quiet\", \"pycairo\"])\n    subprocess.run([\"pip\", \"install\", \"--quiet\", \"torch torchvision torchaudio\"])\n    subprocess.run([\"pip\", \"install\", \"--quiet\", \"torch-geometric\", \"torch-scatter\", \"torch-sparse\", \"torch-cluster\", \"torch-spline-conv\"])\n    subprocess.run([\"pip\", \"install\", \"--quiet\", \"pandas\", \"numpy\", \"matplotlib\", \"pillow\", \"tqdm\", \"seaborn\", \"scipy\"])\n    subprocess.run([\"pip\", \"install\", \"--quiet\", \"scikit-learn\"])\n    subprocess.run([\"pip\", \"install\", \"--quiet\", \"networkx\"])\n\n    print(\"All dependencies installed.\")\n\n\nprint(\"✅ All required dependencies are installed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T03:02:42.640070Z","iopub.execute_input":"2025-02-09T03:02:42.640444Z","iopub.status.idle":"2025-02-09T03:02:42.651548Z","shell.execute_reply.started":"2025-02-09T03:02:42.640413Z","shell.execute_reply":"2025-02-09T03:02:42.650627Z"}},"outputs":[{"name":"stdout","text":"✅ All required dependencies are installed!\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Check if TPU is available\ndevice = xm.xla_device()\nprint(f\"✅ Using device: {device}\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T03:02:46.516781Z","iopub.execute_input":"2025-02-09T03:02:46.517241Z","iopub.status.idle":"2025-02-09T03:02:46.522653Z","shell.execute_reply.started":"2025-02-09T03:02:46.517198Z","shell.execute_reply":"2025-02-09T03:02:46.521660Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: xla:0\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Adjust article ID format\ndef adjust_id(x):\n    x = str(x)\n    return \"0\" + x if len(x) == 9 else x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T03:02:49.577515Z","iopub.execute_input":"2025-02-09T03:02:49.577831Z","iopub.status.idle":"2025-02-09T03:02:49.582125Z","shell.execute_reply.started":"2025-02-09T03:02:49.577804Z","shell.execute_reply":"2025-02-09T03:02:49.581096Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nPREPROCESSED_DIR = \"/kaggle/input/preprocessed-data\"\n\n# # ✅ Define file paths for saving preprocessed data\n# PREPROCESSED_DIR = \"/kaggle/working/preprocessed-data\"\n# ZIP_FILE = \"/kaggle/working/preprocessed-data.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T03:06:19.572198Z","iopub.execute_input":"2025-02-09T03:06:19.572606Z","iopub.status.idle":"2025-02-09T03:06:19.576521Z","shell.execute_reply.started":"2025-02-09T03:06:19.572567Z","shell.execute_reply":"2025-02-09T03:06:19.575507Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(PREPROCESSED_DIR, \"articles_with_images.pkl\")):\n    print(\"✅ Preprocessed data already exists. Loading...\")\n\n    with open(os.path.join(PREPROCESSED_DIR, \"articles_with_images.pkl\"), \"rb\") as f:\n        articles_with_images = pickle.load(f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"transactions_filtered.pkl\"), \"rb\") as f:\n        transactions_filtered = pickle.load(f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"customers_processed.pkl\"), \"rb\") as f:\n        customers = pickle.load(f)\n\n    print(\"✅ Preprocessed data loaded successfully!\")\n\nelse :\n    print(\"⚡ Running preprocessing for the first time...\")\n    articles = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv')\n    customers = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/customers.csv')\n    transactions = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\n\n    # Get all paths from the image folder\n    all_image_paths = glob.glob(\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/*/*\")\n    \n    # Adjust the article ID and product code to be string & add \"0\"\n    articles[\"article_id\"] = articles[\"article_id\"].apply(lambda x: adjust_id(x))\n    articles[\"product_code\"] = articles[\"article_id\"].apply(lambda x: x[:3])\n    \n    # Get all valid article IDs and create a set to store image IDs\n    all_image_ids = set()\n    \n    for path in tqdm(all_image_paths, desc=\"Processing Images\"):\n        article_id = os.path.basename(path).split('.')[0]  # Extract image ID from filename\n        all_image_ids.add(article_id)\n    \n    \n    # Construct full image paths and filter invalid ones\n    images_path = \"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/\"\n    articles[\"path\"] = articles[\"article_id\"].apply(\n        lambda x: images_path + x[:3] + \"/\" + x + \".jpg\" if x in all_image_ids else None\n    )\n    \n    # ✅ Keep only articles with valid images\n    articles_with_images = articles.dropna(subset=[\"path\"]).reset_index(drop=True)\n\n    # Adjust the article ID and product code to be string & add \"0\"\n\n    articles_with_images[\"article_id\"] = articles_with_images[\"article_id\"].astype(\"category\")\n    articles_with_images[\"product_code\"] = articles_with_images[\"product_code\"].astype(\"category\")\n\n    # Fill missing values\n    customers.fillna({\n        \"FN\": 0,\n        \"Active\": 0,\n        \"club_member_status\": \"UNKNOWN\",\n        \"fashion_news_frequency\": \"UNKNOWN\",\n        \"age\": customers[\"age\"].median()\n    }, inplace=True)\n    \n    customers[\"customer_id\"] = customers[\"customer_id\"].astype(\"category\")\n    customers[\"Active\"] = customers[\"Active\"].astype(np.int8)\n    customers[\"FN\"] = customers[\"FN\"].astype(np.int8)\n    customers[\"age\"] = customers[\"age\"].astype(np.float16)\n\n    if 'age' in customers.columns:\n        scaler_age = MinMaxScaler()\n        customers['normalized_age'] = scaler_age.fit_transform(customers[['age']])\n    else:\n        raise ValueError(\"Error: `age` column is missing in customers!\")\n    \n    # customers[\"fashion_news_frequency\"] = customers[\"fashion_news_frequency\"].replace({\"None\": \"NONE\"})\n    # customers[\"age_interval\"] = customers[\"age\"].apply(create_age_interval)\n\n    # Adjust article_id (as did for articles dataframe)\n    transactions[\"article_id\"] = transactions[\"article_id\"].apply(lambda x: adjust_id(x))\n    \n    # Filter the transactions dataset to keep only valid article IDs\n    transactions_filtered = transactions[transactions[\"article_id\"].isin(set(articles_with_images[\"article_id\"]))].reset_index(drop=True)\n    \n    # Optionally save the filtered transactions dataset\n    # transactions_filtered.to_csv(\"transactions_filtered.csv\", index=False)\n    transactions_filtered[\"article_id\"] = transactions_filtered[\"article_id\"].astype(\"category\")\n    transactions_filtered[\"price\"] = transactions_filtered[\"price\"].astype(np.float16)\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(PREPROCESSED_DIR, \"articles_with_images.pkl\"), \"wb\") as f:\n        pickle.dump(articles_with_images, f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"transactions_filtered.pkl\"), \"wb\") as f:\n        pickle.dump(transactions_filtered, f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"customers_processed.pkl\"), \"wb\") as f:\n        pickle.dump(customers, f)\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(PREPROCESSED_DIR):\n            zipf.write(os.path.join(PREPROCESSED_DIR, file), arcname=file)\n\n    print(f\"✅ Preprocessing completed! Saved as {ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T03:07:30.212474Z","iopub.execute_input":"2025-02-09T03:07:30.212859Z","iopub.status.idle":"2025-02-09T03:07:41.507722Z","shell.execute_reply.started":"2025-02-09T03:07:30.212827Z","shell.execute_reply":"2025-02-09T03:07:41.506679Z"}},"outputs":[{"name":"stdout","text":"✅ Preprocessed data already exists. Loading...\n✅ Preprocessed data loaded successfully!\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()  # If using GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:16:48.742783Z","iopub.execute_input":"2025-02-09T05:16:48.743129Z","iopub.status.idle":"2025-02-09T05:16:49.001794Z","shell.execute_reply.started":"2025-02-09T05:16:48.743101Z","shell.execute_reply":"2025-02-09T05:16:49.000361Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Set random seed for reproducibility\nnp.random.seed(42)\nrandom.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:16:51.679041Z","iopub.execute_input":"2025-02-09T05:16:51.679376Z","iopub.status.idle":"2025-02-09T05:16:51.684113Z","shell.execute_reply.started":"2025-02-09T05:16:51.679348Z","shell.execute_reply":"2025-02-09T05:16:51.682424Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"creating embeddings","metadata":{}},{"cell_type":"code","source":"transactions_filtered.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:16:54.276811Z","iopub.execute_input":"2025-02-09T05:16:54.277118Z","iopub.status.idle":"2025-02-09T05:16:54.302098Z","shell.execute_reply.started":"2025-02-09T05:16:54.277091Z","shell.execute_reply":"2025-02-09T05:16:54.301278Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n  has_large_values = (abs_vals > 1e6).any()\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"        t_dat                                        customer_id  article_id  \\\n0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n2  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n3  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n4  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n\n      price  sales_channel_id  \n0  0.050842                 2  \n1  0.030487                 2  \n2  0.015236                 2  \n3  0.016937                 2  \n4  0.016937                 2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-09-20</td>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>0663713001</td>\n      <td>0.050842</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-09-20</td>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>0541518023</td>\n      <td>0.030487</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-09-20</td>\n      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n      <td>0505221004</td>\n      <td>0.015236</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-09-20</td>\n      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n      <td>0685687003</td>\n      <td>0.016937</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-09-20</td>\n      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n      <td>0685687004</td>\n      <td>0.016937</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"# # ✅ Define file paths for saving preprocessed data\n# IMAGE_EMBEDDINGS_DIR = \"/kaggle/working/image_embeddings\"\n# IMAGE_EMBEDDINGS_ZIP_FILE = \"/kaggle/working/image_embeddings.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(IMAGE_EMBEDDINGS_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nIMAGE_EMBEDDINGS_DIR = \"/kaggle/input/image-embeddings\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:17:43.664892Z","iopub.execute_input":"2025-02-09T05:17:43.665226Z","iopub.status.idle":"2025-02-09T05:17:43.669618Z","shell.execute_reply.started":"2025-02-09T05:17:43.665198Z","shell.execute_reply":"2025-02-09T05:17:43.668280Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(IMAGE_EMBEDDINGS_DIR, \"image_embeddings.pkl\")):\n    print(\"✅ image_embeddings data already exists. Loading...\")\n\n    with open(os.path.join(IMAGE_EMBEDDINGS_DIR, \"image_embeddings.pkl\"), \"rb\") as f:\n        image_embeddings = pickle.load(f)\n\n    print(\"✅ image embeddings data loaded successfully!\")\n\nelse:\n    print(\"⚡ Extracting image embeddings for the first time...\")\n\n    # ✅ Load pre-trained ResNet50 model (Feature Extraction)\n    resnet_model = models.resnet50(pretrained=True)\n    resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])  # Remove the last FC layer\n    resnet_model = resnet_model.to(device).eval()  # Move to device & set to eval mode\n\n    # ✅ Define image transformations\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  \n        transforms.ToTensor(),          \n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n    ])\n\n    # ✅ Function to extract image embeddings\n    def get_image_embeddings_batch(image_paths_batch):\n        images = [Image.open(path).convert('RGB') for path in image_paths_batch]\n        images = [transform(image).unsqueeze(0) for image in images]\n        images_tensor = torch.cat(images, dim=0).to(device)\n\n        with torch.no_grad():\n            features = resnet_model(images_tensor).squeeze(-1).squeeze(-1)  # Remove extra dimensions\n        \n        return features.cpu().numpy()\n\n    # ✅ Process images in batches to avoid memory issues\n    batch_size = 544  # Adjust batch size based on available memory\n    image_embeddings = []\n\n    for i in tqdm(range(0, len(articles_with_images), batch_size), desc=\"Extracting Features\"):\n        image_paths_batch = articles_with_images['path'][i:i + batch_size]\n        embeddings_batch = get_image_embeddings_batch(image_paths_batch)\n        image_embeddings.extend(embeddings_batch)\n\n    # ✅ Convert to efficient tensor format\n    image_embeddings = torch.tensor(image_embeddings, dtype=dtype).to(device)\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(IMAGE_EMBEDDINGS_DIR, \"image_embeddings.pkl\"), \"wb\") as f:\n        pickle.dump(image_embeddings, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(IMAGE_EMBEDDINGS_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(IMAGE_EMBEDDINGS_DIR):\n            zipf.write(os.path.join(IMAGE_EMBEDDINGS_DIR, file), arcname=file)\n\n    print(f\"✅ image_embeddings completed! Saved as {IMAGE_EMBEDDINGS_ZIP_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:17:46.045546Z","iopub.execute_input":"2025-02-09T05:17:46.045860Z","iopub.status.idle":"2025-02-09T05:17:49.406145Z","shell.execute_reply.started":"2025-02-09T05:17:46.045835Z","shell.execute_reply":"2025-02-09T05:17:49.404872Z"}},"outputs":[{"name":"stdout","text":"✅ image_embeddings data already exists. Loading...\n✅ image embeddings data loaded successfully!\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# # ✅ Define file paths for saving preprocessed data\n# TEXT_EMBEDDINGS_DIR = \"/kaggle/working/text_embeddings\"\n# TEXT_EMBEDDINGS_ZIP_FILE = \"/kaggle/working/text_embeddings.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(TEXT_EMBEDDINGS_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nTEXT_EMBEDDINGS_DIR = \"/kaggle/input/text-embeddings\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:21:33.817718Z","iopub.execute_input":"2025-02-09T05:21:33.818098Z","iopub.status.idle":"2025-02-09T05:21:33.822209Z","shell.execute_reply.started":"2025-02-09T05:21:33.818067Z","shell.execute_reply":"2025-02-09T05:21:33.821049Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(TEXT_EMBEDDINGS_DIR, \"text_embeddings.pkl\")):\n    print(\"✅ text_embeddings data already exists. Loading...\")\n\n    with open(os.path.join(TEXT_EMBEDDINGS_DIR, \"text_embeddings.pkl\"), \"rb\") as f:\n        text_embeddings = pickle.load(f)\n\n    print(\"✅ text embeddings data loaded successfull\")\n\nelse :\n\n    # Define text features\n    if all(col in articles_with_images.columns for col in ['detail_desc', 'prod_name', 'product_type_name', 'product_group_name',\n                                               'graphical_appearance_name', 'colour_group_name', 'index_name',\n                                               'index_group_name', 'section_name', 'garment_group_name']):\n        articles_with_images['text_data'] = (\n            articles_with_images['detail_desc'].fillna('') + ' ' +\n            articles_with_images['prod_name'].fillna('') + ' ' +\n            articles_with_images['product_type_name'].fillna('') + ' ' +\n            articles_with_images['product_group_name'].fillna('') + ' ' +\n            articles_with_images['graphical_appearance_name'].fillna('') + ' ' +\n            articles_with_images['colour_group_name'].fillna('') + ' ' +\n            articles_with_images['index_name'].fillna('') + ' ' +\n            articles_with_images['index_group_name'].fillna('') + ' ' +\n            articles_with_images['section_name'].fillna('') + ' ' +\n            articles_with_images['garment_group_name'].fillna('')\n        )\n    else:\n        raise ValueError(\"Error: One or more textual columns are missing!\")\n\n    \n    # Load pre-trained GloVe embeddings (for example, GloVe 100D embeddings)\n    def load_glove_embeddings(glove_file_path):\n        embeddings_index = {}\n        with open(glove_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                vector = np.asarray(values[1:], dtype='float32')\n                embeddings_index[word] = vector\n        return embeddings_index\n    \n    # Example usage (change the path to your GloVe file)\n    glove_file_path = '/kaggle/input/test-feature-1/glove.6B.100d.txt'\n    embeddings_index = load_glove_embeddings(glove_file_path)\n    \n    # Checking a sample word embedding\n    # print(embeddings_index['king'])  # Example of checking the embedding for 'king'\n\n    # Function to convert text data to embeddings using GloVe\n    def text_to_glove_embeddings(text_data, embeddings_index, embedding_dim=100):\n        embeddings = []\n        for text in text_data:\n            words = text.split()  # Split the text into words\n            word_embeddings = np.zeros(embedding_dim)\n            valid_word_count = 0\n            \n            # For each word, get its GloVe embedding (if it exists)\n            for word in words:\n                if word in embeddings_index:\n                    word_embeddings += embeddings_index[word]\n                    valid_word_count += 1\n            \n            # Average the embeddings of the words in the text\n            if valid_word_count > 0:\n                word_embeddings /= valid_word_count\n            embeddings.append(word_embeddings)\n        \n        return np.array(embeddings)\n    \n    # Convert the product descriptions to GloVe embeddings\n    text_embeddings = text_to_glove_embeddings(articles_with_images['text_data'], embeddings_index)\n\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(TEXT_EMBEDDINGS_DIR, \"text_embeddings.pkl\"), \"wb\") as f:\n        pickle.dump(text_embeddings, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(TEXT_EMBEDDINGS_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(TEXT_EMBEDDINGS_DIR):\n            zipf.write(os.path.join(TEXT_EMBEDDINGS_DIR, file), arcname=file)\n\n    print(f\"✅ text_embeddings completed! Saved as {TEXT_EMBEDDINGS_ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:22:39.900247Z","iopub.execute_input":"2025-02-09T05:22:39.900598Z","iopub.status.idle":"2025-02-09T05:22:40.414913Z","shell.execute_reply.started":"2025-02-09T05:22:39.900572Z","shell.execute_reply":"2025-02-09T05:22:40.413728Z"}},"outputs":[{"name":"stdout","text":"✅ text_embeddings data already exists. Loading...\n✅ text embeddings data loaded successfull\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# # ✅ Define file paths for saving preprocessed data\n# CUSTOMER_EMBEDDINGS_DIR = \"/kaggle/working/customer_embeddings\"\n# CUSTOMER_EMBEDDINGS_ZIP_FILE = \"/kaggle/working/customer_embeddings.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(CUSTOMER_EMBEDDINGS_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nCUSTOMER_EMBEDDINGS_DIR = \"/kaggle/input/customer-embeddings\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:17.277906Z","iopub.execute_input":"2025-02-09T05:28:17.278427Z","iopub.status.idle":"2025-02-09T05:28:17.283967Z","shell.execute_reply.started":"2025-02-09T05:28:17.278362Z","shell.execute_reply":"2025-02-09T05:28:17.282347Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(CUSTOMER_EMBEDDINGS_DIR, \"customer_embeddings.pkl\")):\n    print(\"✅ customer_embeddings data already exists. Loading...\")\n\n    with open(os.path.join(CUSTOMER_EMBEDDINGS_DIR, \"customer_embeddings.pkl\"), \"rb\") as f:\n        customer_embeddings = pickle.load(f)\n\n    print(\"✅ customer_embeddings data loaded successfull\")\n\nelse :\n\n    # Step 2: One-Hot Encode Categorical Features and Convert to float32\n    categorical_features_encoded = pd.get_dummies(customers[['Active', 'club_member_status']]).astype(np.float32)\n    \n    # Step 3: Combine Both Categorical and Numerical Features (Ensure float32)\n    customer_features = np.hstack([\n        customers['normalized_age'].values.reshape(-1, 1).astype(np.float32),  \n        categorical_features_encoded.astype(np.float32)\n    ])\n    \n    # Convert to Tensor and Move to GPU\n    customer_features_tensor = torch.tensor(customer_features, dtype=torch.float32).to(device)\n    \n    # Step 4: Use a Neural Network to Create the Customer Embedding\n    class CustomerEmbedding(nn.Module):\n        def __init__(self, input_dim, embedding_dim):\n            super(CustomerEmbedding, self).__init__()\n            self.fc1 = nn.Linear(input_dim, 128)  # Hidden layer\n            self.fc2 = nn.Linear(128, embedding_dim)  # Output layer: Customer embedding\n    \n        def forward(self, x):\n            x = torch.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    \n    # Define the model\n    embedding_dim = 64  # Size of the customer embedding\n    model = CustomerEmbedding(input_dim=customer_features.shape[1], embedding_dim=embedding_dim).to(device)  # Move the model to GPU\n    \n    # Convert the customer features into a tensor and move it to GPU\n    customer_features_tensor = torch.tensor(customer_features, dtype=torch.float32).to(device)\n    \n    # Get customer embeddings by passing through the model\n    customer_embeddings = model(customer_features_tensor)\n    \n    # Move the embeddings back to CPU if needed (optional)\n    customer_embeddings = customer_embeddings.cpu().detach().numpy()\n    customer_embeddings = torch.tensor(customer_embeddings, dtype=torch.bfloat16).to(device)\n    \n    \n     # ✅ Save preprocessed data locally\n    with open(os.path.join(CUSTOMER_EMBEDDINGS_DIR, \"customer_embeddings.pkl\"), \"wb\") as f:\n        pickle.dump(customer_embeddings, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(CUSTOMER_EMBEDDINGS_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(CUSTOMER_EMBEDDINGS_DIR):\n            zipf.write(os.path.join(CUSTOMER_EMBEDDINGS_DIR, file), arcname=file)\n\n    print(f\"✅ customer_embeddings completed! Saved as {CUSTOMER_EMBEDDINGS_ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:20.072953Z","iopub.execute_input":"2025-02-09T05:28:20.073260Z","iopub.status.idle":"2025-02-09T05:28:21.368958Z","shell.execute_reply.started":"2025-02-09T05:28:20.073235Z","shell.execute_reply":"2025-02-09T05:28:21.367821Z"}},"outputs":[{"name":"stdout","text":"✅ customer_embeddings data already exists. Loading...\n✅ customer_embeddings data loaded successfull\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"edge_types = [\"product_to_product\", \"customer_to_product\", \"customer_to_customer\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:28.118038Z","iopub.execute_input":"2025-02-09T05:28:28.118349Z","iopub.status.idle":"2025-02-09T05:28:28.121886Z","shell.execute_reply.started":"2025-02-09T05:28:28.118323Z","shell.execute_reply":"2025-02-09T05:28:28.120945Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"def sliced_wasserstein_distance(P, Q, num_projections=50):\n    \"\"\"\n    Computes the Sliced Wasserstein Distance (SWD) between two trajectory distributions P and Q.\n    Uses random projections to compute the distance between the two distributions.\n    \"\"\"\n    device = P.device\n    proj_vectors = torch.randn((P.shape[1], num_projections), device=device)  # Random projection vectors\n    proj_vectors = proj_vectors / torch.norm(proj_vectors, dim=0, keepdim=True)  # Normalize\n\n    P_proj = P @ proj_vectors  # Project P onto vectors\n    Q_proj = Q @ proj_vectors  # Project Q onto vectors\n\n    P_sorted, _ = torch.sort(P_proj, dim=0)\n    Q_sorted, _ = torch.sort(Q_proj, dim=0)\n\n    return torch.mean(torch.abs(P_sorted - Q_sorted))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:30.702685Z","iopub.execute_input":"2025-02-09T05:28:30.703027Z","iopub.status.idle":"2025-02-09T05:28:30.708184Z","shell.execute_reply.started":"2025-02-09T05:28:30.702997Z","shell.execute_reply":"2025-02-09T05:28:30.707176Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"data splitting","metadata":{}},{"cell_type":"code","source":"### Step 6: Geometric Distributed Sampling (GDS)\ndef geometric_distributed_sampling(ranks, rho=0.5, max_samples=300000):\n    \"\"\"\n    Optimized Geometric Distributed Sampling (GDS).\n    Ensures probabilities remain valid by normalizing ranks.\n    \"\"\"\n    sorted_indices = torch.argsort(ranks, descending=True)\n    max_categories = min(len(ranks), 16_000_000)  \n    filtered_indices = sorted_indices[:max_categories] \n\n    # Normalize ranks\n    ranks_subset = ranks[filtered_indices]\n    normalized_ranks = (ranks_subset - ranks_subset.min()) / (ranks_subset.max() - ranks_subset.min() + 1e-8)\n\n    # Compute probabilities\n    probabilities = torch.exp(-rho * normalized_ranks)\n    probabilities /= probabilities.sum()  # Normalize to sum = 1\n\n    # Sample indices\n    sampled_relative_indices = torch.multinomial(probabilities, num_samples=min(max_samples, len(filtered_indices)), replacement=False)\n\n    # Map back to original transaction indices\n    sampled_indices = filtered_indices[sampled_relative_indices]\n\n    return sampled_indices.cpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:34.663285Z","iopub.execute_input":"2025-02-09T05:28:34.663657Z","iopub.status.idle":"2025-02-09T05:28:34.669258Z","shell.execute_reply.started":"2025-02-09T05:28:34.663612Z","shell.execute_reply":"2025-02-09T05:28:34.668442Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# # ✅ Define file paths for saving preprocessed data\n# DATA_SPLITTING_DIR = \"/kaggle/working/data_splitting\"\n# DATA_SPLITTING_ZIP_FILE = \"/kaggle/working/data_splitting.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(DATA_SPLITTING_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nDATA_SPLITTING_DIR = \"/kaggle/input/data-splitting\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:38.470530Z","iopub.execute_input":"2025-02-09T05:28:38.470884Z","iopub.status.idle":"2025-02-09T05:28:38.475705Z","shell.execute_reply.started":"2025-02-09T05:28:38.470854Z","shell.execute_reply":"2025-02-09T05:28:38.474309Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\")):\n    print(\"✅ train_data data already exists. Loading...\")\n\n    # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"), \"rb\") as f:\n        train_data = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_data.pkl\"), \"rb\") as f:\n        val_data = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_data.pkl\"), \"rb\") as f:\n        test_data = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"), \"rb\") as f:\n        train_customers = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_customers.pkl\"), \"rb\") as f:\n        val_customers = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_customers.pkl\"), \"rb\") as f:\n        test_customers = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"), \"rb\") as f:\n        train_articles = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_articles.pkl\"), \"rb\") as f:\n        val_articles = pickle.load(f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_articles.pkl\"), \"rb\") as f:\n        test_articles = pickle.load(f)\n\n    print(\"✅ SPLIT data loaded successfully!\")\n\nelse :\n\n    print(\"⚡ Extracting SPLIT for the first time...\")\n\n    ### Step 3: Convert transaction date to datetime\n    transactions_filtered['t_dat'] = pd.to_datetime(transactions_filtered['t_dat'])\n    \n    # Sort transactions by date\n    transactions_filtered = transactions_filtered.sort_values('t_dat', ascending=True)\n    \n    ### Step 4: Compute train, validation, and test cutoffs\n    cutoffs = transactions_filtered['t_dat'].quantile([0.75, 0.875, 1.0]).values\n    train_cutoff, val_cutoff, test_cutoff = cutoffs  # 75% Train, 12.5% Val, 12.5% Test\n    \n    ### Step 5: Assign Ranks Efficiently for GDS\n    transactions_filtered[\"rank\"] = transactions_filtered['t_dat'].rank(method=\"first\", ascending=True)\n    \n    # Convert ranks to a tensor\n    ranks_tensor = torch.tensor(transactions_filtered[\"rank\"].to_numpy(dtype=np.float32), device=device)\n    \n    # Get sampled transactions\n    sampled_indices = geometric_distributed_sampling(ranks_tensor, rho=0.5, max_samples=300000)\n    transactions_filtered_sampled = transactions_filtered.iloc[sampled_indices.numpy()].reset_index(drop=True)\n\n    ### Step 7: Train/Validation/Test Splitting\n    train_mask = transactions_filtered_sampled['t_dat'] <= train_cutoff\n    val_mask = (transactions_filtered_sampled['t_dat'] > train_cutoff) & (transactions_filtered_sampled['t_dat'] <= val_cutoff)\n    test_mask = (transactions_filtered_sampled['t_dat'] > val_cutoff) & (transactions_filtered_sampled['t_dat'] <= test_cutoff)\n    \n    train_data = transactions_filtered_sampled[train_mask]\n    val_data = transactions_filtered_sampled[val_mask]\n    test_data = transactions_filtered_sampled[test_mask]\n    \n    ### Step 8: Cold Start Customer Handling\n    transaction_customers = set(transactions_filtered['customer_id'])  \n    cold_start_customers = customers[~customers['customer_id'].isin(transaction_customers)]\n    \n    # Split cold start customers into train, validation, and test\n    train_cold_start, temp_cold_start = train_test_split(cold_start_customers, test_size=0.25, random_state=42)\n    val_cold_start, test_cold_start = train_test_split(temp_cold_start, test_size=0.5, random_state=42)\n    \n    # Merge cold-start data with train/val/test datasets\n    train_data = pd.concat([train_data, train_cold_start])\n    val_data = pd.concat([val_data, val_cold_start])\n    test_data = pd.concat([test_data, test_cold_start])\n    \n    # Filter customers to only include those appearing in train/val/test sets\n    train_customers = customers[customers['customer_id'].isin(train_data['customer_id'])]\n    val_customers = customers[customers['customer_id'].isin(val_data['customer_id'])]\n    test_customers = customers[customers['customer_id'].isin(test_data['customer_id'])]\n    \n    # Filter articles to only include those appearing in train/val/test sets\n    train_articles = articles_with_images[articles_with_images['article_id'].isin(train_data['article_id'])]\n    val_articles = articles_with_images[articles_with_images['article_id'].isin(val_data['article_id'])]\n    test_articles = articles_with_images[articles_with_images['article_id'].isin(test_data['article_id'])]\n\n\n\n        # ✅ Save preprocessed data locally\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"), \"wb\") as f:\n        pickle.dump(train_data, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_data.pkl\"), \"wb\") as f:\n        pickle.dump(val_data, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_data.pkl\"), \"wb\") as f:\n        pickle.dump(test_data, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"), \"wb\") as f:\n        pickle.dump(train_customers, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_customers.pkl\"), \"wb\") as f:\n        pickle.dump(val_customers, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_customers.pkl\"), \"wb\") as f:\n        pickle.dump(test_customers, f)\n\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"), \"wb\") as f:\n        pickle.dump(train_articles, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_articles.pkl\"), \"wb\") as f:\n        pickle.dump(val_articles, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_articles.pkl\"), \"wb\") as f:\n        pickle.dump(test_articles, f)\n    \n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(DATA_SPLITTING_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(DATA_SPLITTING_DIR):\n            zipf.write(os.path.join(DATA_SPLITTING_DIR, file), arcname=file)\n\n    print(f\"✅ data splitting completed! Saved as {DATA_SPLITTING_ZIP_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:28:52.728610Z","iopub.execute_input":"2025-02-09T05:28:52.728955Z","iopub.status.idle":"2025-02-09T05:29:14.567899Z","shell.execute_reply.started":"2025-02-09T05:28:52.728926Z","shell.execute_reply":"2025-02-09T05:29:14.566586Z"}},"outputs":[{"name":"stdout","text":"✅ data splitting completed! Saved as /kaggle/working/data_splitting.zip\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"### Step 9: Create the final feature matrices\n\ndef filter_and_align_embeddings(data, articles, customers):\n    \"\"\" Ensures that price, text, image, and customer embeddings are aligned properly. \"\"\"\n    \n    # 1️⃣ **Filter transactions to unique article IDs**\n    valid_articles = articles[articles['article_id'].isin(data['article_id'])].reset_index(drop=True)\n\n    # 2️⃣ **Filter customers to only those appearing in `train_data`**\n    valid_customers = customers[customers['customer_id'].isin(data['customer_id'])].reset_index(drop=True)\n\n    # 3️⃣ **Extract price values matching valid articles**\n    price_values = (\n        data.drop_duplicates(subset=[\"article_id\"])  # Deduplicate articles\n        .set_index(\"article_id\")\n        .loc[valid_articles[\"article_id\"], \"price\"]\n        .values.reshape(-1, 1).astype(np.float32)\n    )\n\n    # 4️⃣ **Ensure correct embedding shapes**\n    text_embeds = np.array(text_embeddings[valid_articles.index].tolist(), dtype=np.float32)\n    image_embeds = np.array(image_embeddings[valid_articles.index].tolist(), dtype=np.float32)\n    customer_embeds = np.array(customer_embeddings[valid_customers.index].tolist(), dtype=np.float32)\n\n    # 5️⃣ **Ensure all features have the same row count**\n    min_rows = min(len(price_values), len(text_embeds), len(image_embeds), len(customer_embeds))\n    return np.hstack([\n        price_values[:min_rows],  \n        text_embeds[:min_rows],  \n        image_embeds[:min_rows],  \n        customer_embeds[:min_rows]\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:37:42.869549Z","iopub.execute_input":"2025-02-09T05:37:42.869933Z","iopub.status.idle":"2025-02-09T05:37:42.876945Z","shell.execute_reply.started":"2025-02-09T05:37:42.869898Z","shell.execute_reply":"2025-02-09T05:37:42.875723Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# # ✅ Define file paths for saving preprocessed data\n# FEATURES_DIR = \"/kaggle/working/features\"\n# FEATURES_ZIP_FILE = \"/kaggle/working/features.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(FEATURES_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nFEATURES_DIR = \"/kaggle/input/features\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:37:47.280887Z","iopub.execute_input":"2025-02-09T05:37:47.281199Z","iopub.status.idle":"2025-02-09T05:37:47.284975Z","shell.execute_reply.started":"2025-02-09T05:37:47.281173Z","shell.execute_reply":"2025-02-09T05:37:47.284177Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(FEATURES_DIR, \"train_features.pkl\")):\n    print(\"✅ features are already exists. Loading...\")\n\n    # Load extracted data\n    with open(os.path.join(FEATURES_DIR, \"train_features.pkl\"), \"rb\") as f:\n        train_features = pickle.load(f)\n\n    with open(os.path.join(FEATURES_DIR, \"val_features.pkl\"), \"rb\") as f:\n        val_features = pickle.load(f)\n\n    with open(os.path.join(FEATURES_DIR, \"test_features.pkl\"), \"rb\") as f:\n        test_features = pickle.load(f)\n\n    print(\"✅ features loaded successfully!\")\n\nelse :\n\n    print(\"⚡ Extracting features for the first time...\")\n    # ✅ Align and stack features properly\n    train_features = filter_and_align_embeddings(train_data, train_articles, train_customers)\n    val_features = filter_and_align_embeddings(val_data, val_articles, val_customers)\n    test_features = filter_and_align_embeddings(test_data, test_articles, test_customers)\n\n\n         # ✅ Save preprocessed data locally\n    with open(os.path.join(FEATURES_DIR, \"train_features.pkl\"), \"wb\") as f:\n        pickle.dump(train_features, f)\n\n    with open(os.path.join(FEATURES_DIR, \"val_features.pkl\"), \"wb\") as f:\n        pickle.dump(val_features, f)\n\n    with open(os.path.join(FEATURES_DIR, \"test_features.pkl\"), \"wb\") as f:\n        pickle.dump(test_features, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(FEATURES_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(FEATURES_DIR):\n            zipf.write(os.path.join(FEATURES_DIR, file), arcname=file)\n\n    print(f\"✅ features completed! Saved as {FEATURES_ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:37:52.626788Z","iopub.execute_input":"2025-02-09T05:37:52.627139Z","iopub.status.idle":"2025-02-09T05:38:10.498724Z","shell.execute_reply.started":"2025-02-09T05:37:52.627107Z","shell.execute_reply":"2025-02-09T05:38:10.497650Z"}},"outputs":[{"name":"stdout","text":"⚡ Extracting features for the first time...\n✅ features completed! Saved as /kaggle/working/features.zip\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# ✅ Extract ZIP only if the folder is empty\nif not os.path.exists(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\")):\n    if os.path.exists(DATA_SPLITTING_ZIP_FILE):\n        print(\"✅ Extracting SPLIT data from ZIP...\")\n        with zipfile.ZipFile(DATA_SPLITTING_ZIP_FILE, 'r') as zip_ref:\n            zip_ref.extractall(DATA_SPLITTING_DIR)\n        print(\"✅ SPLIT data loaded successfully!\")\n\n# ✅ Load existing preprocessed files\ndef load_pkl(filepath):\n    with open(filepath, \"rb\") as f:\n        return pickle.load(f)\n\ntrain_data = load_pkl(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"))\ntrain_customers = load_pkl(os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"))\ntrain_articles = load_pkl(os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"))\n\nprint(\"✅ Splitted data loaded. Updating positive indices...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Check if positive indices already exist in datasets\npositive_indices_exist = (\n    \"positive_indices\" in train_articles.columns and \n    \"positive_indices\" in train_customers.columns and\n    \"customer_to_product_positive_indices\" in train_customers.columns\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if positive_indices_exist:\n    print(\"✅ Positive indices already exist. Skipping computation!\")\nelse:\n    print(\"⚡ Positive indices not found! Generating...\")\n    \n    # ✅ Move embeddings to GPU if available\n    article_embedding_matrix = torch.tensor(np.hstack([text_embeddings, image_embeddings]), dtype=torch.float32).to(device)\n    customer_embedding_matrix = torch.tensor(customer_embeddings, dtype=torch.float32).to(device)\n    \n    # ✅ Create Index Mappings\n    article_to_index = {article_id: idx for idx, article_id in enumerate(train_articles['article_id'])}\n    customer_to_index = {customer_id: idx for idx, customer_id in enumerate(train_customers['customer_id'])}\n    \n    # ✅ Define function with GPU support\n    def get_positive_indices(idx, product_group_name, colour_group_code, entity_id, embedding_matrix, entity_type):\n        if entity_type == \"customer\":\n            # Retrieve co-purchased articles for the customer\n            purchased_articles = train_data[train_data['customer_id'] == entity_id]['article_id'].unique()\n            return [article_to_index[aid] for aid in purchased_articles if aid in article_to_index][:10]\n    \n        elif entity_type == \"product\":\n            # Find articles with the same product group and colour group\n            group_indices = train_articles[\n                (train_articles['product_group_name'] == product_group_name) &\n                (train_articles['colour_group_code'] == colour_group_code)\n            ].index.tolist()\n    \n            return group_indices[:10]  # Select top 10 based on group and color (for now)\n    \n        elif entity_type == \"customer_to_product\":\n            # Retrieve customers who purchased a given product\n            purchasing_customers = train_data[train_data['article_id'] == entity_id]['customer_id'].unique()\n            return [customer_to_index[cid] for cid in purchasing_customers if cid in customer_to_index][:10]\n    \n        return []\n    \n    # ✅ Move `apply` operations to GPU-compatible format (Avoid slow `apply`)\n    # Processing in batches instead of iterating over rows (vectorized computation)\n    /\n    # ✅ **Process Articles**\n    articles_product_ids = train_articles['article_id'].values\n    product_positive_indices = [\n        get_positive_indices(idx, row['product_group_name'], row['colour_group_code'], row['article_id'], article_embedding_matrix, \"product\")\n        for idx, row in train_articles.iterrows()\n    ]\n    train_articles['positive_indices'] = product_positive_indices\n    \n    # ✅ **Process Customers**\n    customer_positive_indices = [\n        get_positive_indices(idx, None, None, row['customer_id'], customer_embedding_matrix, \"customer\")\n        for idx, row in train_customers.iterrows()\n    ]\n    train_customers['positive_indices'] = customer_positive_indices\n    \n    # ✅ **Process Customer-to-product Relations**\n    customer_to_product_positive_indices = [\n        get_positive_indices(idx, None, None, row['customer_id'], customer_embedding_matrix, \"customer_to_product\")\n        for idx, row in train_customers.iterrows()\n    ]\n    train_customers['customer_to_product_positive_indices'] = customer_to_product_positive_indices\n    \n    print(\"✅ Positive indices generation complete and moved to GPU where possible!\")\n\n\n    # ✅ Overwrite only modified files\n    def save_pkl(data, filepath):\n        with open(filepath, \"wb\") as f:\n            pickle.dump(data, f)\n    \n    save_pkl(train_data, os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"))\n    save_pkl(train_customers, os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"))\n    save_pkl(train_articles, os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"))\n    \n    # ✅ Update ZIP without deleting existing files\n    with zipfile.ZipFile(DATA_SPLITTING_ZIP_FILE, 'a') as zipf:\n        zipf.write(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"), arcname=\"train_data.pkl\")\n        zipf.write(os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"), arcname=\"train_customers.pkl\")\n        zipf.write(os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"), arcname=\"train_articles.pkl\")\n    \n    print(f\"✅ Updated data saved and zipped at {DATA_SPLITTING_ZIP_FILE}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if CUDA is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Convert lists to GPU tensors\ntotal_products = len(train_articles)\ntotal_customers = len(train_customers)\n\nproduct_indices = torch.arange(total_products, device=device)\ncustomer_indices = torch.arange(total_customers, device=device)\n\n# ✅ **GPU-Accelerated Hard Negative Sampling**\ndef generate_hard_negatives(total_items, positive_sets, entity_type, num_samples=10):\n    \"\"\"\n    Implements HARD NEGATIVE SAMPLING (as in C-STAR).\n    Selects items that are similar to positives but were NOT co-purchased.\n\n    ✅ Now uses **GPU acceleration** for efficiency 🚀\n    \"\"\"\n    negative_indices = torch.full((total_items, num_samples), -1, dtype=torch.long, device=device)  # Pre-allocate\n\n    # ✅ Convert `positive_sets` to **tensors** for GPU processing\n    exclusions = {i: torch.tensor(list(positive_sets.iloc[i]), device=device) if i < len(positive_sets) else torch.tensor([], device=device) for i in range(total_items)}\n\n    for i in range(total_items):\n        if entity_type == \"product\":\n            valid_negatives = product_indices[~torch.isin(product_indices, exclusions[i])]\n        elif entity_type == \"customer\":\n            valid_negatives = customer_indices[~torch.isin(customer_indices, exclusions[i])]\n        elif entity_type == \"customer_to_product\":\n            valid_negatives = customer_indices[~torch.isin(customer_indices, exclusions[i])]\n\n        if len(valid_negatives) > 0:\n            sampled_negatives = valid_negatives[:min(num_samples, len(valid_negatives))]\n            negative_indices[i, :len(sampled_negatives)] = sampled_negatives  # Assign sampled negatives\n\n    return negative_indices.cpu().numpy()  # Move back to CPU **after** computation\n\n# ✅ **Generate Negative Indices for Products (GPU-Accelerated)**\npositive_sets_products = train_articles['positive_indices'].apply(set)\ntrain_articles['negative_indices'] = list(generate_hard_negatives(total_products, positive_sets_products, entity_type=\"product\"))\n\n# ✅ **Generate Negative Indices for Customers (GPU-Accelerated)**\npositive_sets_customers = train_customers['positive_indices'].apply(set)\ntrain_customers['negative_indices'] = list(generate_hard_negatives(total_customers, positive_sets_customers, entity_type=\"customer\"))\n\n# ✅ **Generate Negative Indices for Product-to-Customer (GPU-Accelerated)**\npositive_sets_customer_to_product = train_customers['customer_to_product_positive_indices'].apply(set)\ntrain_customers['negative_customer_to_product_indices'] = list(generate_hard_negatives(total_customers, positive_sets_customer_to_product, entity_type=\"customer_to_product\"))\n\nprint(\"✅ Hard negative sampling moved to GPU for **faster execution** 🚀\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GCNLayer(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(GCNLayer, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, features, edge_index, edge_type):\n        row, col = edge_index\n        degree = torch.bincount(row, minlength=features.size(0)).float() + 1e-6\n        norm = 1.0 / torch.sqrt(degree[row] * degree[col])\n\n        # Different handling for different edge types\n        if edge_type == \"product_to_product\":\n            # Apply specific processing for product-to-product edges\n            pass  # Implement your specific logic here if needed\n        elif edge_type == \"customer_to_product\":\n            # Apply specific processing for customer-to-product edges\n            pass  # Implement your specific logic here if needed\n        elif edge_type == \"customer_to_customer\":\n            # Apply specific processing for customer-to-customer edges\n            pass  # Implement your specific logic here if needed\n\n        # Aggregate the features\n        agg_features = torch.zeros_like(features)\n        agg_features.index_add_(0, row, features[col] * norm.view(-1, 1))\n\n        return F.relu(self.linear(agg_features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class C_STAR(nn.Module):\n    def __init__(self, input_dim, embedding_dim, edge_index, edge_types, num_nodes, num_layers, dropout):\n        super(C_STAR, self).__init__()\n\n        self.embedding_layer = nn.Sequential(\n            nn.Linear(input_dim, embedding_dim),\n            nn.ReLU()\n        )\n\n        self.gcn_layers = nn.ModuleList([GCNLayer(embedding_dim, embedding_dim) for _ in range(num_layers)])\n        self.dropout = dropout\n        self.num_nodes = num_nodes\n        self.edges = edge_index\n        self.edge_types = edge_types  # Store edge types\n        self.reference_embedding = nn.Parameter(torch.randn(embedding_dim))  # Randomly initialized reference\n\n    def forward(self, features, edge_index, edge_type):\n        embeddings = self.embedding_layer(features)\n        embeddings = F.dropout(embeddings, p=self.dropout, training=self.training)\n\n        for gcn_layer, edge_type in zip(self.gcn_layers, self.edge_types):\n            embeddings = gcn_layer(embeddings, edge_index, edge_type)\n            embeddings = F.dropout(embeddings, p=self.dropout, training=self.training)\n\n        return embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inter_trajectory_loss(embeddings, reference_embedding):\n    \"\"\"\n    Computes the inter-trajectory loss using Sliced Wasserstein Distance.\n    \"\"\"\n    return sliced_wasserstein_distance(embeddings, reference_embedding)\n\n\n\ndef intra_trajectory_loss(embeddings, positive_indices, negative_indices, batch_mapping):\n    \"\"\"\n    Computes the intra-trajectory loss.\n    Ensures positive & negative indices are properly mapped and converted to tensors.\n    \"\"\"\n    # ✅ Convert lists to tensors before using `.view(-1)`\n    positive_indices = torch.tensor(positive_indices, dtype=torch.long, device=embeddings.device)\n    negative_indices = torch.tensor(negative_indices, dtype=torch.long, device=embeddings.device)\n\n    # ✅ Map to local batch indices (if needed)\n    positive_indices = torch.tensor(\n        [batch_mapping.get(idx, -1) for idx in positive_indices.tolist()],\n        dtype=torch.long, device=embeddings.device\n    )\n    negative_indices = torch.tensor(\n        [batch_mapping.get(idx, -1) for idx in negative_indices.tolist()],\n        dtype=torch.long, device=embeddings.device\n    )\n\n    # ✅ Filter valid indices\n    positive_indices = positive_indices[positive_indices >= 0]\n    negative_indices = negative_indices[negative_indices >= 0]\n\n    # ✅ Return 0 loss if no valid pairs exist\n    if positive_indices.numel() == 0 or negative_indices.numel() == 0:\n        return torch.tensor(0.0, requires_grad=True, device=embeddings.device)\n\n    # ✅ Compute cosine similarity for loss\n    positive_scores = torch.einsum('ij,ij->i', embeddings[positive_indices], embeddings[positive_indices])\n    negative_scores = torch.einsum('ij,ij->i', embeddings[negative_indices], embeddings[negative_indices])\n\n    # ✅ Adaptive margin based on embedding variance\n    margin = 1.0 + 0.1 * torch.std(embeddings)\n    loss = F.relu(negative_scores + margin - positive_scores).mean()\n    \n    return loss\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss(embeddings, article_idx, pos_indices, neg_indices, model, edge_type):\n    inter_loss = inter_trajectory_loss(embeddings[article_idx].unsqueeze(0), model.reference_embedding)\n    intra_loss = intra_trajectory_loss(embeddings, pos_indices, neg_indices, {})\n    return inter_loss + intra_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Define directory and file paths\nGRAPH_DIR = \"/kaggle/working/graph_data\"\nGRAPH_ZIP_FILE = \"/kaggle/working/graph_data.zip\"\n\n# ✅ Create the directory if it doesn't exist\nos.makedirs(GRAPH_DIR, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Check if precomputed ZIP file exists\nif os.path.exists(GRAPH_ZIP_FILE):\n    print(\"✅ Extracting PR-Graph and Edge Index data from ZIP...\")\n\n    # Extract the ZIP file\n    with zipfile.ZipFile(GRAPH_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(GRAPH_DIR)\n\n    # ✅ Load PR-Graph\n    with open(os.path.join(GRAPH_DIR, \"pr_graph.pkl\"), \"rb\") as f:\n        PR_graph = pickle.load(f)\n\n    # ✅ Load Edge Index Tensors\n    edge_index = torch.load(os.path.join(GRAPH_DIR, \"edge_index.pt\"))\n    product_to_product_edge_index = torch.load(os.path.join(GRAPH_DIR, \"product_to_product_edge_index.pt\"))\n    customer_to_customer_edge_index = torch.load(os.path.join(GRAPH_DIR, \"customer_to_customer_edge_index.pt\"))\n    customer_to_product_edge_index = torch.load(os.path.join(GRAPH_DIR, \"customer_to_product_edge_index.pt\"))\n\n    print(\"✅ PR-Graph and Edge Index loaded successfully!\")\n\nelse:\n    print(\"⚡ Recomputing PR-Graph and Edge Index...\")\n\n    # Construct the PR-Graph (Product-Relation Graph) based on co-purchase pairs\n    PR_graph = nx.Graph()\n\n    # ✅ Step 1: Add customer nodes with embeddings\n    for customer_idx, customer_embedding in enumerate(customer_embeddings):\n        customer_id = train_data['customer_id'].iloc[customer_idx]  # Use `.iloc` to avoid index mismatch\n        PR_graph.add_node(customer_id, embedding=customer_embedding)\n\n    # ✅ Step 2: Add product nodes (without redundant iteration)\n    for article_id in articles_with_images['article_id']:\n        PR_graph.add_node(article_id)  # No embedding needed for products\n\n    # ✅ Step 3: Add customer-to-product edges based on purchase history\n    customer_article_pairs = train_data[['customer_id', 'article_id']].values\n    PR_graph.add_edges_from(customer_article_pairs)\n    \n    # ✅ Step 4: Efficiently Add Customer-to-Customer Similarity Edges using SWD\n    customer_ids = list(customers['customer_id'])  # List of customer IDs for mapping\n\n    # Convert embeddings to tensor\n    customer_embeddings_tensor = torch.tensor(customer_embeddings, dtype=torch.float32)\n    \n    # Compute **pairwise Sliced Wasserstein Distance (SWD)**\n    for i in range(len(customer_embeddings)):\n        swd_distances = []\n        for j in range(i + 1, len(customer_embeddings)):\n            swd_dist = sliced_wasserstein_distance(\n                customer_embeddings_tensor[i], customer_embeddings_tensor[j]\n            )\n            swd_distances.append((customer_ids[i], customer_ids[j], swd_dist.item()))\n    \n        # ✅ Only add edges for customers with SWD < threshold (efficient filtering)\n        for u, v, dist in swd_distances:\n            if dist < 0.5:  # ✅ Adjust the threshold as needed\n                PR_graph.add_edge(u, v)\n\n    # ✅ Step 5: Add product-to-product co-purchase edges (but avoid redundant loops)\n    co_purchase_pairs = train_data.groupby('customer_id')['article_id'].apply(list)\n    for articles_list in co_purchase_pairs:\n        for i in range(len(articles_list)):\n            for j in range(i + 1, len(articles_list)):\n                PR_graph.add_edge(articles_list[i], articles_list[j])\n\n    # ✅ Done! Your PR-Graph is now optimized 🚀\n    print(f\"📌 PR-Graph Constructed: {PR_graph.number_of_nodes()} nodes, {PR_graph.number_of_edges()} edges\")\n\n    # Step 5: Map article IDs and customer IDs to indices for use in edge_index\n    article_to_index = {article_id: idx for idx, article_id in enumerate(PR_graph.nodes)}\n    customer_to_index = {customer_id: idx for idx, customer_id in enumerate(customers['customer_id'])}\n    \n    # Step 6: Convert edge list to tensor (edge_index)\n    edges_indices = []\n    \n    # Separate the edges into product-to-product, customer-to-product, and customer-to-customer\n    product_to_product_edges = []\n    customer_to_product_edges = []\n    customer_to_customer_edges = []\n    \n    # Iterate through the PR graph edges and classify the edges based on node types\n    for u, v in PR_graph.edges():\n        if u in article_to_index and v in article_to_index:\n            # Product-to-product edge\n            product_to_product_edges.append((article_to_index[u], article_to_index[v]))\n            edges_indices.append((article_to_index[u], article_to_index[v]))\n        elif u in customer_to_index and v in article_to_index:\n            # Customer-to-product edge\n            customer_to_product_edges.append((customer_to_index[u], article_to_index[v]))\n            edges_indices.append((customer_to_index[u], article_to_index[v]))\n        elif u in customer_to_index and v in customer_to_index:\n            # Customer-to-customer edge\n            customer_to_customer_edges.append((customer_to_index[u], customer_to_index[v]))\n            edges_indices.append((customer_to_index[u], customer_to_index[v]))\n    \n    # Convert edge list to tensor (edge_index) for all edges\n    edge_index = torch.tensor(edges_indices, dtype=torch.long).t().contiguous()\n    \n    # Step 7: Add self-loops (common in graph-based models)\n    edge_index, _ = add_self_loops(edge_index, num_nodes=len(PR_graph.nodes))\n    \n    # Ensure edge_index dimensions are correct\n    assert edge_index.size(0) == 2, \"Edge index should have two rows: source and target nodes.\"\n    assert edge_index.size(1) > 0, \"Edge index should have at least one edge.\"\n    \n    print(\"PR Graph successfully constructed!\")\n    \n    # Now, you have separate edge indices for each type of edge.\n    product_to_product_edge_index = torch.tensor(product_to_product_edges, dtype=torch.long).t().contiguous()\n    customer_to_product_edge_index = torch.tensor(customer_to_product_edges, dtype=torch.long).t().contiguous()\n    customer_to_customer_edge_index = torch.tensor(customer_to_customer_edges, dtype=torch.long).t().contiguous()\n    \n    # These edge indices are now available for further processing.\n    \n    print(edge_index.shape) \n    \n     # ✅ Save PR-Graph & Edge Index Locally\n    with open(os.path.join(GRAPH_DIR, \"pr_graph.pkl\"), \"wb\") as f:\n        pickle.dump(PR_graph, f)\n\n    torch.save(edge_index, os.path.join(GRAPH_DIR, \"edge_index.pt\"))\n    torch.save(product_to_product_edge_index, os.path.join(GRAPH_DIR, \"product_to_product_edge_index.pt\"))\n    torch.save(customer_to_product_edge_index, os.path.join(GRAPH_DIR, \"customer_to_product_edge_index.pt\"))\n    torch.save(customer_to_customer_edge_index, os.path.join(GRAPH_DIR, \"customer_to_customer_edge_index.pt\"))\n\n    # ✅ Create ZIP Archive for Future Use\n    with zipfile.ZipFile(GRAPH_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(GRAPH_DIR):\n            zipf.write(os.path.join(GRAPH_DIR, file), arcname=file)\n\n    print(f\"✅ PR-Graph and Edge Index processing completed! Saved as {GRAPH_ZIP_FILE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Define checkpoint file path\nCHECKPOINT_FILE = \"/kaggle/working/checkpoint.pth\"\n\n# ✅ Save model checkpoint\ndef save_checkpoint(model, optimizer, epoch, filename=CHECKPOINT_FILE):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    torch.save(checkpoint, filename)\n    print(f\"✅ Model checkpoint saved at epoch {epoch}\")\n\n# ✅ Load model checkpoint\ndef load_checkpoint(filename, model, optimizer=None):\n    if os.path.exists(filename):\n        print(\"✅ Loading checkpoint...\")\n        checkpoint = torch.load(filename, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        if optimizer:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(f\"✅ Resuming training from epoch {checkpoint['epoch'] + 1}\")\n        return checkpoint['epoch'] + 1  # Resume from the next epoch\n    else:\n        print(\"⚡ No checkpoint found. Starting from scratch.\")\n        return 0  # Start from epoch 0 if no checkpoint exists","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Use a smaller dataset for quick testing\ntrain_data = train_data.sample(n=10000, random_state=42)  \nval_data = val_data.sample(n=2000, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Training Loop with Early Stopping + GPU Optimization\ndef train_full_batch_with_early_stopping(model, optimizer, num_epochs, patience, start_epoch):\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n\n    # ✅ Move model & data to GPU (if available)\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    train_features_tensor = torch.tensor(train_features, dtype=dtype).to(device)\n    val_features_tensor = torch.tensor(val_features, dtype=dtype).to(device)\n\n    product_to_product_edge_index_tensor = product_to_product_edge_index.to(device)\n    customer_to_product_edge_index_tensor = customer_to_product_edge_index.to(device)\n    customer_to_customer_edge_index_tensor = customer_to_customer_edge_index.to(device)\n\n    with torch.no_grad():\n        initial_embeddings = model(train_features_tensor, product_to_product_edge_index_tensor)\n        initial_swd = sliced_wasserstein_distance(initial_embeddings, model.reference_embedding).item()\n        print(f\"✅ Initial SWD: {initial_swd:.4f}\")\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        optimizer.zero_grad()\n\n        # ✅ Forward pass for each edge type\n        all_embeddings_product_to_product = model(train_features_tensor, product_to_product_edge_index_tensor)\n        all_embeddings_customer_to_product = model(train_features_tensor, customer_to_product_edge_index_tensor)\n        all_embeddings_customer_to_customer = model(train_features_tensor, customer_to_customer_edge_index_tensor)\n\n        train_loss_acc = torch.zeros([], requires_grad=True, device=device)\n\n        for _, row in train_data.iterrows():\n            article_id = row[\"article_id\"]\n            customer_id = row[\"customer_id\"]\n\n            article_idx = article_to_index.get(article_id, -1)\n            customer_idx = customer_to_index.get(customer_id, -1)\n\n            if article_idx == -1 or customer_idx == -1:\n                continue  # Skip invalid entries\n\n            # ✅ Move positive/negative indices to GPU before converting to tensor\n            pos_indices_product = torch.tensor(row[\"positive_indices_product\"], dtype=torch.long, device=device)\n            neg_indices_product = torch.tensor(row[\"negative_indices_product\"], dtype=torch.long, device=device)\n\n            pos_indices_customer_product = torch.tensor(row[\"positive_indices_customer_product\"], dtype=torch.long, device=device)\n            neg_indices_customer_product = torch.tensor(row[\"negative_indices_customer_product\"], dtype=torch.long, device=device)\n\n            pos_indices_customer = torch.tensor(row[\"positive_indices_customer\"], dtype=torch.long, device=device)\n            neg_indices_customer = torch.tensor(row[\"negative_indices_customer\"], dtype=torch.long, device=device)\n\n            # ✅ Compute losses separately for each edge type\n            train_loss_acc += (\n                compute_loss(all_embeddings_product_to_product, article_idx, pos_indices_product, neg_indices_product, model, \"product_to_product\") +\n                compute_loss(all_embeddings_customer_to_product, customer_idx, pos_indices_customer_product, neg_indices_customer_product, model, \"customer_to_product\") +\n                compute_loss(all_embeddings_customer_to_customer, customer_idx, pos_indices_customer, neg_indices_customer, model, \"customer_to_customer\")\n            )\n\n        # ✅ Backpropagation\n        train_loss_acc.backward()\n        # ✅ TPU Optimized Training Step\n        xm.optimizer_step(optimizer)\n        xm.mark_step()\n\n\n        avg_train_loss = train_loss_acc.item() / max(len(train_data), 1)\n\n        # ✅ Validation Phase\n        model.eval()\n        total_val_loss = 0.0\n\n        with torch.no_grad():\n            val_embeddings_product_to_product = model(val_features_tensor, product_to_product_edge_index_tensor)\n            val_embeddings_customer_to_product = model(val_features_tensor, customer_to_product_edge_index_tensor)\n            val_embeddings_customer_to_customer = model(val_features_tensor, customer_to_customer_edge_index_tensor)\n\n            for _, row in val_data.iterrows():\n                article_id = row[\"article_id\"]\n                customer_id = row[\"customer_id\"]\n\n                article_idx = article_to_index.get(article_id, -1)\n                customer_idx = customer_to_index.get(customer_id, -1)\n\n                if article_idx == -1 or customer_idx == -1:\n                    continue  # Skip invalid entries\n\n                pos_indices_product = torch.tensor(row[\"positive_indices_product\"], dtype=torch.long, device=device)\n                neg_indices_product = torch.tensor(row[\"negative_indices_product\"], dtype=torch.long, device=device)\n\n                pos_indices_customer_product = torch.tensor(row[\"positive_indices_customer_product\"], dtype=torch.long, device=device)\n                neg_indices_customer_product = torch.tensor(row[\"negative_indices_customer_product\"], dtype=torch.long, device=device)\n\n                pos_indices_customer = torch.tensor(row[\"positive_indices_customer\"], dtype=torch.long, device=device)\n                neg_indices_customer = torch.tensor(row[\"negative_indices_customer\"], dtype=torch.long, device=device)\n\n                total_val_loss += (\n                    compute_loss(val_embeddings_product_to_product, article_idx, pos_indices_product, neg_indices_product, model, \"product_to_product\") +\n                    compute_loss(val_embeddings_customer_to_product, customer_idx, pos_indices_customer_product, neg_indices_customer_product, model, \"customer_to_product\") +\n                    compute_loss(val_embeddings_customer_to_customer, customer_idx, pos_indices_customer, neg_indices_customer, model, \"customer_to_customer\")\n                )\n\n        avg_val_loss = total_val_loss / max(len(val_data), 1)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n\n        # ✅ Save checkpoint\n        save_checkpoint(model, optimizer, epoch, filename=CHECKPOINT_FILE)\n        \n        # ✅ Early Stopping Logic\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict()\n            save_checkpoint(model, optimizer, epoch, filename='best_checkpoint.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"🚀 Early stopping triggered! Restoring best model...\")\n                model.load_state_dict(best_model_state)\n                break\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute input dimensions\ninput_dim_text = text_embeddings.shape[1]  # Text feature dimension\ninput_dim_numeric = len(train_data['price'])  # Numeric feature dimension\ninput_dim_image = image_embeddings.shape[1]  # Image feature dimension\ninput_dim_customer = customer_embeddings.shape[1]  # Customer feature dimension\n\n# Total input dimension\ninput_dim = input_dim_text + input_dim_image + input_dim_numeric +input_dim_customer\n\n# Training with updated input dimensions\n# embedding_dim = 64  # Size of the embeddings\n# num_layers = 3      # Number of GCN layers\n# dropout = 0.3       # Dropout rate for regularization\nnum_projections = 50  # Number of projections for Sliced Wasserstein Distance (SWD)\n# learning_rate = 1e-2  # Learning rate for the optimizer\nweight_decay = 1e-5 \n\nlearning_rate = 5e-3  # ✅ Slightly higher but stable\nnum_epochs = 2        # ✅ Just for testing if training runs\nembedding_dim = 32    # ✅ Smaller embeddings reduce computation\nnum_layers = 2        # ✅ Fewer layers for quick runs\ndropout = 0.2         # ✅ Lower dropout for faster learning\n\n\n# Use the correct input dimensions for text and numeric features\n# input_dim_text = len(text_embeddings[0])  # Text feature dimension\n# input_dim_numeric = len(numerical_columns)  # Numeric feature dimension\n\nmodel = C_STAR(input_dim=input_dim, embedding_dim=embedding_dim, edge_index=edge_index, \n               edge_types=edge_types, num_nodes=len(PR_graph.nodes), num_layers=num_layers)\nmodel = model.to(device)\n\noptimizer = optim_xla.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nstart_epoch = load_checkpoint(CHECKPOINT_FILE, model, optimizer)\n\n# Train the model with early stopping\nnum_epochs = 1\npatience = 10\ntrained_model = train_full_batch_with_early_stopping(\n    model=model,\n    optimizer=optimizer,\n    num_epochs=num_epochs,\n    patience=patience, \n    start_epoch=start_epoch\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metrics Evaluation Functions\ndef recall_at_k(recommended_articles, relevant_articles, k=10):\n    recommended_set = set(recommended_articles[:k])\n    relevant_set = set(relevant_articles)\n    intersection = recommended_set.intersection(relevant_set)\n    return len(intersection) / len(relevant_set) if len(relevant_set) > 0 else 0.0\n\ndef ndcg_at_k(recommended_articles, relevant_articles, k=10):\n    recommended_set = set(recommended_articles[:k])\n    dcg = 0.0\n    idcg = 0.0\n\n    for i in range(min(k, len(recommended_articles))):\n        if recommended_articles[i] in relevant_articles:\n            dcg += 1 / np.log2(i + 2)\n\n    for i in range(min(k, len(relevant_articles))):\n        idcg += 1 / np.log2(i + 2)\n\n    return dcg / idcg if idcg > 0 else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Full-batch evaluation with Recall@K and NDCG@K metrics using Sliced Wasserstein Distance (SWD)\ndef test_model_full_batch_with_metrics(model, k=10):\n    model.eval()\n    total_test_loss = 0.0\n    all_recall_at_k = []\n    all_ndcg_at_k = []\n\n    test_features_tensor = torch.tensor(test_features, dtype=dtype).to(device)\n\n    with torch.no_grad():\n        # Compute embeddings for each edge type separately\n        test_embeddings_product_to_product = model(test_features_tensor, product_to_product_edge_index)\n        test_embeddings_customer_to_product = model(test_features_tensor, customer_to_product_edge_index)\n        test_embeddings_customer_to_customer = model(test_features_tensor, customer_to_customer_edge_index)\n\n        for _, row in test_data.iterrows():\n            customer_id = row[\"customer_id\"]\n            article_id = row[\"article_id\"]\n\n            # Get purchased articles for this customer\n            customer_transactions = transactions_filtered[transactions_filtered['customer_id'] == customer_id]\n            relevant_articles = customer_transactions['article_id'].unique()\n\n            # Map article and customer IDs to indices\n            article_idx = article_to_index.get(article_id, -1)\n            customer_idx = customer_to_index.get(customer_id, -1)\n\n            if article_idx == -1 or customer_idx == -1:\n                continue  # Skip invalid entries\n\n            # Compute **Sliced Wasserstein Distance (SWD)**\n            swd_distances_product = []\n            swd_distances_customer = []\n            swd_distances_customer_to_product = []\n\n            # **Compute SWD for Product-to-Product**\n            product_embedding = test_embeddings_product_to_product[article_idx].unsqueeze(0)\n            for i in range(test_embeddings_product_to_product.shape[0]):\n                if i != article_idx:  # Skip the same article\n                    dist = sliced_wasserstein_distance(product_embedding, test_embeddings_product_to_product[i].unsqueeze(0))\n                    swd_distances_product.append((i, dist.item()))\n\n            # **Compute SWD for Customer-to-Customer**\n            customer_embedding = test_embeddings_customer_to_customer[customer_idx].unsqueeze(0)\n            for i in range(test_embeddings_customer_to_customer.shape[0]):\n                if i != customer_idx:  # Skip the same customer\n                    dist = sliced_wasserstein_distance(customer_embedding, test_embeddings_customer_to_customer[i].unsqueeze(0))\n                    swd_distances_customer.append((i, dist.item()))\n\n            # **Compute SWD for Customer-to-Product**\n            for i in range(test_embeddings_product_to_product.shape[0]):\n                dist = sliced_wasserstein_distance(customer_embedding, test_embeddings_product_to_product[i].unsqueeze(0))\n                swd_distances_customer_to_product.append((i, dist.item()))\n\n            # Sort by SWD distance (lower is better)\n            swd_distances_product.sort(key=lambda x: x[1])\n            swd_distances_customer.sort(key=lambda x: x[1])\n            swd_distances_customer_to_product.sort(key=lambda x: x[1])\n\n            # Get top-k recommendations\n            top_indices_product = [item[0] for item in swd_distances_product[:k]]\n            top_indices_customer = [item[0] for item in swd_distances_customer[:k]]\n            top_indices_customer_to_product = [item[0] for item in swd_distances_customer_to_product[:k]]\n\n            recommended_articles = [list(article_to_index.keys())[idx] for idx in top_indices_customer_to_product]\n\n            # **Compute Recall@K and NDCG@K**\n            recall = recall_at_k(recommended_articles, relevant_articles, k)\n            ndcg = ndcg_at_k(recommended_articles, relevant_articles, k)\n\n            all_recall_at_k.append(recall)\n            all_ndcg_at_k.append(ndcg)\n\n            # Retrieve **correct positive and negative indices**\n            pos_indices_product = articles_with_images.loc[articles_with_images[\"article_id\"] == article_id, \"positive_indices\"].values[0]\n            neg_indices_product = articles_with_images.loc[articles_with_images[\"article_id\"] == article_id, \"negative_indices\"].values[0]\n\n            pos_indices_customer = customers.loc[customers[\"customer_id\"] == customer_id, \"positive_indices\"].values[0]\n            neg_indices_customer = customers.loc[customers[\"customer_id\"] == customer_id, \"negative_indices\"].values[0]\n\n            pos_indices_customer_product = customers.loc[customers[\"customer_id\"] == customer_id, \"customer_to_product_positive_indices\"].values[0]\n            neg_indices_customer_product = customers.loc[customers[\"customer_id\"] == customer_id, \"negative_customers\"].values[0]\n\n            # Compute **Inter-Trajectory and Intra-Trajectory Loss**\n            inter_loss_product = inter_trajectory_loss(test_embeddings_product_to_product[article_idx].unsqueeze(0), model.reference_embedding)\n            intra_loss_product = intra_trajectory_loss(test_embeddings_product_to_product, pos_indices_product, neg_indices_product, {})\n\n            inter_loss_customer = inter_trajectory_loss(test_embeddings_customer_to_customer[customer_idx].unsqueeze(0), model.reference_embedding)\n            intra_loss_customer = intra_trajectory_loss(test_embeddings_customer_to_customer, pos_indices_customer, neg_indices_customer, {})\n\n            inter_loss_customer_product = inter_trajectory_loss(test_embeddings_customer_to_product[customer_idx].unsqueeze(0), model.reference_embedding)\n            intra_loss_customer_product = intra_trajectory_loss(test_embeddings_customer_to_product, pos_indices_customer_product, neg_indices_customer_product, {})\n\n            # **Total Loss**\n            total_test_loss += (inter_loss_product + intra_loss_product +\n                                inter_loss_customer + intra_loss_customer +\n                                inter_loss_customer_product + intra_loss_customer_product).item()\n\n    # **Normalize Loss**\n    avg_test_loss = total_test_loss / max(len(test_data), 1)\n    avg_recall_at_k = np.mean(all_recall_at_k)\n    avg_ndcg_at_k = np.mean(all_ndcg_at_k)\n\n    # **Print Metrics**\n    print(f\"✅ Test Loss: {avg_test_loss:.4f}\")\n    print(f\"✅ Recall@{k}: {avg_recall_at_k:.4f}\")\n    print(f\"✅ NDCG@{k}: {avg_ndcg_at_k:.4f}\")\n\n    return avg_test_loss, avg_recall_at_k, avg_ndcg_at_k\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage of testing\ntest_model_full_batch_with_metrics(\n    model=trained_model,\n    edge_index=edge_index,\n    k=10\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_similar_embeddings(target_embedding, reference_embeddings, top_n=10):\n    \"\"\"\n    Compute Sliced Wasserstein Distance (SWD) between a target embedding and all reference embeddings.\n    Returns the indices of the top-N most similar embeddings.\n    \"\"\"\n    swd_distances = []\n    \n    for i in range(reference_embeddings.shape[0]):\n        dist = sliced_wasserstein_distance(target_embedding.unsqueeze(0), reference_embeddings[i].unsqueeze(0))\n        swd_distances.append((i, dist.item()))\n\n    # ✅ Sort by SWD distance (lower is better)\n    swd_distances.sort(key=lambda x: x[1])\n    \n    return [item[0] for item in swd_distances[:top_n]]  # Return top N indices\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recommend_articles_any_input(\n    model, edge_index, top_n=10, image_path=None, text_input=None, numeric_input=None, \n    age=None, active=None, club_status=None):\n    \"\"\"\n    Multi-modal product recommendation function.\n    Users can input any combination of image, text, and numeric data.\n    Supports cold-start users by recommending based on similar customers if any of `age`, `active`, or `club_status` is provided.\n    \"\"\"\n    input_features = []\n    test_features_tensor = torch.tensor(test_features, dtype=dtype).to(device)\n\n    # ✅ 1. Process Image Input (if provided)\n    if image_path:\n        image = Image.open(image_path).convert('RGB')\n        image = transform(image).unsqueeze(0).to(device)\n        with torch.no_grad():\n            image_embedding = resnet_model(image).squeeze(0).cpu().numpy()\n        input_features.append(image_embedding)\n\n    # ✅ 2. Process Text Input (if provided)\n    if text_input:\n        words = text_input.split()  \n        word_embeddings = np.zeros(100)  \n        valid_word_count = 0\n\n        for word in words:\n            if word in embeddings_index:  \n                word_embeddings += embeddings_index[word]\n                valid_word_count += 1\n\n        if valid_word_count > 0:\n            word_embeddings /= valid_word_count  \n        input_features.append(word_embeddings)\n\n    # ✅ 3. Process Numeric Input (if provided)\n    if numeric_input:\n        numeric_input = np.array(numeric_input).reshape(1, -1)\n        scaled_numeric_input = scaler.transform(numeric_input)  \n        input_features.append(scaled_numeric_input.flatten())\n\n    # ✅ 4. Handle Partial Cold Start (One or More Customer Attributes Given)\n    if any([age is not None, active is not None, club_status is not None]):  # If at least one is given\n        print(\"🚀 Partial Cold Start Detected: Generating Customer Embedding...\")\n\n        # ✅ Create a DataFrame with the provided inputs\n        cold_start_input = pd.DataFrame({\"Active\": [active], \"club_member_status\": [club_status]})\n\n        # ✅ One-hot encode categorical features using the **same approach as training**\n        dummy_encoded = pd.get_dummies(cold_start_input)\n        \n        # ✅ Ensure missing categorical columns are handled correctly\n        missing_cols = [col for col in categorical_features_encoded.columns if col not in dummy_encoded.columns]\n        for col in missing_cols:\n            dummy_encoded[col] = 0  # Add missing columns with default 0 values\n\n        # ✅ Ensure column order matches training\n        dummy_encoded = dummy_encoded[categorical_features_encoded.columns].astype(np.float32).values\n\n        # ✅ Normalize age if given\n        normalized_age = np.zeros(1) if age is None else scaler_age.transform([[age]]).flatten()\n\n        # ✅ Construct customer input feature\n        new_customer_features = np.hstack([normalized_age.reshape(-1, 1), dummy_encoded]).astype(np.float32)\n        new_customer_features_tensor = torch.tensor(new_customer_features, dtype=torch.float32).to(device)\n\n        # ✅ Generate embedding for the new customer\n        with torch.no_grad():\n            new_customer_embedding = model(new_customer_features_tensor).squeeze(0)to(device)\n\n        # ✅ Find top-N similar customers\n        top_similar_customers = find_similar_embeddings(new_customer_embedding, customer_embeddings, top_n)\n\n        # ✅ Retrieve products purchased by these similar customers\n        recommended_articles = transactions_filtered[\n            transactions_filtered[\"customer_id\"].isin([customers.iloc[i][\"customer_id\"] for i in top_similar_customers])\n        ]\n        recommended_articles = recommended_articles.groupby(\"article_id\").size().reset_index(name=\"purchase_count\")\n        recommended_articles = recommended_articles.sort_values(\"purchase_count\", ascending=False).head(top_n)\n        recommended_articles = articles_with_images[articles_with_images[\"article_id\"].isin(recommended_articles[\"article_id\"])]\n\n        print(\"🔹 **Final Recommendations for Partial Cold-Start User** 🔹\\n\")\n        for _, row in recommended_articles.iterrows():\n            print(f\"🛍️ Product: {row['prod_name']}\\n📜 Description: {row['text_data']}\\n\")\n            display(Image.open(row['path']))\n            print(\"\\n\")\n\n        return recommended_articles[['article_id', 'prod_name']]\n\n    # ✅ 5. If NOT a cold-start user, proceed with normal recommendations\n    input_features = np.hstack(input_features)\n    input_tensor = torch.tensor(input_features, dtype=torch.float32).unsqueeze(0).to(device)\n\n    # ✅ Compute the full embedding space using the trained model\n    with torch.no_grad():\n        all_embeddings = model(test_features_tensor, edge_index)\n\n    # ✅ Find top-N similar product indices\n    top_indices = find_similar_embeddings(input_tensor, all_embeddings, top_n)\n\n    # ✅ Retrieve recommended products\n    recommended_articles = articles_with_images.iloc[top_indices][['article_id', 'prod_name', 'text_data', 'path']]\n\n    print(\"🔹 **Final Recommendations Based on Multi-Modal Input** 🔹\\n\")\n    for _, row in recommended_articles.iterrows():\n        print(f\"🛍️ Product: {row['prod_name']}\\n📜 Description: {row['text_data']}\\n\")\n        display(Image.open(row['path']))\n        print(\"\\n\")\n\n    return recommended_articles[['article_id', 'prod_name']]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image Input Recommendation\nrecommend_articles_any_input(\n    model=trained_model,\n    edge_index=edge_index,\n    top_n=5,\n    image_path=\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/079/0797892010.jpg\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Text Input Recommendation\nrecommend_articles_any_input(\n    model=trained_model,\n    edge_index=edge_index,\n    top_n=5,\n    text_input=\"Floral summer dress with short sleeves\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numeric Input Recommendation\nnumeric_input_example = [22, 5, 14, 8, 1, 3]  # Example input for product_type_no, colour_group_code, etc.\nrecommend_articles_any_input(\n    model=trained_model,\n    edge_index=edge_index,\n    top_n=5,\n    numeric_input=numeric_input_example\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combined Multi-modal Recommendation\nrecommend_articles_any_input(\n    model=trained_model,\n    edge_index=edge_index,\n    top_n=5,\n    image_path=\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/072/0720572001.jpg\",\n    text_input=\"Black mini dress\",\n    numeric_input=[30,40,50]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"recommend_articles_any_input(\n    model=trained_model,\n    edge_index=edge_index,\n    top_n=5,\n    age=30\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}