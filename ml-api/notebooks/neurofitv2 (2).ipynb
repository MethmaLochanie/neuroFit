{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":10751747,"sourceType":"datasetVersion","datasetId":6668414},{"sourceId":10790163,"sourceType":"datasetVersion","datasetId":6695980},{"sourceId":10791840,"sourceType":"datasetVersion","datasetId":6697123},{"sourceId":10792494,"sourceType":"datasetVersion","datasetId":6697579},{"sourceId":10792663,"sourceType":"datasetVersion","datasetId":6697699},{"sourceId":10793330,"sourceType":"datasetVersion","datasetId":6698188},{"sourceId":10793757,"sourceType":"datasetVersion","datasetId":6698516},{"sourceId":10801784,"sourceType":"datasetVersion","datasetId":6704323}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport torch\nimport cudf  # RAPIDS cuDF (GPU-accelerated Pandas)\nimport cupy as cp  # GPU-accelerated NumPy\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport faiss\nfrom torch_geometric.data import HeteroData\nfrom torch_geometric.utils import add_self_loops\nimport pickle\nimport zipfile\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.737477Z","iopub.execute_input":"2025-02-21T04:02:52.737874Z","iopub.status.idle":"2025-02-21T04:02:52.743287Z","shell.execute_reply.started":"2025-02-21T04:02:52.737823Z","shell.execute_reply":"2025-02-21T04:02:52.742322Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.762076Z","iopub.status.idle":"2025-02-21T04:02:52.762315Z","shell.execute_reply":"2025-02-21T04:02:52.762214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adjust article ID format\ndef adjust_id(x):\n    x = str(x)\n    return \"0\" + x if len(x) == 9 else x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.763385Z","iopub.status.idle":"2025-02-21T04:02:52.763762Z","shell.execute_reply":"2025-02-21T04:02:52.763598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nPREPROCESSED_DIR = \"/kaggle/input/pre-processed-data-3\"\n\n# # ✅ Define file paths for saving preprocessed data\n# PREPROCESSED_DIR = \"/kaggle/working/preprocessed-data\"\n# ZIP_FILE = \"/kaggle/working/preprocessed-data.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.764560Z","iopub.status.idle":"2025-02-21T04:02:52.764955Z","shell.execute_reply":"2025-02-21T04:02:52.764773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(PREPROCESSED_DIR, \"articles_with_images.pkl\")):\n    print(\"✅ Preprocessed data already exists. Loading...\")\n\n    with open(os.path.join(PREPROCESSED_DIR, \"articles_with_images.pkl\"), \"rb\") as f:\n        articles_with_images = pickle.load(f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"transactions_filtered.pkl\"), \"rb\") as f:\n        transactions_filtered = pickle.load(f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"customers_processed.pkl\"), \"rb\") as f:\n        customers = pickle.load(f)\n\n    print(\"✅ Preprocessed data loaded successfully!\")\n\nelse :\n    print(\"✅ Preprocessed data not found\")\n\n    articles = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv')\n    customers = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/customers.csv')\n    transactions = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\n    \n    # Get all paths from the image folder\n    all_image_paths = glob.glob(\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/*/*\")\n    \n    # Adjust the article ID and product code to be string & add \"0\"\n    articles[\"article_id\"] = articles[\"article_id\"].apply(lambda x: adjust_id(x))\n    articles[\"product_code\"] = articles[\"article_id\"].apply(lambda x: x[:3])\n    \n    # Get all valid article IDs and create a set to store image IDs\n    all_image_ids = set()\n    \n    for path in tqdm(all_image_paths, desc=\"Processing Images\"):\n        article_id = os.path.basename(path).split('.')[0]  # Extract image ID from filename\n        all_image_ids.add(article_id)\n    \n    \n    # Construct full image paths and filter invalid ones\n    images_path = \"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/\"\n    articles[\"path\"] = articles[\"article_id\"].apply(\n        lambda x: images_path + x[:3] + \"/\" + x + \".jpg\" if x in all_image_ids else None\n    )\n    \n    # ✅ Keep only articles with valid images\n    articles_with_images = articles.dropna(subset=[\"path\"]).reset_index(drop=True)\n    \n    # Adjust the article ID and product code to be string & add \"0\"\n    \n    articles_with_images[\"article_id\"] = articles_with_images[\"article_id\"].astype(\"category\")\n    articles_with_images[\"product_code\"] = articles_with_images[\"product_code\"].astype(\"category\")\n    \n    # Fill missing values\n    customers.fillna({\n        \"FN\": 0,\n        \"Active\": 0,\n        \"club_member_status\": \"UNKNOWN\",\n        \"fashion_news_frequency\": \"UNKNOWN\",\n        \"age\": customers[\"age\"].median()\n    }, inplace=True)\n    \n    customers[\"customer_id\"] = customers[\"customer_id\"].astype(\"category\")\n    customers[\"Active\"] = customers[\"Active\"].astype(np.int8)\n    customers[\"FN\"] = customers[\"FN\"].astype(np.int8)\n    customers[\"age\"] = customers[\"age\"].astype(np.float16)\n    \n    if 'age' in customers.columns:\n        scaler_age = MinMaxScaler()\n        customers['normalized_age'] = scaler_age.fit_transform(customers[['age']])\n    else:\n        raise ValueError(\"Error: `age` column is missing in customers!\")\n    \n    # Adjust article_id (as did for articles dataframe)\n    transactions[\"article_id\"] = transactions[\"article_id\"].apply(lambda x: adjust_id(x))\n    \n    # Filter the transactions dataset to keep only valid article IDs\n    transactions_filtered = transactions[transactions[\"article_id\"].isin(set(articles_with_images[\"article_id\"]))].reset_index(drop=True)\n    \n    # Optionally save the filtered transactions dataset\n    # transactions_filtered.to_csv(\"transactions_filtered.csv\", index=False)\n    transactions_filtered[\"article_id\"] = transactions_filtered[\"article_id\"].astype(\"category\")\n    transactions_filtered[\"price\"] = transactions_filtered[\"price\"].astype(np.float16)\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(PREPROCESSED_DIR, \"articles_with_images.pkl\"), \"wb\") as f:\n        pickle.dump(articles_with_images, f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"transactions_filtered.pkl\"), \"wb\") as f:\n        pickle.dump(transactions_filtered, f)\n\n    with open(os.path.join(PREPROCESSED_DIR, \"customers_processed.pkl\"), \"wb\") as f:\n        pickle.dump(customers, f)\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(PREPROCESSED_DIR):\n            zipf.write(os.path.join(PREPROCESSED_DIR, file), arcname=file)\n\n    print(f\"✅ Preprocessing completed! Saved as {ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.765778Z","iopub.status.idle":"2025-02-21T04:02:52.766179Z","shell.execute_reply":"2025-02-21T04:02:52.766015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ✅ Define file paths for saving preprocessed data\n# IMAGE_EMBEDDINGS_DIR = \"/kaggle/working/image_embeddings\"\n# IMAGE_EMBEDDINGS_ZIP_FILE = \"/kaggle/working/image_embeddings.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(IMAGE_EMBEDDINGS_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nIMAGE_EMBEDDINGS_DIR = \"/kaggle/input/image-embeddings-2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.928219Z","iopub.execute_input":"2025-02-21T04:02:52.928506Z","iopub.status.idle":"2025-02-21T04:02:52.931618Z","shell.execute_reply.started":"2025-02-21T04:02:52.928476Z","shell.execute_reply":"2025-02-21T04:02:52.930982Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(IMAGE_EMBEDDINGS_DIR, \"image_embeddings.pkl\")):\n    print(\"✅ image_embeddings data already exists. Loading...\")\n\n    with open(os.path.join(IMAGE_EMBEDDINGS_DIR, \"image_embeddings.pkl\"), \"rb\") as f:\n        image_embeddings = pickle.load(f)\n\n    print(\"✅ image embeddings data loaded successfully!\")\n\nelse:\n\n    print(\"✅ image embeddings not found!\")\n    torch.backends.cudnn.benchmark = True  # Optimizes GPU computation\n    \n    # ✅ Load pre-trained ResNet50 model (Feature Extraction)\n    resnet_model = models.resnet50(pretrained=True)\n    resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])  # Remove the last FC layer\n    resnet_model = resnet_model.to(device).eval()  # Move to GPU & set eval mode\n    \n    # ✅ Define image transformations\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  \n        transforms.ToTensor(),          \n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n    ])\n    \n    # ✅ Function to extract image embeddings (Batch Processing)\n    def get_image_embeddings_batch(image_paths_batch):\n        images = [transform(Image.open(path).convert('RGB')).unsqueeze(0) for path in image_paths_batch]\n        images_tensor = torch.cat(images, dim=0).to(device)  # Stack tensors & move to GPU\n        \n        with torch.no_grad():\n            features = resnet_model(images_tensor).squeeze(-1).squeeze(-1)  # Remove extra dimensions\n    \n        return features  # Return GPU tensor directly (no need for .cpu().numpy())\n    \n    # ✅ Process images in batches to avoid memory issues\n    batch_size = 512  # Adjust batch size based on available memory\n    image_embeddings = []\n    \n    for i in tqdm(range(0, len(articles_with_images), batch_size), desc=\"Extracting Features\"):\n        image_paths_batch = articles_with_images['path'][i:i + batch_size].tolist()\n        image_embeddings.append(get_image_embeddings_batch(image_paths_batch))\n    \n    # ✅ Convert list of tensors to a single tensor\n    image_embeddings = torch.cat(image_embeddings, dim=0)\n    \n    print(f\"✅ Image feature extraction complete. Shape: {image_embeddings.shape}\")\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(IMAGE_EMBEDDINGS_DIR, \"image_embeddings.pkl\"), \"wb\") as f:\n        pickle.dump(image_embeddings, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(IMAGE_EMBEDDINGS_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(IMAGE_EMBEDDINGS_DIR):\n            zipf.write(os.path.join(IMAGE_EMBEDDINGS_DIR, file), arcname=file)\n\n    print(f\"✅ image_embeddings completed! Saved as {IMAGE_EMBEDDINGS_ZIP_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:52.932996Z","iopub.execute_input":"2025-02-21T04:02:52.933258Z","iopub.status.idle":"2025-02-21T04:02:55.026695Z","shell.execute_reply.started":"2025-02-21T04:02:52.933237Z","shell.execute_reply":"2025-02-21T04:02:55.026003Z"}},"outputs":[{"name":"stdout","text":"✅ image_embeddings data already exists. Loading...\n✅ image embeddings data loaded successfully!\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# ✅ Define file paths for saving preprocessed data\n# TEXT_EMBEDDINGS_DIR = \"/kaggle/working/text_embeddings\"\n# TEXT_EMBEDDINGS_ZIP_FILE = \"/kaggle/working/text_embeddings.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(TEXT_EMBEDDINGS_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nTEXT_EMBEDDINGS_DIR = \"/kaggle/input/text-embeddings-2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:55.028158Z","iopub.execute_input":"2025-02-21T04:02:55.028394Z","iopub.status.idle":"2025-02-21T04:02:55.031715Z","shell.execute_reply.started":"2025-02-21T04:02:55.028373Z","shell.execute_reply":"2025-02-21T04:02:55.031009Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(TEXT_EMBEDDINGS_DIR, \"text_embeddings.pkl\")):\n    print(\"✅ text_embeddings data already exists. Loading...\")\n\n    with open(os.path.join(TEXT_EMBEDDINGS_DIR, \"text_embeddings.pkl\"), \"rb\") as f:\n        text_embeddings = pickle.load(f)\n\n    print(\"✅ text embeddings data loaded successfull\")\n\nelse :\n\n    print(\"✅ text embeddings data not found\")\n    # ✅ Ensure Required Columns Exist\n    required_cols = [\n        'detail_desc', 'prod_name', 'product_type_name', 'product_group_name',\n        'graphical_appearance_name', 'colour_group_name', 'index_name',\n        'index_group_name', 'section_name', 'garment_group_name'\n    ]\n    \n    missing_cols = [col for col in required_cols if col not in articles_with_images.columns]\n    \n    if missing_cols:\n        raise ValueError(f\"Error: Missing columns: {missing_cols}\")\n    \n    # ✅ Efficiently Combine Text Columns\n    articles_with_images['text_data'] = articles_with_images[required_cols].fillna('').agg(' '.join, axis=1)\n    \n    # ✅ Load GloVe Embeddings Efficiently\n    def load_glove_embeddings(glove_file_path):\n        embeddings_index = {}\n        with open(glove_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                vector = np.asarray(values[1:], dtype='float32')\n                embeddings_index[word] = vector\n        return embeddings_index\n    \n    # ✅ Load GloVe 100D embeddings\n    glove_file_path = '/kaggle/input/text-glove/glove.6B.100d.txt'\n    embeddings_index = load_glove_embeddings(glove_file_path)\n    \n    # ✅ Convert Text Data to GloVe Embeddings (Optimized)\n    def text_to_glove_embeddings(text_data, embeddings_index, embedding_dim=100, device=\"cuda\"):\n        text_embeddings = torch.zeros((len(text_data), embedding_dim), dtype=torch.float32, device=device)\n    \n        for i, text in enumerate(text_data):\n            words = text.split()\n            word_vectors = [torch.tensor(embeddings_index[word], device=device) for word in words if word in embeddings_index]\n            \n            if word_vectors:\n                text_embeddings[i] = torch.stack(word_vectors).mean(dim=0)  # Compute mean embedding\n    \n        return text_embeddings\n    \n    # ✅ Convert Text Data to GloVe Embeddings using GPU\n    text_embeddings = text_to_glove_embeddings(articles_with_images['text_data'], embeddings_index)\n    \n    print(f\"✅ Text feature extraction complete. Embeddings Shape: {text_embeddings.shape}\")\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(TEXT_EMBEDDINGS_DIR, \"text_embeddings.pkl\"), \"wb\") as f:\n        pickle.dump(text_embeddings, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(TEXT_EMBEDDINGS_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(TEXT_EMBEDDINGS_DIR):\n            zipf.write(os.path.join(TEXT_EMBEDDINGS_DIR, file), arcname=file)\n\n    print(f\"✅ text_embeddings completed! Saved as {TEXT_EMBEDDINGS_ZIP_FILE}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:55.032876Z","iopub.execute_input":"2025-02-21T04:02:55.033166Z","iopub.status.idle":"2025-02-21T04:02:55.153094Z","shell.execute_reply.started":"2025-02-21T04:02:55.033142Z","shell.execute_reply":"2025-02-21T04:02:55.152358Z"}},"outputs":[{"name":"stdout","text":"✅ text_embeddings data already exists. Loading...\n✅ text embeddings data loaded successfull\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# ✅ Define file paths for saving preprocessed data\n# CUSTOMER_EMBEDDINGS_DIR = \"/kaggle/working/customer_embeddings\"\n# CUSTOMER_EMBEDDINGS_ZIP_FILE = \"/kaggle/working/customer_embeddings.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(CUSTOMER_EMBEDDINGS_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nCUSTOMER_EMBEDDINGS_DIR = \"/kaggle/input/customer-embeddings-2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:55.153847Z","iopub.execute_input":"2025-02-21T04:02:55.154150Z","iopub.status.idle":"2025-02-21T04:02:55.157552Z","shell.execute_reply.started":"2025-02-21T04:02:55.154120Z","shell.execute_reply":"2025-02-21T04:02:55.156816Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(CUSTOMER_EMBEDDINGS_DIR, \"customer_embeddings.pkl\")):\n    print(\"✅ customer_embeddings data already exists. Loading...\")\n\n    with open(os.path.join(CUSTOMER_EMBEDDINGS_DIR, \"customer_embeddings.pkl\"), \"rb\") as f:\n        customer_embeddings = pickle.load(f)\n\n    print(\"✅ customer_embeddings data loaded successfull\")\n\nelse :\n\n\n    # ✅ One-Hot Encode Categorical Features (Efficiently)\n    categorical_features = pd.get_dummies(customers[['Active', 'club_member_status']], dtype=np.float32)\n    \n    # ✅ Stack Numerical & Categorical Features\n    customer_features = np.hstack([\n        customers['normalized_age'].values.reshape(-1, 1),  # Ensure it's a column vector\n        categorical_features.values  # One-hot encoded categorical features\n    ])\n    \n    # ✅ Convert to PyTorch Tensor & Move to GPU\n    customer_features_tensor = torch.tensor(customer_features, dtype=torch.float32, device=device)\n    \n    # ✅ Define Customer Embedding Model\n    class CustomerEmbedding(nn.Module):\n        def __init__(self, input_dim, embedding_dim=64):\n            super(CustomerEmbedding, self).__init__()\n            self.model = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, embedding_dim)\n            )\n    \n        def forward(self, x):\n            return self.model(x)\n    \n    # ✅ Initialize Model & Move to GPU\n    embedding_dim = 64\n    model = CustomerEmbedding(input_dim=customer_features.shape[1], embedding_dim=embedding_dim).to(device)\n    \n    # ✅ Get Customer Embeddings\n    with torch.no_grad():\n        customer_embeddings = model(customer_features_tensor)  # Compute embeddings\n    \n    print(f\"✅ Customer embeddings generated. Shape: {customer_embeddings.shape}\")\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(CUSTOMER_EMBEDDINGS_DIR, \"customer_embeddings.pkl\"), \"wb\") as f:\n        pickle.dump(customer_embeddings, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(CUSTOMER_EMBEDDINGS_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(CUSTOMER_EMBEDDINGS_DIR):\n            zipf.write(os.path.join(CUSTOMER_EMBEDDINGS_DIR, file), arcname=file)\n\n    print(f\"✅ customer_embeddings completed! Saved as {CUSTOMER_EMBEDDINGS_ZIP_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:55.158223Z","iopub.execute_input":"2025-02-21T04:02:55.158449Z","iopub.status.idle":"2025-02-21T04:02:56.014394Z","shell.execute_reply.started":"2025-02-21T04:02:55.158431Z","shell.execute_reply":"2025-02-21T04:02:56.013630Z"}},"outputs":[{"name":"stdout","text":"✅ customer_embeddings data already exists. Loading...\n✅ customer_embeddings data loaded successfull\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"def sliced_wasserstein_distance(P, Q, num_projections=100):\n    \"\"\"\n    Computes the Sliced Wasserstein Distance (SWD) between two trajectory distributions.\n    Uses GPU-optimized random projections.\n    \"\"\"\n    assert P.shape == Q.shape, f\"Error: Shape mismatch! P.shape={P.shape}, Q.shape={Q.shape}\"\n\n    device = P.device  # Ensure computations are on the same device\n    proj_vectors = torch.randn((P.shape[1], num_projections), device=device)\n    proj_vectors = proj_vectors / torch.linalg.norm(proj_vectors, dim=0, keepdim=True)  # Normalize projections\n\n    P_proj = P @ proj_vectors\n    Q_proj = Q @ proj_vectors\n\n    return torch.mean(torch.abs(P_proj.sort(dim=0)[0] - Q_proj.sort(dim=0)[0]))  # Compute distance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:56.015163Z","iopub.execute_input":"2025-02-21T04:02:56.015441Z","iopub.status.idle":"2025-02-21T04:02:56.020674Z","shell.execute_reply.started":"2025-02-21T04:02:56.015419Z","shell.execute_reply":"2025-02-21T04:02:56.019838Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def geometric_distributed_sampling(ranks, rho=0.5, max_samples=300000):\n    \"\"\"\n    Optimized Geometric Distributed Sampling (GDS).\n    Ensures probabilities remain valid by normalizing ranks.\n    \"\"\"\n    assert ranks.numel() > 0, \"Error: Input `ranks` tensor is empty!\"\n\n    device = ranks.device  # Ensure all operations remain on the same device\n\n    # Sort ranks in descending order and keep top categories\n    sorted_indices = torch.argsort(ranks, descending=True)\n    max_categories = min(len(ranks), 16_000_000)\n    filtered_indices = sorted_indices[:max_categories]  \n\n    # Normalize ranks safely (avoid division by zero)\n    ranks_subset = ranks[filtered_indices]\n    min_rank, max_rank = ranks_subset.min(), ranks_subset.max()\n    range_rank = max_rank - min_rank + 1e-8  # Small epsilon for numerical stability\n\n    normalized_ranks = (ranks_subset - min_rank) / range_rank  # Scale between 0-1\n    normalized_ranks = torch.clamp(normalized_ranks, 0, 1)  # Ensure within bounds\n\n    # Compute probabilities\n    probabilities = torch.exp(-rho * normalized_ranks)\n    probabilities /= probabilities.sum()  # Normalize to sum = 1\n\n    # Sample indices\n    num_samples = min(max_samples, len(filtered_indices))\n    sampled_relative_indices = torch.multinomial(probabilities, num_samples=num_samples, replacement=False)\n\n    # Map back to original transaction indices\n    sampled_indices = filtered_indices[sampled_relative_indices]\n\n    return sampled_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:56.021532Z","iopub.execute_input":"2025-02-21T04:02:56.021810Z","iopub.status.idle":"2025-02-21T04:02:56.029680Z","shell.execute_reply.started":"2025-02-21T04:02:56.021790Z","shell.execute_reply":"2025-02-21T04:02:56.028988Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# ✅ **Improved GPU-Optimized Hard Negative Sampling**\ndef generate_hard_negatives(total_items, positive_sets, entity_type, product_indices, customer_indices, num_samples=10):\n    \"\"\"\n    Implements HARD NEGATIVE SAMPLING with IMPROVEMENTS:\n    - Ensures negatives are not in positive sets\n    - Uses \"harder\" negatives (more similar but not exact matches)\n    - Handles cases where there aren’t enough negatives\n    - Fully GPU-optimized 🚀\n    \"\"\"\n\n    negative_indices = torch.full((total_items, num_samples), -1, dtype=torch.long, device=\"cuda\")  # Preallocate memory\n\n    # ✅ Convert `positive_sets` to tensors **ONCE** (avoid repeated conversions)\n    exclusions = [torch.tensor(positive_sets.get(i, []), device=\"cuda\") if len(positive_sets.get(i, [])) > 0 else torch.tensor([], device=\"cuda\") for i in range(total_items)]\n\n    for i in range(total_items):\n        # ✅ Select valid negatives based on entity type\n        if entity_type == \"product\":\n            valid_negatives = product_indices[~torch.isin(product_indices, exclusions[i])]\n        elif entity_type in [\"customer\", \"customer_to_product\"]:\n            valid_negatives = customer_indices[~torch.isin(customer_indices, exclusions[i])]\n\n        # ✅ Ensure there are valid negatives available\n        if valid_negatives.numel() > 0:\n            # ✅ **Select \"harder\" negatives** (closer in embedding space but not exact matches)\n            random_negatives = valid_negatives[torch.randperm(len(valid_negatives))[:num_samples]]\n            \n            # ✅ Add \"harder\" negatives if possible\n            hard_negatives = valid_negatives[:num_samples]  # Take first `num_samples` as \"harder\" negatives\n\n            # ✅ Combine both types of negatives\n            combined_negatives = torch.cat((random_negatives, hard_negatives), dim=0).unique()[:num_samples]\n\n            # ✅ Assign to the preallocated tensor\n            negative_indices[i, :len(combined_negatives)] = combined_negatives\n\n    return negative_indices.cpu().numpy()  # ✅ Move back to CPU after computation\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:56.032283Z","iopub.execute_input":"2025-02-21T04:02:56.032471Z","iopub.status.idle":"2025-02-21T04:02:56.047771Z","shell.execute_reply.started":"2025-02-21T04:02:56.032454Z","shell.execute_reply":"2025-02-21T04:02:56.046982Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# # ✅ Convert Embeddings to Float32 and Move to GPU\n# if isinstance(text_embeddings, torch.Tensor):\n#     text_embeddings = text_embeddings.to(torch.float32).cpu().numpy()\n# if isinstance(image_embeddings, torch.Tensor):\n#     image_embeddings = image_embeddings.to(torch.float32).cpu().numpy()\n# if isinstance(customer_embeddings, torch.Tensor):\n#     customer_embeddings = customer_embeddings.to(torch.float32).cpu().numpy()\n\n# # ✅ Stack Article Embeddings Efficiently (GPU Optimized)\n# article_embedding_matrix = torch.tensor(np.hstack([text_embeddings, image_embeddings]), dtype=torch.float32, device=\"cuda\")\n# customer_embedding_matrix = torch.tensor(customer_embeddings, dtype=torch.float32, device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:56.049206Z","iopub.execute_input":"2025-02-21T04:02:56.049408Z","iopub.status.idle":"2025-02-21T04:02:56.063112Z","shell.execute_reply.started":"2025-02-21T04:02:56.049390Z","shell.execute_reply":"2025-02-21T04:02:56.062326Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# ✅ Define file paths for saving preprocessed data\n# DATA_SPLITTING_DIR = \"/kaggle/working/data_splitting\"\n# DATA_SPLITTING_ZIP_FILE = \"/kaggle/working/data_splitting.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(DATA_SPLITTING_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nDATA_SPLITTING_DIR = \"/kaggle/input/data-splitting-3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:56.063874Z","iopub.execute_input":"2025-02-21T04:02:56.064165Z","iopub.status.idle":"2025-02-21T04:02:56.075624Z","shell.execute_reply.started":"2025-02-21T04:02:56.064136Z","shell.execute_reply":"2025-02-21T04:02:56.074815Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\")):\n    print(\"✅ train_data data already exists. Loading...\")\n\n    # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"), \"rb\") as f:\n        train_data = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_data.pkl\"), \"rb\") as f:\n        val_data = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_data.pkl\"), \"rb\") as f:\n        test_data = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"), \"rb\") as f:\n        train_customers = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_customers.pkl\"), \"rb\") as f:\n        val_customers = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_customers.pkl\"), \"rb\") as f:\n        test_customers = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"), \"rb\") as f:\n        train_articles = pickle.load(f)\n\n        # Load extracted data\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_articles.pkl\"), \"rb\") as f:\n        val_articles = pickle.load(f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_articles.pkl\"), \"rb\") as f:\n        test_articles = pickle.load(f)\n\n    print(\"✅ SPLIT data loaded successfully!\")\n\nelse :\n\n    print(\"⚡ Extracting SPLIT for the first time...\")\n    \n    ### ✅ Step 3: Convert Transaction Date to Datetime (Efficiently)\n    transactions_filtered['t_dat'] = pd.to_datetime(transactions_filtered['t_dat'])\n    \n    # ✅ Sort transactions by date (ascending)\n    transactions_filtered.sort_values('t_dat', ascending=True, inplace=True)\n    \n    ### ✅ Step 4: Compute Train, Validation, and Test Cutoffs\n    train_cutoff, val_cutoff, test_cutoff = transactions_filtered['t_dat'].quantile([0.75, 0.875, 1.0]).values\n    \n    ### ✅ Step 5: Assign Ranks for GDS Efficiently\n    transactions_filtered[\"rank\"] = transactions_filtered['t_dat'].rank(method=\"first\", ascending=True)\n    \n    # ✅ Convert ranks to GPU Tensor\n    ranks_tensor = torch.tensor(transactions_filtered[\"rank\"].values, dtype=torch.float32, device=\"cuda\")\n    \n    # ✅ Sample transactions using Geometric Distributed Sampling (GDS)\n    sampled_indices = geometric_distributed_sampling(ranks_tensor, rho=0.5, max_samples=300000)\n    \n    # ✅ Efficiently extract sampled transactions\n    transactions_filtered_sampled = transactions_filtered.loc[sampled_indices.cpu().numpy()].reset_index(drop=True)\n    \n    ### ✅ Step 7: Train/Validation/Test Splitting (Optimized)\n    train_mask = transactions_filtered_sampled['t_dat'] <= train_cutoff\n    val_mask = (transactions_filtered_sampled['t_dat'] > train_cutoff) & (transactions_filtered_sampled['t_dat'] <= val_cutoff)\n    test_mask = transactions_filtered_sampled['t_dat'] > val_cutoff\n    \n    train_data, val_data, test_data = (\n        transactions_filtered_sampled[train_mask], \n        transactions_filtered_sampled[val_mask], \n        transactions_filtered_sampled[test_mask]\n    )\n    \n    ### ✅ Step 8: Cold Start Customer Handling (Optimized)\n    transaction_customers = set(transactions_filtered['customer_id'])\n    cold_start_customers = customers[~customers['customer_id'].isin(transaction_customers)]\n    \n    # ✅ Split cold start customers into train, validation, and test\n    train_cold_start, temp_cold_start = train_test_split(\n        cold_start_customers, test_size=0.25, random_state=42\n    )\n    val_cold_start, test_cold_start = train_test_split(\n        temp_cold_start, test_size=0.5, random_state=42\n    )\n    \n    # ✅ Merge cold-start customers with train/val/test sets\n    train_data = pd.concat([train_data, train_cold_start], ignore_index=True)\n    val_data = pd.concat([val_data, val_cold_start], ignore_index=True)\n    test_data = pd.concat([test_data, test_cold_start], ignore_index=True)\n    \n    # ✅ Filter customers appearing in train/val/test sets\n    train_customers = customers[customers['customer_id'].isin(train_data['customer_id'])]\n    val_customers = customers[customers['customer_id'].isin(val_data['customer_id'])]\n    test_customers = customers[customers['customer_id'].isin(test_data['customer_id'])]\n    \n    # ✅ Filter articles appearing in train/val/test sets\n    train_articles = articles_with_images[articles_with_images['article_id'].isin(train_data['article_id'])]\n    val_articles = articles_with_images[articles_with_images['article_id'].isin(val_data['article_id'])]\n    test_articles = articles_with_images[articles_with_images['article_id'].isin(test_data['article_id'])]\n    \n    print(\"✅ Data split completed successfully! 🚀\")\n\n\n    \n    # ✅ Convert IDs to Strings for Consistency\n    train_customers['customer_id'] = train_customers['customer_id'].astype(str)\n    train_articles['article_id'] = train_articles['article_id'].astype(str)\n    \n    # ✅ Create Fast Index Mappings\n    article_to_index = {str(article_id): idx for idx, article_id in enumerate(train_articles['article_id'])}\n    customer_to_index = {str(customer_id): idx for idx, customer_id in enumerate(train_customers['customer_id'])}\n    \n    # ✅ Initialize Positive Index Dictionaries\n    customer_positive_indices = {str(cid): [] for cid in train_customers['customer_id']}\n    article_positive_indices = {str(aid): [] for aid in train_articles['article_id']}\n    customer_to_product_positive_indices = {str(cid): [] for cid in train_customers['customer_id']}  # New\n    \n    # ✅ Populate Positive Indices Using `train_transactions`\n    for _, row in train_data.iterrows():\n        cid, aid = str(row['customer_id']), str(row['article_id'])\n    \n        if cid in customer_to_index and aid in article_to_index:  # Ensure valid entries\n            customer_positive_indices[cid].append(article_to_index[aid])  # Customer's purchased articles\n            article_positive_indices[aid].append(customer_to_index[cid])  # Articles linked to customers\n    \n    # ✅ Generate Customer-to-Product Positive Indices\n    for aid in train_data['article_id'].astype(str).unique():\n        customer_list = train_data.loc[train_data['article_id'] == aid, 'customer_id'].astype(str).unique()\n        mapped_customers = [customer_to_index[c] for c in customer_list if c in customer_to_index]\n    \n        for cid in mapped_customers:\n            if cid in customer_to_product_positive_indices:  \n                customer_to_product_positive_indices[cid].extend(\n                    [c for c in mapped_customers if c != cid][:10]\n                )\n            else:\n                customer_to_product_positive_indices[cid] = []  # Assign empty list\n    \n    # ✅ Map Positive Indices Efficiently\n    train_customers['positive_indices'] = train_customers['customer_id'].map(customer_positive_indices)\n    train_articles['positive_indices'] = train_articles['article_id'].map(article_positive_indices)\n    train_customers['customer_to_product_positive_indices'] = train_customers['customer_id'].map(customer_to_product_positive_indices)\n    \n    print(\"✅ Positive indices generated successfully!\")\n\n    # ✅ **Define Product and Customer Indices**\n    product_indices = torch.arange(len(train_articles), device=\"cuda\")\n    customer_indices = torch.arange(len(train_customers), device=\"cuda\")\n    \n    \n    # ✅ **Generate Negative Indices for Products (GPU-Accelerated)**\n    print(\"⚡ Generating Hard Negative Samples for Products...\")\n    train_articles['negative_indices'] = list(generate_hard_negatives(\n        len(train_articles), train_articles['positive_indices'], \"product\", product_indices, customer_indices\n    ))\n    print(\"✅ Done!\")\n    \n    # ✅ **Generate Negative Indices for Customers (GPU-Accelerated)**\n    print(\"⚡ Generating Hard Negative Samples for Customers...\")\n    train_customers['negative_indices'] = list(generate_hard_negatives(\n        len(train_customers), train_customers['positive_indices'], \"customer\", product_indices, customer_indices\n    ))\n    print(\"✅ Done!\")\n    \n    # ✅ **Generate Negative Indices for Customer-to-Product Interactions (GPU-Accelerated)**\n    print(\"⚡ Generating Hard Negative Samples for Customer-to-Product Interactions...\")\n    train_customers['negative_customer_to_product_indices'] = list(generate_hard_negatives(\n        len(train_customers), train_customers['customer_to_product_positive_indices'], \"customer_to_product\", product_indices, customer_indices\n    ))\n    print(\"✅ Done!\")\n    \n    print(\"🚀 Hard Negative Sampling Completed with Full GPU Optimization! ✅\")\n\n\n    # ✅ Save preprocessed data locally\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_data.pkl\"), \"wb\") as f:\n        pickle.dump(train_data, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_data.pkl\"), \"wb\") as f:\n        pickle.dump(val_data, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_data.pkl\"), \"wb\") as f:\n        pickle.dump(test_data, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_customers.pkl\"), \"wb\") as f:\n        pickle.dump(train_customers, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_customers.pkl\"), \"wb\") as f:\n        pickle.dump(val_customers, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_customers.pkl\"), \"wb\") as f:\n        pickle.dump(test_customers, f)\n\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"train_articles.pkl\"), \"wb\") as f:\n        pickle.dump(train_articles, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"val_articles.pkl\"), \"wb\") as f:\n        pickle.dump(val_articles, f)\n\n    with open(os.path.join(DATA_SPLITTING_DIR, \"test_articles.pkl\"), \"wb\") as f:\n        pickle.dump(test_articles, f)\n    \n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(DATA_SPLITTING_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(DATA_SPLITTING_DIR):\n            zipf.write(os.path.join(DATA_SPLITTING_DIR, file), arcname=file)\n\n    print(f\"✅ data splitting completed! Saved as {DATA_SPLITTING_ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:56.076553Z","iopub.execute_input":"2025-02-21T04:02:56.076827Z","iopub.status.idle":"2025-02-21T04:02:58.113246Z","shell.execute_reply.started":"2025-02-21T04:02:56.076799Z","shell.execute_reply":"2025-02-21T04:02:58.112412Z"}},"outputs":[{"name":"stdout","text":"✅ train_data data already exists. Loading...\n✅ SPLIT data loaded successfully!\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"def filter_and_align_embeddings(data, articles, customers):\n    \"\"\" Ensures that price, text, image, and customer embeddings are aligned properly. \"\"\"\n    \n    # ✅ 1️⃣ Filter valid articles and customers\n    valid_articles = articles.loc[articles['article_id'].isin(data['article_id'])].reset_index(drop=True)\n    valid_customers = customers.loc[customers['customer_id'].isin(data['customer_id'])].reset_index(drop=True)\n\n    # ✅ 2️⃣ Extract & Align Price Values\n    price_values = (\n        data.drop_duplicates(subset=[\"article_id\"])\n        .set_index(\"article_id\")\n        .reindex(valid_articles[\"article_id\"])[\"price\"]\n        .fillna(0)  # Handle missing prices\n        .values.reshape(-1, 1)\n        .astype(np.float32)\n    )\n\n    # ✅ 3️⃣ Extract Text, Image, and Customer Embeddings (REMOVE `.cpu()` BECAUSE THEY ARE NUMPY ARRAYS)\n    text_embeds = text_embeddings[valid_articles.index]  # ✅ FIXED\n    image_embeds = image_embeddings[valid_articles.index]  # ✅ FIXED\n    customer_embeds = customer_embeddings[valid_customers.index]  # ✅ FIXED\n\n    # ✅ 4️⃣ Ensure Feature Matrices Have the Same Row Count\n    min_rows = min(len(price_values), len(text_embeds), len(image_embeds), len(customer_embeds))\n    \n    # ✅ 5️⃣ Stack Features Efficiently\n    final_features = np.hstack([\n        price_values[:min_rows],  \n        text_embeds[:min_rows],  \n        image_embeds[:min_rows],  \n        customer_embeds[:min_rows]\n    ])\n\n    print(f\"✅ Features Extracted: {final_features.shape}\")\n    return final_features\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.114041Z","iopub.execute_input":"2025-02-21T04:02:58.114291Z","iopub.status.idle":"2025-02-21T04:02:58.120133Z","shell.execute_reply.started":"2025-02-21T04:02:58.114270Z","shell.execute_reply":"2025-02-21T04:02:58.119383Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# ✅ Define file paths for saving preprocessed data\n# FEATURES_DIR = \"/kaggle/working/features\"\n# FEATURES_ZIP_FILE = \"/kaggle/working/features.zip\"  # Final zipped archive\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(FEATURES_DIR, exist_ok=True)\n\n# ✅ Correct path (inside `/kaggle/input/preprocessed-data/`)\nFEATURES_DIR = \"/kaggle/input/features-2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.120894Z","iopub.execute_input":"2025-02-21T04:02:58.121098Z","iopub.status.idle":"2025-02-21T04:02:58.136603Z","shell.execute_reply.started":"2025-02-21T04:02:58.121080Z","shell.execute_reply":"2025-02-21T04:02:58.135736Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# ✅ Extract ZIP only if the preprocessed files don't already exist\nif os.path.exists(os.path.join(FEATURES_DIR, \"train_features.pkl\")):\n    print(\"✅ features are already exists. Loading...\")\n\n    # Load extracted data\n    with open(os.path.join(FEATURES_DIR, \"train_features.pkl\"), \"rb\") as f:\n        train_features = pickle.load(f)\n\n    with open(os.path.join(FEATURES_DIR, \"val_features.pkl\"), \"rb\") as f:\n        val_features = pickle.load(f)\n\n    with open(os.path.join(FEATURES_DIR, \"test_features.pkl\"), \"rb\") as f:\n        test_features = pickle.load(f)\n\n    print(\"✅ features loaded successfully!\")\n\nelse :\n\n    # ✅ Extract & Align Features for Train/Validation/Test Sets\n    print(\"⚡ Extracting features for the first time...\")\n    \n    train_features = filter_and_align_embeddings(train_data, train_articles, train_customers)\n    val_features = filter_and_align_embeddings(val_data, val_articles, val_customers)\n    test_features = filter_and_align_embeddings(test_data, test_articles, test_customers)\n    \n    print(\"✅ Feature Extraction Completed! 🚀\")\n          # ✅ Save preprocessed data locally\n    with open(os.path.join(FEATURES_DIR, \"train_features.pkl\"), \"wb\") as f:\n        pickle.dump(train_features, f)\n\n    with open(os.path.join(FEATURES_DIR, \"val_features.pkl\"), \"wb\") as f:\n        pickle.dump(val_features, f)\n\n    with open(os.path.join(FEATURES_DIR, \"test_features.pkl\"), \"wb\") as f:\n        pickle.dump(test_features, f)\n\n\n    # ✅ Create a ZIP archive containing all preprocessed files\n    with zipfile.ZipFile(FEATURES_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(FEATURES_DIR):\n            zipf.write(os.path.join(FEATURES_DIR, file), arcname=file)\n\n    print(f\"✅ features completed! Saved as {FEATURES_ZIP_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.137523Z","iopub.execute_input":"2025-02-21T04:02:58.137779Z","iopub.status.idle":"2025-02-21T04:02:58.544417Z","shell.execute_reply.started":"2025-02-21T04:02:58.137748Z","shell.execute_reply":"2025-02-21T04:02:58.543474Z"}},"outputs":[{"name":"stdout","text":"✅ features are already exists. Loading...\n✅ features loaded successfully!\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# # ✅ Define directory and file paths\n# GRAPH_DIR = \"/kaggle/working/graph_data\"\n# GRAPH_ZIP_FILE = \"/kaggle/working/graph_data.zip\"\n\n# # ✅ Create the directory if it doesn't exist\n# os.makedirs(GRAPH_DIR, exist_ok=True)\n\n# ✅ Define Graph Load Path\nGRAPH_DIR = \"/kaggle/input/graph-data-2\"\nGRAPH_FILE = os.path.join(GRAPH_DIR, \"pr_graph.pt\")  # Define the PR-Graph file path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.545212Z","iopub.execute_input":"2025-02-21T04:02:58.545450Z","iopub.status.idle":"2025-02-21T04:02:58.549290Z","shell.execute_reply.started":"2025-02-21T04:02:58.545431Z","shell.execute_reply":"2025-02-21T04:02:58.548313Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# ✅ Check if precomputed PR Graph exists\nif os.path.exists(GRAPH_FILE):\n    print(\"✅ Loading PR Graph from Disk...\")\n\n    # ✅ Load PR-Graph and associated embeddings\n    graph_data = torch.load(GRAPH_FILE, map_location=device)\n    graph = graph_data[\"graph\"]\n\n    print(\"✅ PR-Graph and embeddings loaded successfully! 🚀\")\nelse:\n    print(\"⚠️ No saved PR Graph found! Please generate it first.\")\n\n    if isinstance(customer_embeddings, torch.Tensor):\n        customer_embeddings = customer_embeddings.to(torch.float32).cpu().numpy()\n\n    article_to_index = {article_id: idx for idx, article_id in enumerate(train_articles['article_id'].values)}\n    customer_to_index = {customer_id: idx for idx, customer_id in enumerate(train_customers['customer_id'].values)}\n    \n    # ✅ **Construct PR-Graph and Edge Indices Efficiently**\n    print(\"⚡ Constructing PR-Graph and Edge Index...\")\n    \n    train_data.dropna(subset=['customer_id', 'article_id'], inplace=True)\n    \n    customer_to_product_edges = torch.tensor([\n        (customer_to_index[u], article_to_index[v])\n        for u, v in train_data[['customer_id', 'article_id']].values\n        if u in customer_to_index and v in article_to_index\n    ], dtype=torch.long, device=device).T\n    \n    print(\"✅ Customer-to-Product Edge Construction Done\")\n    \n    # ✅ **Construct Product-to-Product Co-Purchase Graph**\n    product_to_product_edges = []\n    co_purchase_pairs = train_data.groupby('customer_id')['article_id'].apply(list)\n    for articles_list in co_purchase_pairs:\n        product_to_product_edges.extend([(article_to_index[articles_list[i]], article_to_index[articles_list[j]]) \n                                         for i in range(len(articles_list)) for j in range(i + 1, len(articles_list))])\n    \n    product_to_product_edges = torch.tensor(product_to_product_edges, dtype=torch.long, device=device).T\n    print(\"✅ Product-to-Product Edge Construction Done\")\n    \n    # ✅ **Customer-to-Customer Similarity Graph Using FAISS**\n    num_customers = len(train_customers)  # Total customers\n    index = faiss.IndexFlatL2(customer_embeddings.shape[1])\n    index.add(customer_embeddings)\n    k = min(10, num_customers)  # Ensure `k` is within valid range\n    \n    # ✅ Perform FAISS search\n    D, I = index.search(customer_embeddings, k)\n    print(\"✅ FAISS search completed.\")\n    \n    # ✅ Ensure FAISS indices are valid before using them\n    customer_to_customer_edges = []\n    \n    for i in range(num_customers):  # Loop through each customer\n        for j in range(1, k):  # Start from 1 to avoid self-loop\n            if I[i][j] < 0 or I[i][j] >= num_customers:  # Ensure index is within range\n                continue  # Skip invalid indices\n    \n            src_customer = train_customers.iloc[i]['customer_id']  # Source customer\n            tgt_customer = train_customers.iloc[I[i][j]]['customer_id']  # Target customer\n    \n            # ✅ Ensure both customers exist in the customer_to_index mapping\n            if src_customer in customer_to_index and tgt_customer in customer_to_index:\n                customer_to_customer_edges.append(\n                    (customer_to_index[src_customer], customer_to_index[tgt_customer])\n                )\n    \n    # ✅ Convert edges to PyTorch tensor\n    if customer_to_customer_edges:\n        customer_to_customer_edges = torch.tensor(customer_to_customer_edges, dtype=torch.long, device=device).T\n    else:\n        customer_to_customer_edges = torch.empty((2, 0), dtype=torch.long, device=device)  # Handle empty case\n    \n    print(\"✅ Customer-to-Customer Similarity Graph Done\")\n    \n    # ✅ **Ensure Edges Are on GPU**\n    customer_to_product_edges = customer_to_product_edges.to(device)\n    product_to_product_edges = product_to_product_edges.to(device)\n    customer_to_customer_edges = customer_to_customer_edges.to(device)\n    \n    # ✅ **Construct PyTorch Geometric Heterogeneous Graph**\n    graph = HeteroData()\n    \n    # ✅ **Define Graph Structure**\n    graph['customer', 'buys', 'product'].edge_index = customer_to_product_edges  # No self-loops needed\n    graph['product', 'co_purchased_with', 'product'].edge_index = product_to_product_edges\n    graph['customer', 'similar_to', 'customer'].edge_index = customer_to_customer_edges\n    \n    # ✅ **Add Self-Loops Only Where Needed**\n    graph['customer', 'similar_to', 'customer'].edge_index = add_self_loops(graph['customer', 'similar_to', 'customer'].edge_index)[0]\n    graph['product', 'co_purchased_with', 'product'].edge_index = add_self_loops(graph['product', 'co_purchased_with', 'product'].edge_index)[0]\n    \n    # ❌ **No self-loops for customer-to-product (bipartite graph)**\n    # graph['customer', 'buys', 'product'].edge_index = add_self_loops(graph['customer', 'buys', 'product'].edge_index)[0]  # ❌ REMOVE this\n    \n    # ✅ **Move Graph to GPU**\n    graph = graph.to(device)\n     \n    print(\"✅ PR-Graph Constructed Successfully 🚀\")\n    \n    # ✅ Save PR-Graph as a Dictionary (Recommended by PyG)\n    graph_data = {\n        \"graph\": graph,  # Full graph structure\n    }\n    \n    \n    # ✅ Save the entire graph in one file\n    torch.save(graph_data, GRAPH_FILE)\n    print(f\"✅ PR-Graph and embeddings saved at {GRAPH_FILE}\")\n    \n    # ✅ Create ZIP Archive for Future Use\n    with zipfile.ZipFile(GRAPH_ZIP_FILE, 'w') as zipf:\n        for file in os.listdir(GRAPH_DIR):\n            zipf.write(os.path.join(GRAPH_DIR, file), arcname=file)\n    \n    print(f\"✅ PR-Graph and Edge Index processing completed! Saved as {GRAPH_ZIP_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.550216Z","iopub.execute_input":"2025-02-21T04:02:58.550495Z","iopub.status.idle":"2025-02-21T04:02:58.594729Z","shell.execute_reply.started":"2025-02-21T04:02:58.550464Z","shell.execute_reply":"2025-02-21T04:02:58.593875Z"}},"outputs":[{"name":"stdout","text":"✅ Loading PR Graph from Disk...\n✅ PR-Graph and embeddings loaded successfully! 🚀\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-65-f6bbd1a53dc6>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load(GRAPH_FILE, map_location=device)\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"print(train_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:34:19.390250Z","iopub.execute_input":"2025-02-21T04:34:19.390547Z","iopub.status.idle":"2025-02-21T04:34:19.425077Z","shell.execute_reply.started":"2025-02-21T04:34:19.390525Z","shell.execute_reply":"2025-02-21T04:34:19.423906Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-98e58a4635bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     def backward(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0mguard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DisableFuncTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    623\u001b[0m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             return torch.cat(\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             )\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":78},{"cell_type":"code","source":"# ✅ **Define Checkpoint Path**\nCHECKPOINT_FILE = \"/kaggle/working/checkpoint.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.601713Z","iopub.execute_input":"2025-02-21T04:02:58.602034Z","iopub.status.idle":"2025-02-21T04:02:58.612852Z","shell.execute_reply.started":"2025-02-21T04:02:58.602005Z","shell.execute_reply":"2025-02-21T04:02:58.612210Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# ✅ **Save Model Checkpoint**\ndef save_checkpoint(model, optimizer, epoch, filename=CHECKPOINT_FILE):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    torch.save(checkpoint, filename)\n    print(f\"✅ Model checkpoint saved at epoch {epoch}\")\n\n# ✅ **Load Model Checkpoint**\ndef load_checkpoint(filename, model, optimizer=None):\n    if os.path.exists(filename):\n        print(\"✅ Loading checkpoint...\")\n        checkpoint = torch.load(filename, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        if optimizer:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(f\"✅ Resuming training from epoch {checkpoint['epoch'] + 1}\")\n        return checkpoint['epoch'] + 1  # Resume from the next epoch\n    else:\n        print(\"⚡ No checkpoint found. Starting from scratch.\")\n        return 0  # Start from epoch 0 if no checkpoint exists","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.613719Z","iopub.execute_input":"2025-02-21T04:02:58.614022Z","iopub.status.idle":"2025-02-21T04:02:58.624344Z","shell.execute_reply.started":"2025-02-21T04:02:58.613984Z","shell.execute_reply":"2025-02-21T04:02:58.623642Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"class MultiRelationGCNLayer(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(MultiRelationGCNLayer, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)  # Linear transformation\n        self.edge_weight = nn.Parameter(torch.Tensor(3))  # Separate weights for each edge type\n\n        # Initialize weights\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.constant_(self.edge_weight, 1.0)\n\n    def forward(self, features, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index):\n        row_cp, col_cp = customer_to_product_edge_index  # Customer → Product\n        row_pp, col_pp = product_to_product_edge_index  # Product ↔ Product\n        row_cc, col_cc = customer_to_customer_edge_index  # Customer ↔ Customer\n        \n        # Compute degree normalization (avoid division by zero)\n        degree = torch.zeros(features.size(0), device=features.device)\n\n        if torch.min(col_cc) < 0:\n            print(\"Error: Negative indices found in col_cc!\")\n        if torch.max(col_cc) >= degree.shape[0]:\n            print(\"Error: col_cc contains out-of-bounds indices!\")\n\n        \n        degree.index_add_(0, col_cp, torch.ones_like(col_cp, dtype=torch.float))\n        degree.index_add_(0, col_pp, torch.ones_like(col_pp, dtype=torch.float))\n        degree.index_add_(0, col_cc, torch.ones_like(col_cc, dtype=torch.float))\n        degree = degree.clamp(min=1).pow(-0.5)\n\n        \n\n        # Message Passing for Each Relation Type\n        agg_cp = torch.zeros_like(features)\n        agg_pp = torch.zeros_like(features)\n        agg_cc = torch.zeros_like(features)\n\n        agg_cp.index_add_(0, row_cp, features[col_cp] * degree[col_cp].view(-1, 1))\n        agg_pp.index_add_(0, row_pp, features[col_pp] * degree[col_pp].view(-1, 1))\n        agg_cc.index_add_(0, row_cc, features[col_cc] * degree[col_cc].view(-1, 1))\n\n        # Weighted Aggregation\n        aggregated = (\n            self.edge_weight[0] * agg_cp +\n            self.edge_weight[1] * agg_pp +\n            self.edge_weight[2] * agg_cc\n        ) / 3  # Normalize across relations\n\n        return F.relu(self.linear(aggregated))  # Apply transformation & ReLU activation\n\n\nclass ImprovedCSTAR(nn.Module):\n    def __init__(self, input_dim, embedding_dim, num_layers, dropout):\n        super(ImprovedCSTAR, self).__init__()\n\n        self.embedding_layer = nn.Linear(input_dim, embedding_dim)  # Learn feature transformation\n        self.gcn_layers = nn.ModuleList([MultiRelationGCNLayer(embedding_dim, embedding_dim) for _ in range(num_layers)])\n        self.dropout = dropout\n        self.reference_embedding = nn.Parameter(torch.randn(embedding_dim, dtype=torch.float32))  # Global trajectory reference\n\n    def forward(self, features, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index):\n        embeddings = self.embedding_layer(features.to(torch.float32))  # Convert to float32 & transform\n        embeddings = F.dropout(embeddings, p=self.dropout, training=self.training)\n\n        for gcn_layer in self.gcn_layers:\n            embeddings = gcn_layer(embeddings, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index)\n            embeddings = F.dropout(embeddings, p=self.dropout, training=self.training)\n\n        return embeddings\n\n\ndef inter_trajectory_loss(embeddings, reference_embedding):\n    \"\"\"\n    Computes inter-trajectory loss to ensure different customer trajectories are correctly spaced.\n    \"\"\"\n    return sliced_wasserstein_distance(embeddings, reference_embedding)\n\n\ndef intra_trajectory_loss(embeddings, positive_indices, negative_indices):\n    \"\"\"\n    Intra-trajectory loss to ensure a trajectory is closer to its positive interactions than negatives.\n    \"\"\"\n    pos_indices = torch.as_tensor(positive_indices, dtype=torch.long, device=embeddings.device)\n    neg_indices = torch.as_tensor(negative_indices, dtype=torch.long, device=embeddings.device)\n\n    if pos_indices.numel() == 0 or neg_indices.numel() == 0:\n        return torch.tensor(0.0, requires_grad=True, device=embeddings.device)\n\n    pos_scores = torch.einsum('ij,ij->i', embeddings[pos_indices], embeddings[pos_indices])\n    neg_scores = torch.einsum('ij,ij->i', embeddings[neg_indices], embeddings[neg_indices])\n\n    margin = 1.0 + 0.1 * torch.std(embeddings)\n    return F.relu(neg_scores + margin - pos_scores).mean()\n\ndef compute_loss(embeddings, model):\n    \"\"\"\n    Computes loss for product-to-product, customer-to-customer, and customer-to-product.\n    Uses stored positive & negative indices.\n    \"\"\"\n\n    # ✅ **Move Positive & Negative Indices to GPU (Precompute for Faster Batching)**\n    train_articles['positive_indices'] = train_articles['positive_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n    train_articles['negative_indices'] = train_articles['negative_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n\n    train_customers['customer_to_product_positive_indices'] = train_customers['customer_to_product_positive_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n    train_customers['negative_customer_to_product_indices'] = train_customers['negative_customer_to_product_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n\n    train_customers['positive_indices'] = train_customers['positive_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n    train_customers['negative_indices'] = train_customers['negative_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n\n    total_loss = 0.0\n\n    # ✅ Compute Product-to-Product Loss\n    if 'positive_indices' in train_articles and 'negative_indices' in train_articles:\n        total_loss += intra_trajectory_loss(\n            embeddings, \n            torch.cat(train_articles['positive_indices'].tolist()), \n            torch.cat(train_articles['negative_indices'].tolist())\n        )\n\n    # ✅ Compute Customer-to-Customer Loss\n    if 'positive_indices' in train_customers and 'negative_indices' in train_customers:\n        total_loss += intra_trajectory_loss(\n            embeddings, \n            torch.cat(train_customers['positive_indices'].tolist()), \n            torch.cat(train_customers['negative_indices'].tolist())\n        )\n\n    # ✅ Compute Customer-to-Product Loss\n    if 'customer_to_product_positive_indices' in train_customers and 'negative_customer_to_product_indices' in train_customers:\n        total_loss += intra_trajectory_loss(\n            embeddings, \n            torch.cat(train_customers['customer_to_product_positive_indices'].tolist()), \n            torch.cat(train_customers['negative_customer_to_product_indices'].tolist())\n        )\n\n    # ✅ Add Inter-Trajectory Loss\n    total_loss += inter_trajectory_loss(embeddings, model.reference_embedding)\n\n    return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.625364Z","iopub.execute_input":"2025-02-21T04:02:58.625644Z","iopub.status.idle":"2025-02-21T04:02:58.642693Z","shell.execute_reply.started":"2025-02-21T04:02:58.625617Z","shell.execute_reply":"2025-02-21T04:02:58.641858Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# ✅ **Move Edge Indices to GPU**\ncustomer_to_product_edge_index = graph['customer', 'buys', 'product'].edge_index.to(device)\nproduct_to_product_edge_index = graph['product', 'co_purchased_with', 'product'].edge_index.to(device)\ncustomer_to_customer_edge_index = graph['customer', 'similar_to', 'customer'].edge_index.to(device)\n\n# ✅ Ensure valid edges for customer-to-product\nnum_products = len(train_articles)\nnum_customers = len(train_customers)\n\n\ndef filter_invalid_edges(edge_index, num_nodes):\n    \"\"\"Removes out-of-bound edges from edge_index\"\"\"\n    mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n    return edge_index[:, mask]\n\n# ✅ Validate Edge Indices Before Using Them\ncustomer_to_product_edge_index = filter_invalid_edges(graph['customer', 'buys', 'product'].edge_index, num_customers)\nproduct_to_product_edge_index = filter_invalid_edges(graph['product', 'co_purchased_with', 'product'].edge_index, num_products)\ncustomer_to_customer_edge_index = filter_invalid_edges(graph['customer', 'similar_to', 'customer'].edge_index, num_customers)\n\nprint(f\"Filtered Edge Counts: Customer-Product={customer_to_product_edge_index.shape[1]}, \"\n      f\"Product-Product={product_to_product_edge_index.shape[1]}, \"\n      f\"Customer-Customer={customer_to_customer_edge_index.shape[1]}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.643511Z","iopub.execute_input":"2025-02-21T04:02:58.643781Z","iopub.status.idle":"2025-02-21T04:02:58.660680Z","shell.execute_reply.started":"2025-02-21T04:02:58.643759Z","shell.execute_reply":"2025-02-21T04:02:58.659843Z"}},"outputs":[{"name":"stdout","text":"Filtered Edge Counts: Customer-Product=170598, Product-Product=78023, Customer-Customer=1407970\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# # ✅ Use a smaller dataset for quick testing\n# train_data = train_data.sample(n=10000, random_state=42)  \n# val_data = val_data.sample(n=2000, random_state=42)\n# test_data = train_data.sample(n=2000, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.661455Z","iopub.execute_input":"2025-02-21T04:02:58.661682Z","iopub.status.idle":"2025-02-21T04:02:58.671642Z","shell.execute_reply.started":"2025-02-21T04:02:58.661652Z","shell.execute_reply":"2025-02-21T04:02:58.670853Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# ✅ **Train Model with Early Stopping & Checkpoints**\ndef train_cstar_model(model, optimizer, num_epochs, patience, train_features, val_features):\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n    start_epoch = load_checkpoint(CHECKPOINT_FILE, model, optimizer)  # ✅ Start from checkpoint\n\n    model.train()\n    train_features = train_features.to(torch.float32).to(device)\n    val_features = val_features.to(torch.float32).to(device)\n\n    for epoch in range(start_epoch, num_epochs):\n        optimizer.zero_grad()\n\n        embeddings = model(train_features, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index)\n        loss = compute_loss(embeddings, model)\n\n        loss.backward()\n        optimizer.step()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {loss.item():.4f}\")\n\n        # Validation Phase\n        model.eval()\n        with torch.no_grad():\n            val_embeddings = model(val_features, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index)\n            val_loss = compute_loss(val_embeddings, model)\n\n        print(f\"Validation Loss: {val_loss.item():.4f}\")\n\n\n        # ✅ **Early Stopping & Checkpointing**\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            patience_counter = 0\n            save_checkpoint(model, optimizer, epoch)  # ✅ Save Best Model\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"✅ Early stopping triggered!\")\n                break\n\n    # ✅ **Load Best Model Before Returning**\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.672441Z","iopub.execute_input":"2025-02-21T04:02:58.672706Z","iopub.status.idle":"2025-02-21T04:02:58.686229Z","shell.execute_reply.started":"2025-02-21T04:02:58.672687Z","shell.execute_reply":"2025-02-21T04:02:58.685469Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"num_customers = graph['customer', 'similar_to', 'customer'].edge_index.max().item() + 1 if graph['customer', 'similar_to', 'customer'].edge_index.numel() > 0 else 0\nnum_products = graph['product', 'co_purchased_with', 'product'].edge_index.max().item() + 1 if graph['product', 'co_purchased_with', 'product'].edge_index.numel() > 0 else 0\n\ngraph['customer'].num_nodes = num_customers\ngraph['product'].num_nodes = num_products\n\ntotal_nodes = num_customers + num_products\n\nprint(f\"Total Customers: {num_customers}\")\nprint(f\"Total Products: {num_products}\")\nprint(f\"Total Nodes in the Graph: {total_nodes}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.686845Z","iopub.execute_input":"2025-02-21T04:02:58.687066Z","iopub.status.idle":"2025-02-21T04:02:58.698091Z","shell.execute_reply.started":"2025-02-21T04:02:58.687048Z","shell.execute_reply":"2025-02-21T04:02:58.697377Z"}},"outputs":[{"name":"stdout","text":"Total Customers: 141059\nTotal Products: 25484\nTotal Nodes in the Graph: 166543\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"if torch.max(graph['customer', 'buys', 'product'].edge_index) >= total_nodes:\n    print(\"Error: customer_to_product_edge_index contains out-of-bounds indices\")\nif torch.max(graph['product', 'co_purchased_with', 'product'].edge_index) >= total_nodes:\n    print(\"Error: product_to_product_edge_index contains out-of-bounds indices\")\nif torch.max(graph['customer', 'similar_to', 'customer'].edge_index) >= total_nodes:\n    print(\"Error: customer_to_customer_edge_index contains out-of-bounds indices\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.698891Z","iopub.execute_input":"2025-02-21T04:02:58.699120Z","iopub.status.idle":"2025-02-21T04:02:58.711578Z","shell.execute_reply.started":"2025-02-21T04:02:58.699103Z","shell.execute_reply":"2025-02-21T04:02:58.710974Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"customer_to_product_edge_index = customer_to_product_edge_index.to('cuda')\nproduct_to_product_edge_index = product_to_product_edge_index.to('cuda')\ncustomer_to_customer_edge_index = customer_to_customer_edge_index.to('cuda')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:02:58.714311Z","iopub.execute_input":"2025-02-21T04:02:58.714538Z","iopub.status.idle":"2025-02-21T04:02:58.720033Z","shell.execute_reply.started":"2025-02-21T04:02:58.714519Z","shell.execute_reply":"2025-02-21T04:02:58.719258Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# ✅ Force clear CUDA memory before re-running\ntorch.cuda.empty_cache()\n\nif isinstance(text_embeddings, torch.Tensor):\n    text_embeddings = text_embeddings.to(torch.float32).cpu().numpy()\nif isinstance(image_embeddings, torch.Tensor):\n    image_embeddings = image_embeddings.to(torch.float32).cpu().numpy()\nif isinstance(customer_embeddings, torch.Tensor):\n    customer_embeddings = customer_embeddings.to(torch.float32).cpu().numpy()\n\n\n# ✅ Ensure shape consistency\nprint(f\"Text Embedding Shape: {text_embeddings.shape}\")\nprint(f\"Image Embedding Shape: {image_embeddings.shape}\")\nprint(f\"Customer Embedding Shape: {customer_embeddings.shape}\")\n\n# ✅ Convert NumPy arrays to PyTorch tensors first (on CPU)\ntext_tensor = torch.tensor(text_embeddings, dtype=torch.float32, device=\"cpu\")\nimage_tensor = torch.tensor(image_embeddings, dtype=torch.float32, device=\"cpu\")\n\nprint(\"Moving text_tensor to CUDA...\")\ntext_tensor = text_tensor.clone().detach().to(\"cuda\")\nprint(\"Moving image_tensor to CUDA...\")\nimage_tensor = image_tensor.clone().detach().to(\"cuda\")\n\n# ✅ Now concatenate them on CPU first\narticle_embedding_matrix = torch.cat([text_tensor, image_tensor], dim=1)\n\n# ✅ Move article_embedding_matrix to CUDA\nprint(\"Moving article_embedding_matrix to CUDA...\")\narticle_embedding_matrix = article_embedding_matrix.clone().detach().to(\"cuda\")\n\n# ✅ Move customer embeddings safely\nprint(\"Moving customer_embedding_matrix to CUDA...\")\ncustomer_embedding_matrix = torch.tensor(customer_embeddings.astype(np.float32), dtype=torch.float32, device=\"cuda\")\n\nprint(\"✅ Embeddings successfully moved to GPU!\")\n\n# ✅ **Define Training Parameters**\nlearning_rate = 5e-3\nnum_epochs = 2\nembedding_dim = 32\nnum_layers = 2\ndropout = 0.2\npatience = 10\n\n# ✅ **Initialize & Train Model**\nmodel = ImprovedCSTAR(\n    input_dim = article_embedding_matrix.shape[1] + customer_embedding_matrix.shape[1] + 1,\n    embedding_dim=embedding_dim,\n    num_layers=num_layers,\n    dropout=dropout\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# ✅ Convert features to PyTorch tensors before training\ntrain_features = torch.tensor(train_features, dtype=torch.float32, device=device)\nval_features = torch.tensor(val_features, dtype=torch.float32, device=device)\n\n# ✅ **Train Model**\ntrained_model = train_cstar_model(\n    model, optimizer, num_epochs, patience, train_features, val_features\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:03:03.714201Z","iopub.execute_input":"2025-02-21T04:03:03.714481Z","iopub.status.idle":"2025-02-21T04:03:06.525037Z","shell.execute_reply.started":"2025-02-21T04:03:03.714457Z","shell.execute_reply":"2025-02-21T04:03:06.523763Z"}},"outputs":[{"name":"stdout","text":"Text Embedding Shape: (105100, 100)\nImage Embedding Shape: (105100, 2048)\nCustomer Embedding Shape: (1371980, 64)\nMoving text_tensor to CUDA...\nMoving image_tensor to CUDA...\nMoving article_embedding_matrix to CUDA...\nMoving customer_embedding_matrix to CUDA...\n✅ Embeddings successfully moved to GPU!\n⚡ No checkpoint found. Starting from scratch.\nError: col_cc contains out-of-bounds indices!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-76-78cf1942f3a8>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# ✅ **Train Model**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m trained_model = train_cstar_model(\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m )\n","\u001b[0;32m<ipython-input-72-cc0c0ca0ae39>\u001b[0m in \u001b[0;36mtrain_cstar_model\u001b[0;34m(model, optimizer, num_epochs, patience, train_features, val_features)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_to_product_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_to_product_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_to_customer_edge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-cac0f799535e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgcn_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_to_product_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_to_product_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_to_customer_edge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-cac0f799535e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, customer_to_product_edge_index, product_to_product_edge_index, customer_to_customer_edge_index)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_cp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_cp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":76},{"cell_type":"code","source":"# Metrics Evaluation Functions\ndef recall_at_k(recommended_articles, relevant_articles, k=10):\n    recommended_set = set(recommended_articles[:k])\n    relevant_set = set(relevant_articles)\n    intersection = recommended_set.intersection(relevant_set)\n    return len(intersection) / len(relevant_set) if len(relevant_set) > 0 else 0.0\n\ndef ndcg_at_k(recommended_articles, relevant_articles, k=10):\n    recommended_set = set(recommended_articles[:k])\n    dcg = 0.0\n    idcg = 0.0\n\n    for i in range(min(k, len(recommended_articles))):\n        if recommended_articles[i] in relevant_articles:\n            dcg += 1 / np.log2(i + 2)\n\n    for i in range(min(k, len(relevant_articles))):\n        idcg += 1 / np.log2(i + 2)\n\n    return dcg / idcg if idcg > 0 else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Full-batch evaluation with Recall@K, NDCG@K, and Sliced Wasserstein Distance (SWD)\ndef test_model_full_batch_with_metrics(model, k=10):\n    model.eval()\n    total_test_loss = 0.0\n    all_recall_at_k = []\n    all_ndcg_at_k = []\n\n    test_features_tensor = torch.tensor(test_features, dtype=torch.float32).to(device)\n\n    with torch.no_grad():\n        # ✅ Compute embeddings for each edge type separately\n        test_embeddings_product_to_product = model(test_features_tensor, product_to_product_edge_index)\n        test_embeddings_customer_to_product = model(test_features_tensor, customer_to_product_edge_index)\n        test_embeddings_customer_to_customer = model(test_features_tensor, customer_to_customer_edge_index)\n\n        # ✅ **Precompute Positive & Negative Indices on GPU**\n        test_articles['positive_indices'] = test_articles['positive_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n        test_articles['negative_indices'] = test_articles['negative_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n\n        test_customers['customer_to_product_positive_indices'] = test_customers['customer_to_product_positive_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n        test_customers['negative_customer_to_product_indices'] = test_customers['negative_customer_to_product_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n\n        test_customers['positive_indices'] = test_customers['positive_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n        test_customers['negative_indices'] = test_customers['negative_indices'].apply(lambda x: torch.tensor(x, dtype=torch.long, device=device))\n\n        # ✅ **Batch Process Instead of Iterating Over Rows**\n        for customer_idx, customer_id in enumerate(test_customers['customer_id'].values):\n            relevant_articles = transactions_filtered[transactions_filtered['customer_id'] == customer_id]['article_id'].unique()\n\n            # **Compute SWD for Customer-to-Product**\n            customer_embedding = test_embeddings_customer_to_product[customer_idx].unsqueeze(0)\n            swd_distances_customer_to_product = [\n                (i, sliced_wasserstein_distance(customer_embedding, test_embeddings_product_to_product[i].unsqueeze(0)).item())\n                for i in range(test_embeddings_product_to_product.shape[0])\n            ]\n\n            # **Sort by SWD Distance (Lower is Better)**\n            swd_distances_customer_to_product.sort(key=lambda x: x[1])\n\n            # **Get Top-K Recommendations**\n            top_indices_customer_to_product = [item[0] for item in swd_distances_customer_to_product[:k]]\n            recommended_articles = [list(article_to_index.keys())[idx] for idx in top_indices_customer_to_product]\n\n            # **Compute Recall@K and NDCG@K**\n            recall = recall_at_k(recommended_articles, relevant_articles, k)\n            ndcg = ndcg_at_k(recommended_articles, relevant_articles, k)\n\n            all_recall_at_k.append(recall)\n            all_ndcg_at_k.append(ndcg)\n\n            # ✅ **Retrieve Positive & Negative Indices**\n            pos_indices_product = test_articles.loc[test_articles[\"article_id\"] == recommended_articles[0], \"positive_indices\"].values[0]\n            neg_indices_product = test_articles.loc[test_articles[\"article_id\"] == recommended_articles[0], \"negative_indices\"].values[0]\n\n            pos_indices_customer_product = test_customers.loc[test_customers[\"customer_id\"] == customer_id, \"customer_to_product_positive_indices\"].values[0]\n            neg_indices_customer_product = test_customers.loc[test_customers[\"customer_id\"] == customer_id, \"negative_customer_to_product_indices\"].values[0]\n\n            pos_indices_customer = test_customers.loc[test_customers[\"customer_id\"] == customer_id, \"positive_indices\"].values[0]\n            neg_indices_customer = test_customers.loc[test_customers[\"customer_id\"] == customer_id, \"negative_indices\"].values[0]\n\n            # ✅ **Compute Inter-Trajectory and Intra-Trajectory Loss**\n            inter_loss_product = inter_trajectory_loss(test_embeddings_product_to_product[customer_idx].unsqueeze(0), model.reference_embedding)\n            intra_loss_product = intra_trajectory_loss(test_embeddings_product_to_product, pos_indices_product, neg_indices_product, {})\n\n            inter_loss_customer_product = inter_trajectory_loss(test_embeddings_customer_to_product[customer_idx].unsqueeze(0), model.reference_embedding)\n            intra_loss_customer_product = intra_trajectory_loss(test_embeddings_customer_to_product, pos_indices_customer_product, neg_indices_customer_product, {})\n\n            inter_loss_customer = inter_trajectory_loss(test_embeddings_customer_to_customer[customer_idx].unsqueeze(0), model.reference_embedding)\n            intra_loss_customer = intra_trajectory_loss(test_embeddings_customer_to_customer, pos_indices_customer, neg_indices_customer, {})\n\n            # ✅ **Total Loss**\n            total_test_loss += (\n                inter_loss_product + intra_loss_product +\n                inter_loss_customer_product + intra_loss_customer_product +\n                inter_loss_customer + intra_loss_customer\n            ).item()\n\n    # ✅ **Normalize Loss**\n    avg_test_loss = total_test_loss / max(len(test_customers), 1)\n    avg_recall_at_k = np.mean(all_recall_at_k)\n    avg_ndcg_at_k = np.mean(all_ndcg_at_k)\n\n    # ✅ **Print Metrics**\n    print(f\"✅ Test Loss: {avg_test_loss:.4f}\")\n    print(f\"✅ Recall@{k}: {avg_recall_at_k:.4f}\")\n    print(f\"✅ NDCG@{k}: {avg_ndcg_at_k:.4f}\")\n\n    return avg_test_loss, avg_recall_at_k, avg_ndcg_at_k\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Example usage of testing\ntest_loss, recall, ndcg = test_model_full_batch_with_metrics(\n    model=trained_model,\n    k=10\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ **Find Similar Embeddings Using Sliced Wasserstein Distance (Optimized)**\ndef find_similar_embeddings(target_embedding, reference_embeddings, top_n=10):\n    \"\"\"\n    Compute Sliced Wasserstein Distance (SWD) in batch mode.\n    Returns indices of the top-N most similar embeddings.\n    \"\"\"\n    # ✅ Compute SWD distances efficiently\n    distances = torch.tensor([\n        sliced_wasserstein_distance(target_embedding.unsqueeze(0), reference_embeddings[i].unsqueeze(0)).item()\n        for i in range(reference_embeddings.shape[0])\n    ], device=device)\n\n    # ✅ Get top-N closest indices (lower distance is better)\n    return torch.argsort(distances)[:top_n].tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ **Multi-Modal Recommendation Function**\ndef recommend_articles_any_input(\n    model, top_n=10, image_path=None, text_input=None, numeric_input=None, \n    age=None, active=None, club_status=None\n):\n    \"\"\"\n    Multi-modal product recommendation function.\n    Supports **cold-start users** by recommending based on similar customers.\n    Accepts: Image, Text, Numeric Features, and Customer Demographics.\n    \"\"\"\n    model.eval()\n    input_features = []\n\n    # ✅ **Process Image Input (if provided)**\n    if image_path:\n        image = Image.open(image_path).convert('RGB')\n        image = transform(image).unsqueeze(0).to(device)\n        with torch.no_grad():\n            image_embedding = resnet_model(image).squeeze(0).to(device)\n        input_features.append(image_embedding)\n\n    # ✅ **Process Text Input (if provided)**\n    if text_input:\n        words = text_input.split()  \n        word_embeddings = np.zeros(100)  \n        valid_word_count = 0\n\n        for word in words:\n            if word in embeddings_index:  \n                word_embeddings += embeddings_index[word]\n                valid_word_count += 1\n\n        if valid_word_count > 0:\n            word_embeddings /= valid_word_count  \n        input_features.append(torch.tensor(word_embeddings, dtype=torch.float32).to(device))\n\n    # ✅ **Process Numeric Input (if provided)**\n    if numeric_input:\n        numeric_input = np.array(numeric_input).reshape(1, -1)\n        scaled_numeric_input = torch.tensor(scaler.transform(numeric_input), dtype=torch.float32).to(device)\n        input_features.append(scaled_numeric_input.flatten())\n\n    # ✅ **Handle Cold Start (Customer Attributes Given)**\n    if any([age is not None, active is not None, club_status is not None]):\n        print(\"🚀 Cold-Start User Detected: Generating Customer Embedding...\")\n\n        # ✅ Create Customer Feature Tensor\n        cold_start_input = pd.DataFrame({\"Active\": [active], \"club_member_status\": [club_status]})\n        dummy_encoded = pd.get_dummies(cold_start_input)\n\n        # ✅ Handle Missing Columns\n        missing_cols = [col for col in categorical_features_encoded.columns if col not in dummy_encoded.columns]\n        for col in missing_cols:\n            dummy_encoded[col] = 0  \n\n        dummy_encoded = dummy_encoded[categorical_features_encoded.columns].astype(np.float32).values\n        normalized_age = np.zeros(1) if age is None else scaler_age.transform([[age]]).flatten()\n\n        new_customer_features = np.hstack([normalized_age.reshape(-1, 1), dummy_encoded]).astype(np.float32)\n        new_customer_features_tensor = torch.tensor(new_customer_features, dtype=torch.float32).to(device)\n\n        # ✅ Generate Customer Embedding\n        with torch.no_grad():\n            new_customer_embedding = model(new_customer_features_tensor).squeeze(0).to(device)\n\n        # ✅ Find Similar Customers\n        top_similar_customers = find_similar_embeddings(new_customer_embedding, customer_embeddings, top_n)\n\n        # ✅ Retrieve Products Purchased by Similar Customers\n        recommended_articles = transactions_filtered[\n            transactions_filtered[\"customer_id\"].isin([customers.iloc[i][\"customer_id\"] for i in top_similar_customers])\n        ]\n        recommended_articles = recommended_articles.groupby(\"article_id\").size().reset_index(name=\"purchase_count\")\n        recommended_articles = recommended_articles.sort_values(\"purchase_count\", ascending=False).head(top_n)\n        recommended_articles = articles_with_images[articles_with_images[\"article_id\"].isin(recommended_articles[\"article_id\"])]\n\n        print(\"🔹 **Final Recommendations for Cold-Start User** 🔹\\n\")\n        for _, row in recommended_articles.iterrows():\n            print(f\"🛍️ Product: {row['prod_name']}\\n📜 Description: {row['text_data']}\\n\")\n            display(Image.open(row['path']))\n            print(\"\\n\")\n\n        return recommended_articles[['article_id', 'prod_name']]\n\n    # ✅ **Process Multi-Modal Features for Non-Cold Start Users**\n    if len(input_features) == 0:\n        print(\"⚠️ No input features provided!\")\n        return None\n\n    input_features = torch.cat(input_features, dim=0).unsqueeze(0).to(device)\n\n    # ✅ Construct PR Graph & Edge Indices\n    customer_to_product_edge_index = graph['customer', 'buys', 'product'].edge_index.to(device)\n    product_to_product_edge_index = graph['product', 'co_purchased_with', 'product'].edge_index.to(device)\n    customer_to_customer_edge_index = graph['customer', 'similar_to', 'customer'].edge_index.to(device)\n\n\n    with torch.no_grad():\n        all_embeddings = model(\n            test_features_tensor, \n            customer_to_product_edge_index, \n            product_to_product_edge_index, \n            customer_to_customer_edge_index  # ✅ Now all edges are correctly passed\n        )\n\n\n    # ✅ Find Top-N Similar Products\n    top_indices = find_similar_embeddings(input_features, all_embeddings, top_n)\n\n    # ✅ Retrieve Recommended Products\n    recommended_articles = articles_with_images.iloc[top_indices][['article_id', 'prod_name', 'text_data', 'path']]\n\n    print(\"🔹 **Final Recommendations Based on Multi-Modal Input** 🔹\\n\")\n    for _, row in recommended_articles.iterrows():\n        print(f\"🛍️ Product: {row['prod_name']}\\n📜 Description: {row['text_data']}\\n\")\n        display(Image.open(row['path']))\n        print(\"\\n\")\n\n    return recommended_articles[['article_id', 'prod_name']]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ **Example Recommendations**\n# Image Input Recommendation\nrecommend_articles_any_input(model=trained_model, top_n=5, image_path=\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/079/0797892010.jpg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Text Input Recommendation\nrecommend_articles_any_input(model=trained_model, top_n=5, text_input=\"Floral summer dress with short sleeves\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numeric Input Recommendation\nrecommend_articles_any_input(model=trained_model, top_n=5, numeric_input=[22, 5, 14, 8, 1, 3])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combined Multi-modal Recommendation\nrecommend_articles_any_input(model=trained_model, top_n=5, \n    image_path=\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/072/0720572001.jpg\",\n    text_input=\"Black mini dress\",\n    numeric_input=[30, 40, 50]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cold-Start User Recommendation (Using Age)\nrecommend_articles_any_input(model=trained_model, top_n=5, age=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}