{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T18:12:55.952070Z",
     "iopub.status.busy": "2025-03-31T18:12:55.951764Z",
     "iopub.status.idle": "2025-03-31T18:13:17.476449Z",
     "shell.execute_reply": "2025-03-31T18:13:17.475779Z",
     "shell.execute_reply.started": "2025-03-31T18:12:55.952044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import os, glob, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-31T18:13:32.277993Z",
     "iopub.status.busy": "2025-03-31T18:13:32.277359Z",
     "iopub.status.idle": "2025-03-31T18:13:32.338138Z",
     "shell.execute_reply": "2025-03-31T18:13:32.337227Z",
     "shell.execute_reply.started": "2025-03-31T18:13:32.277961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Configuration\n",
    "# --------------------------\n",
    "class Config:\n",
    "    text_max_length = 64\n",
    "    batch_size = 128\n",
    "    emb_dim = 256\n",
    "    num_heads = 4\n",
    "    dropout = 0.5\n",
    "    lr = 5e-5           # Lower LR for stability\n",
    "    weight_decay = 1e-5\n",
    "    epochs = 14\n",
    "    patience = 5        # For early stopping\n",
    "    k_folds = 2\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --------------------------\n",
    "# 1. Comprehensive Preprocessing\n",
    "# --------------------------\n",
    "def load_and_preprocess():\n",
    "    # Load raw CSVs\n",
    "    articles = pd.read_csv(\"/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv\")\n",
    "    customers = pd.read_csv(\"/kaggle/input/h-and-m-personalized-fashion-recommendations/customers.csv\")\n",
    "    transactions = pd.read_csv(\"/kaggle/input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")\n",
    "    \n",
    "    # Adjust IDs\n",
    "    def adjust_id(x):\n",
    "        x = str(x)\n",
    "        return \"0\" + x if len(x) == 9 else x\n",
    "    transactions[\"article_id\"] = transactions[\"article_id\"].apply(adjust_id)\n",
    "    articles[\"article_id\"] = articles[\"article_id\"].apply(adjust_id)\n",
    "    \n",
    "    # Filter customers and transactions\n",
    "    customers = customers[['customer_id', 'age']].dropna(subset=['age'])\n",
    "    valid_article_ids = set(articles['article_id'].unique())\n",
    "    filtered_transactions = transactions[transactions['article_id'].isin(valid_article_ids)]\n",
    "\n",
    "\n",
    "    \n",
    "    # Cold-start handling using transaction counts and customer age (demographics)\n",
    "    transaction_counts = filtered_transactions['customer_id'].value_counts()\n",
    "    frequent_customers = transaction_counts[transaction_counts > 8].index.tolist()\n",
    "    cold_start_few = transaction_counts[transaction_counts <= 2].index.tolist()\n",
    "    cold_start_no = list(set(customers['customer_id']) - set(filtered_transactions['customer_id']))\n",
    "    \n",
    "    # Stratified sampling: (50-30-20)\n",
    "    def sample_customers(group, target_size):\n",
    "        return np.random.choice(group, size=min(len(group), target_size), replace=False)\n",
    "    sample_sizes = [50000, 30000, 20000]\n",
    "    sampled = [\n",
    "        sample_customers(frequent_customers, sample_sizes[0]),\n",
    "        sample_customers(cold_start_few, sample_sizes[1]),\n",
    "        sample_customers(cold_start_no, sample_sizes[2])\n",
    "    ]\n",
    "    sampled_customers = np.concatenate(sampled)\n",
    "    sampled_customers = customers[customers['customer_id'].isin(sampled_customers)].reset_index(drop=True)\n",
    "    sampled_customers = sampled_customers[['customer_id', 'age']]\n",
    "    \n",
    "    # Filter transactions for sampled customers\n",
    "    filtered_trans = filtered_transactions[\n",
    "        (filtered_transactions['customer_id'].isin(sampled_customers['customer_id'])) &\n",
    "        (filtered_transactions['article_id'].isin(valid_article_ids))\n",
    "    ]\n",
    "\n",
    "\n",
    "    \n",
    "    # Get image paths and filter articles with valid images\n",
    "    all_image_paths = glob.glob(\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/*/*.jpg\")\n",
    "    valid_ids = set(os.path.splitext(os.path.basename(p))[0] for p in all_image_paths)\n",
    "    def get_image_path(aid):\n",
    "        subfolder = aid[:3]\n",
    "        path = f\"/kaggle/input/h-and-m-personalized-fashion-recommendations/images/{subfolder}/{aid}.jpg\"\n",
    "        return path if aid in valid_ids else None\n",
    "    articles['image_path'] = articles['article_id'].apply(get_image_path)\n",
    "    articles = articles.dropna(subset=['image_path']).reset_index(drop=True)\n",
    "    \n",
    "    # Merge price information from transactions\n",
    "    article_prices = filtered_trans[['article_id', 'price']].drop_duplicates(subset=['article_id'])\n",
    "    articles = articles.merge(article_prices, on='article_id', how='inner')\n",
    "    articles['detail_desc'] = articles['detail_desc'].fillna('').astype(str)\n",
    "    articles = articles[['article_id', 'detail_desc', 'image_path', 'price']]\n",
    "    \n",
    "    valid_articles = set(articles['article_id'])\n",
    "    filtered_trans = filtered_trans[filtered_trans['article_id'].isin(valid_articles)]\n",
    "    \n",
    "    # Validate images\n",
    "    articles['image_exists'] = articles['image_path'].apply(os.path.exists)\n",
    "    missing = articles[~articles['image_exists']]\n",
    "    if len(missing) > 0:\n",
    "        print(f\"Found {len(missing)} articles with missing images. Samples:\")\n",
    "        print(missing.sample(min(5, len(missing))))\n",
    "        articles = articles[articles['image_exists']].drop(columns=['image_exists'])\n",
    "    else:\n",
    "        print(\"All images validated successfully!\")\n",
    "\n",
    "    \n",
    "    # Create mapped IDs for articles and customers\n",
    "    articles = articles.reset_index(drop=True)\n",
    "    articles[\"article_mapped_id\"] = articles.index\n",
    "    sampled_customers = sampled_customers.reset_index(drop=True)\n",
    "    sampled_customers[\"customer_mapped_id\"] = sampled_customers.index\n",
    "    \n",
    "    filtered_trans = filtered_trans.merge(\n",
    "        articles[['article_id', 'article_mapped_id']],\n",
    "        on='article_id',\n",
    "        how='inner'\n",
    "    ).merge(\n",
    "        sampled_customers[['customer_id', 'customer_mapped_id']],\n",
    "        on='customer_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # --------------------------\n",
    "    # Precompute product raw features: image and text features.\n",
    "    # --------------------------\n",
    "    fp = FeatureProcessor()\n",
    "    img_feats, txt_feats = [], []\n",
    "    # Use pretrained models for feature extraction\n",
    "    resnet_model = models.resnet18(pretrained=True).eval().to(device)\n",
    "    bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').eval().to(device)\n",
    "    for idx, row in tqdm(articles.iterrows(), total=len(articles), desc=\"Precomputing product features\"):\n",
    "        try:\n",
    "            # Extract image feature\n",
    "            img = Image.open(row['image_path']).convert('RGB')\n",
    "            img_tensor = fp.image_transform(img).unsqueeze(0).to(device)\n",
    "            img_feat = resnet_model(img_tensor).squeeze().cpu()  # shape: [1000]\n",
    "            img_feats.append(img_feat)\n",
    "            \n",
    "            # Extract text feature using DistilBERT\n",
    "            text_enc = fp.tokenizer(row['detail_desc'], padding='max_length', truncation=True,\n",
    "                                      max_length=config.text_max_length, return_tensors='pt').to(device)\n",
    "            txt_feat = bert_model(**text_enc).last_hidden_state[:, 0].squeeze().cpu()  # shape: [768]\n",
    "            txt_feats.append(txt_feat)\n",
    "        except Exception as e:\n",
    "            print(f\"Error precomputing features for article {row['article_id']}: {e}\")\n",
    "            img_feats.append(torch.zeros(1000))\n",
    "            txt_feats.append(torch.zeros(768))\n",
    "    articles['img_feat'] = img_feats\n",
    "    articles['txt_feat'] = txt_feats\n",
    "    \n",
    "    return sampled_customers, filtered_trans, articles\n",
    "\n",
    "# --------------------------\n",
    "# 2. Feature Processing (for text tokenization, etc.)\n",
    "# --------------------------\n",
    "class FeatureProcessor:\n",
    "    def __init__(self):\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    def process_text(self, texts):\n",
    "        return self.tokenizer(texts, padding='max_length', truncation=True,\n",
    "                              max_length=config.text_max_length, return_tensors='pt')\n",
    "\n",
    "# --------------------------\n",
    "# Build product feature dictionary for fast lookup during training\n",
    "# --------------------------\n",
    "def build_product_feature_dict(articles):\n",
    "    prod_dict = {}\n",
    "    for idx, row in articles.iterrows():\n",
    "        prod_dict[int(row['article_mapped_id'])] = {\n",
    "            'img_feat': row['img_feat'],\n",
    "            'txt_feat': row['txt_feat'],\n",
    "            'price': torch.tensor(row['price'], dtype=torch.float32)\n",
    "        }\n",
    "    return prod_dict\n",
    "\n",
    "# --------------------------\n",
    "# 3. Graph Construction (now we also add user age)\n",
    "# --------------------------\n",
    "def build_graph(transactions, articles, customers):\n",
    "    data = HeteroData()\n",
    "    data['user'].num_nodes = len(customers)\n",
    "    data['product'].num_nodes = len(articles)\n",
    "    edge_index = torch.tensor([\n",
    "        transactions['customer_mapped_id'].values,\n",
    "        transactions['article_mapped_id'].values\n",
    "    ], dtype=torch.long)\n",
    "    edge_index = sort_edge_index(edge_index)\n",
    "    data['user', 'buys', 'product'].edge_index = edge_index\n",
    "    data['product', 'rev_buys', 'user'].edge_index = edge_index.flip(0)\n",
    "    data['user'].x = torch.arange(len(customers), dtype=torch.long)\n",
    "    data['user'].age = torch.tensor(customers['age'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    data['product'].x = torch.zeros(len(articles), dtype=torch.float32)\n",
    "    data['product'].price = torch.tensor(articles.price.values, dtype=torch.float32).unsqueeze(1)\n",
    "    return data\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. Model Architecture with Age Incorporation\n",
    "# --------------------------\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "class MultiModalGNN(nn.Module):\n",
    "    def __init__(self, metadata, num_users, num_products):\n",
    "        super(MultiModalGNN, self).__init__()\n",
    "        # User embedding for ID\n",
    "        self.user_emb = nn.Embedding(num_users, config.emb_dim)\n",
    "        # New age encoder for customer age (input dimension 1 to emb_dim)\n",
    "        self.age_encoder = nn.Linear(1, config.emb_dim)\n",
    "        # Product feature layers to combine image, text, and price features\n",
    "        self.img_fc = nn.Linear(1000, config.emb_dim)\n",
    "        self.txt_fc = nn.Linear(768, config.emb_dim)\n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, config.emb_dim)\n",
    "        )\n",
    "        # Graph Neural Network layers\n",
    "        self.conv1 = HGTConv(config.emb_dim, config.emb_dim, metadata, heads=config.num_heads)\n",
    "        self.conv2 = HGTConv(config.emb_dim, config.emb_dim, metadata, heads=config.num_heads)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x_dict, edge_index_dict, user_age):\n",
    "        # x_dict['user'] contains user IDs.\n",
    "        user_ids = x_dict['user'].to(device)\n",
    "        base_emb = self.user_emb(user_ids)  # [num_users, emb_dim]\n",
    "        age_emb = self.age_encoder(user_age.to(device))  # [num_users, emb_dim]\n",
    "        # Combine the ID embedding with the age embedding\n",
    "        x_dict['user'] = base_emb + age_emb\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {k: F.gelu(v) for k, v in x_dict.items()}\n",
    "        x_dict = {k: self.dropout(v) for k, v in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "# --------------------------\n",
    "# 5. Data Splitting\n",
    "# --------------------------\n",
    "def create_splits(transactions):\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "    train_idx, test_idx = next(splitter.split(transactions, groups=transactions['customer_mapped_id']))\n",
    "    train_trans = transactions.iloc[train_idx]\n",
    "    temp_trans = transactions.iloc[test_idx]\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    val_idx, test_idx = next(splitter.split(temp_trans, groups=temp_trans['customer_mapped_id']))\n",
    "    return train_trans, temp_trans.iloc[val_idx], temp_trans.iloc[test_idx]\n",
    "\n",
    "# --------------------------\n",
    "# 6. Custom Neighbor Loader (unchanged)\n",
    "# --------------------------\n",
    "def sample_neighbors(data: HeteroData, edge_type: tuple, src_nodes: torch.Tensor, num_samples: int) -> torch.Tensor:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    src = edge_index[0]\n",
    "    tgt = edge_index[1]\n",
    "    sampled_list = []\n",
    "    for node in src_nodes.tolist():\n",
    "        mask = (src == node)\n",
    "        candidates = tgt[mask]\n",
    "        if candidates.numel() == 0:\n",
    "            continue\n",
    "        if candidates.numel() > num_samples:\n",
    "            perm = torch.randperm(candidates.numel())[:num_samples]\n",
    "            sampled = candidates[perm]\n",
    "        else:\n",
    "            sampled = candidates\n",
    "        sampled_list.append(sampled)\n",
    "    if sampled_list:\n",
    "        sampled_tgts = torch.cat(sampled_list)\n",
    "    else:\n",
    "        sampled_tgts = torch.tensor([], dtype=torch.long)\n",
    "    return torch.unique(sampled_tgts)\n",
    "\n",
    "class CustomNeighborLoader:\n",
    "    def __init__(self, data: HeteroData, input_nodes: tuple, batch_size: int,\n",
    "                 num_neighbors: dict, shuffle: bool = True):\n",
    "        self.data = data\n",
    "        self.input_nodes = input_nodes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.shuffle = shuffle\n",
    "        self.node_type = input_nodes[0]\n",
    "        self.node_indices = input_nodes[1]\n",
    "        if self.shuffle:\n",
    "            self.node_indices = self.node_indices[torch.randperm(self.node_indices.size(0))]\n",
    "        self.num_batches = math.ceil(self.node_indices.size(0) / batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(self.num_batches):\n",
    "            batch_seed = self.node_indices[i * self.batch_size: (i+1) * self.batch_size]\n",
    "            n1 = self.num_neighbors.get(('user', 'buys', 'product'), [0])[0]\n",
    "            sampled_products = sample_neighbors(self.data, ('user', 'buys', 'product'), batch_seed, n1)\n",
    "            n2 = self.num_neighbors.get(('product', 'rev_buys', 'user'), [0])[0]\n",
    "            sampled_users_hop2 = sample_neighbors(self.data, ('product', 'rev_buys', 'user'), sampled_products, n2)\n",
    "            final_users = torch.unique(torch.cat([batch_seed, sampled_users_hop2]))\n",
    "            final_products = sampled_products\n",
    "            sub_data = HeteroData()\n",
    "            sorted_users, _ = torch.sort(final_users)\n",
    "            user_map = {int(u.item()): i for i, u in enumerate(sorted_users)}\n",
    "            sub_data['user'].num_nodes = sorted_users.size(0)\n",
    "            sub_data['user'].x = sorted_users.clone().to(torch.long)\n",
    "            # Also pass along user age from the full graph\n",
    "            sub_data['user'].age = self.data['user'].age[sorted_users]\n",
    "\n",
    "            sorted_products, _ = torch.sort(final_products)\n",
    "            prod_map = {int(p.item()): i for i, p in enumerate(sorted_products)}\n",
    "            sub_data['product'].num_nodes = sorted_products.size(0)\n",
    "            sub_data['product'].x = torch.zeros(len(sorted_products), dtype=torch.float32)\n",
    "            edge_index = self.data['user', 'buys', 'product'].edge_index\n",
    "            mask = (torch.isin(edge_index[0], sorted_users) & torch.isin(edge_index[1], sorted_products))\n",
    "            sub_edge_index = edge_index[:, mask].clone()\n",
    "            for j in range(sub_edge_index.size(1)):\n",
    "                src = int(sub_edge_index[0, j].item())\n",
    "                tgt = int(sub_edge_index[1, j].item())\n",
    "                sub_edge_index[0, j] = user_map[src]\n",
    "                sub_edge_index[1, j] = prod_map[tgt]\n",
    "            sub_data['user', 'buys', 'product'].edge_index = sub_edge_index\n",
    "            edge_index = self.data['product', 'rev_buys', 'user'].edge_index\n",
    "            mask = (torch.isin(edge_index[0], sorted_products) & torch.isin(edge_index[1], sorted_users))\n",
    "            sub_edge_index = edge_index[:, mask].clone()\n",
    "            for j in range(sub_edge_index.size(1)):\n",
    "                src = int(sub_edge_index[0, j].item())\n",
    "                tgt = int(sub_edge_index[1, j].item())\n",
    "                sub_edge_index[0, j] = prod_map[src]\n",
    "                sub_edge_index[1, j] = user_map[tgt]\n",
    "            sub_data['product', 'rev_buys', 'user'].edge_index = sub_edge_index\n",
    "            seed_mask = torch.zeros(sorted_users.size(0), dtype=torch.bool)\n",
    "            for u in batch_seed.tolist():\n",
    "                if u in user_map:\n",
    "                    seed_mask[user_map[u]] = True\n",
    "            sub_data['user'].seed_mask = seed_mask\n",
    "            yield sub_data\n",
    "\n",
    "# --------------------------\n",
    "# 7. Training & Evaluation (Improved with In-Batch Negatives, LR Scheduler, Early Stopping)\n",
    "# --------------------------\n",
    "def train(model, train_data, val_data, optimizer, articles, prod_feature_dict, save_path='best_model.pth'):\n",
    "    best_ndcg = -1\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        loader = CustomNeighborLoader(\n",
    "            data=train_data,\n",
    "            input_nodes=('user', torch.arange(train_data['user'].num_nodes, device='cpu')),\n",
    "            batch_size=config.batch_size,\n",
    "            num_neighbors={('user', 'buys', 'product'): [10],\n",
    "                           ('product', 'rev_buys', 'user'): [5]},\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(f\"\\nStarting epoch {epoch+1}\")\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            batch = batch.to(device)\n",
    "            # Quickly retrieve product raw features from the dictionary\n",
    "            prod_indices = batch['product'].x.cpu().numpy().astype(int)\n",
    "            img_feat_list, txt_feat_list, price_list = [], [], []\n",
    "            for pid in prod_indices:\n",
    "                feat = prod_feature_dict.get(pid)\n",
    "                if feat is None:\n",
    "                    continue\n",
    "                img_feat_list.append(feat['img_feat'].to(device))\n",
    "                txt_feat_list.append(feat['txt_feat'].to(device))\n",
    "                price_list.append(feat['price'].to(device))\n",
    "            if len(img_feat_list) == 0:\n",
    "                print(\"No product features in this batch; skipping.\")\n",
    "                continue\n",
    "            img_feats = torch.stack(img_feat_list)\n",
    "            txt_feats = torch.stack(txt_feat_list)\n",
    "            prices = torch.stack(price_list)\n",
    "            img_emb = model.img_fc(img_feats)\n",
    "            txt_emb = model.txt_fc(txt_feats)\n",
    "            price_emb = model.price_encoder(prices.unsqueeze(1))\n",
    "            prod_emb_batch = img_emb + txt_emb + price_emb\n",
    "            batch['product'].x = prod_emb_batch\n",
    "            optimizer.zero_grad()\n",
    "            # Get user age from the batch\n",
    "            user_age = batch['user'].age.to(device)\n",
    "            with torch.amp.autocast('cuda',enabled=True):\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, user_age)\n",
    "                # Get the positive edge indices (shape: [2, E])\n",
    "                pos_edges = batch['user', 'buys', 'product'].edge_index  \n",
    "                # Compute positive scores directly for each edge:\n",
    "                user_pos = out['user'][pos_edges[0]]\n",
    "                prod_pos = out['product'][pos_edges[1]]\n",
    "                pos_scores = (user_pos * prod_pos).sum(dim=1)  # shape: [E]\n",
    "                \n",
    "                # For each positive edge, sample one negative product from the batch\n",
    "                num_edges = pos_edges.size(1)\n",
    "                neg_indices = torch.randint(0, out['product'].size(0), (num_edges,), device=device)\n",
    "                neg_scores = (user_pos * out['product'][neg_indices]).sum(dim=1)\n",
    "                \n",
    "\n",
    "                # Use margin ranking loss (which encourages pos_scores > neg_scores + margin)\n",
    "                margin = 0.2\n",
    "                loss = F.margin_ranking_loss(pos_scores, neg_scores, target=torch.ones_like(pos_scores), margin=margin)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        if val_data is not None:\n",
    "            ndcg, recall, auc, map12, recall12 = evaluate(model, val_data, articles)\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, NDCG@10={ndcg:.4f}, Recall@10={recall:.4f}, AUC={auc:.4f}, MAP@12={map12:.4f}, Recall@12={recall12:.4f}\")\n",
    "            scheduler.step(ndcg)\n",
    "            if ndcg > best_ndcg:\n",
    "                best_ndcg = ndcg\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Best model saved at epoch {epoch+1}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= config.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f} (No evaluation)\")\n",
    "    return best_ndcg\n",
    "\n",
    "def evaluate(model, val_data, articles):\n",
    "    model.eval()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    loader = CustomNeighborLoader(\n",
    "        data=val_data,\n",
    "        input_nodes=('user', torch.arange(val_data['user'].num_nodes, device='cpu')),\n",
    "        batch_size=config.batch_size,\n",
    "        num_neighbors={('user', 'buys', 'product'): [10],\n",
    "                       ('product', 'rev_buys', 'user'): [5]},\n",
    "        shuffle=False\n",
    "    )\n",
    "    all_ndcgs, all_recalls, all_aucs, all_maps_12, all_recalls_12 = [], [], [], [], []\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        prod_indices = batch['product'].x.cpu().numpy().astype(int)\n",
    "        img_feat_list, txt_feat_list, price_list = [], [], []\n",
    "        for pid in prod_indices:\n",
    "            feat = prod_feature_dict.get(pid)\n",
    "            if feat is None:\n",
    "                continue\n",
    "            img_feat_list.append(feat['img_feat'].to(device))\n",
    "            txt_feat_list.append(feat['txt_feat'].to(device))\n",
    "            price_list.append(feat['price'].to(device))\n",
    "        if len(img_feat_list) == 0:\n",
    "            continue\n",
    "        img_feats = torch.stack(img_feat_list)\n",
    "        txt_feats = torch.stack(txt_feat_list)\n",
    "        prices = torch.stack(price_list)\n",
    "        img_emb = model.img_fc(img_feats)\n",
    "        txt_emb = model.txt_fc(txt_feats)\n",
    "        price_emb = model.price_encoder(prices.unsqueeze(1))\n",
    "        batch['product'].x = (img_emb + txt_emb + price_emb).to(device)\n",
    "        try:\n",
    "            with torch.amp.autocast('cuda',enabled=True):\n",
    "                # Get user age for evaluation as well\n",
    "                user_age = batch['user'].age.to(device)\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, user_age)\n",
    "            user_embeddings = out['user'].detach()\n",
    "            product_embeddings = out['product'].detach()\n",
    "            scores = torch.mm(user_embeddings, product_embeddings.t())\n",
    "            \n",
    "            pos_edges = batch['user', 'buys', 'product'].edge_index\n",
    "            ndcg = calculate_ndcg(scores, pos_edges, k=10)\n",
    "            recall = calculate_recall(scores, pos_edges, k=10)\n",
    "            auc = calculate_auc(scores, pos_edges)\n",
    "            map_12 = calculate_map(scores, pos_edges, k=12)\n",
    "            recall_12 = calculate_recall(scores, pos_edges, k=12)\n",
    "            all_ndcgs.append(ndcg)\n",
    "            all_recalls.append(recall)\n",
    "            all_aucs.append(auc)\n",
    "            all_maps_12.append(map_12)\n",
    "            all_recalls_12.append(recall_12)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation of batch {batch_idx+1}: {e}\")\n",
    "            continue\n",
    "    return (np.mean(all_ndcgs) if all_ndcgs else 0,\n",
    "            np.mean(all_recalls) if all_recalls else 0,\n",
    "            np.mean(all_aucs) if all_aucs else 0,\n",
    "            np.mean(all_maps_12) if all_maps_12 else 0,\n",
    "            np.mean(all_recalls_12) if all_recalls_12 else 0)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 8. Metrics (unchanged)\n",
    "# --------------------------\n",
    "def calculate_ndcg(scores, edges, k=10):\n",
    "    scores = scores.detach().cpu()\n",
    "    user_items = {}\n",
    "    for u, i in edges.t().tolist():\n",
    "        if u not in user_items:\n",
    "            user_items[u] = set()\n",
    "        user_items[u].add(i)\n",
    "    ndcgs = []\n",
    "    for u in user_items.keys():\n",
    "        relevant_items = user_items[u]\n",
    "        if len(relevant_items) == 0:\n",
    "            continue\n",
    "        user_scores = scores[u]\n",
    "        top_k_items = torch.topk(user_scores, k=k).indices.tolist()\n",
    "        dcg = sum(1 / np.log2(rank + 2) for rank, item_id in enumerate(top_k_items) if item_id in relevant_items)\n",
    "        ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_items), k)))\n",
    "        if ideal_dcg > 0:\n",
    "            ndcgs.append(dcg / ideal_dcg)\n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "def calculate_recall(scores, edges, k=10):\n",
    "    user_items = {}\n",
    "    for u, i in edges.t().tolist():\n",
    "        user_items.setdefault(u, set()).add(i)\n",
    "    recalls = []\n",
    "    for u in range(scores.size(0)):\n",
    "        pred = set(scores[u].argsort(descending=True)[:k].tolist())\n",
    "        rel = user_items.get(u, set())\n",
    "        if len(rel) == 0:\n",
    "            continue\n",
    "        recalls.append(len(pred & rel)/len(rel))\n",
    "    return np.mean(recalls) if recalls else 0\n",
    "\n",
    "def calculate_auc(scores, edges):\n",
    "    pos_pairs = scores[edges[0], edges[1]]\n",
    "    neg_pairs = scores[torch.randint(0, scores.size(0), (len(pos_pairs),)),\n",
    "                       torch.randint(0, scores.size(1), (len(pos_pairs),))]\n",
    "    y_true = torch.cat([torch.ones_like(pos_pairs), torch.zeros_like(neg_pairs)])\n",
    "    y_score = torch.cat([pos_pairs.sigmoid(), neg_pairs.sigmoid()])\n",
    "    return roc_auc_score(y_true.cpu().numpy(), y_score.cpu().numpy())\n",
    "\n",
    "def calculate_map(scores, edges, k=12):\n",
    "    user_items = {}\n",
    "    for u, i in edges.t().tolist():\n",
    "        user_items.setdefault(u, set()).add(i)\n",
    "    maps = []\n",
    "    for u in range(scores.size(0)):\n",
    "        rel = user_items.get(u, set())\n",
    "        if not rel: continue\n",
    "        pred = scores[u].argsort(descending=True)[:k].tolist()\n",
    "        hits, ap = 0, 0\n",
    "        for i, item in enumerate(pred):\n",
    "            if item in rel:\n",
    "                hits += 1\n",
    "                ap += hits / (i + 1)\n",
    "        maps.append(ap / min(len(rel), k))\n",
    "    return np.mean(maps) if maps else 0.0\n",
    "# --------------------------\n",
    "# 9. Cross-Validation (unchanged)\n",
    "# --------------------------\n",
    "def cross_validate(articles, customers, transactions):\n",
    "    kf = GroupKFold(config.k_folds)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(transactions, groups=transactions['customer_mapped_id'])):\n",
    "        print(f\"\\n=== Fold {fold+1} ===\")\n",
    "        \n",
    "        fold_train_trans = transactions.iloc[train_idx]\n",
    "        fold_val_trans = transactions.iloc[val_idx]\n",
    "        \n",
    "        # Extract user IDs from the split transactions\n",
    "        train_users = fold_train_trans['customer_mapped_id'].unique()\n",
    "        val_users = fold_val_trans['customer_mapped_id'].unique()\n",
    "\n",
    "        # Keep only the users involved in this fold\n",
    "        fold_customers = customers[customers['customer_mapped_id'].isin(np.concatenate([train_users, val_users]))].copy()\n",
    "\n",
    "        # Reindex fold_customers to have contiguous IDs\n",
    "        fold_customers = fold_customers.reset_index(drop=True)\n",
    "        fold_customers['customer_mapped_id'] = fold_customers.index\n",
    "\n",
    "        # Re-map the transactions accordingly\n",
    "        user_id_map = dict(zip(fold_customers['customer_id'], fold_customers['customer_mapped_id']))\n",
    "        fold_train_trans = fold_train_trans.copy()\n",
    "        fold_val_trans = fold_val_trans.copy()\n",
    "        fold_train_trans['customer_mapped_id'] = fold_train_trans['customer_id'].map(user_id_map)\n",
    "        fold_val_trans['customer_mapped_id'] = fold_val_trans['customer_id'].map(user_id_map)\n",
    "\n",
    "        # Filter articles\n",
    "        fold_article_ids = set(fold_train_trans.article_mapped_id).union(set(fold_val_trans.article_mapped_id))\n",
    "        fold_articles = articles[articles.article_mapped_id.isin(fold_article_ids)].copy()\n",
    "\n",
    "        train_data = build_graph(fold_train_trans, fold_articles, fold_customers)\n",
    "        val_data = build_graph(fold_val_trans, fold_articles, fold_customers)\n",
    "\n",
    "        num_users = fold_customers['customer_mapped_id'].max() + 1\n",
    "        num_products = fold_articles['article_mapped_id'].max() + 1\n",
    "        model = MultiModalGNN(train_data.metadata(), num_users, num_products).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        _ = train(model, train_data, val_data, optimizer, fold_articles, prod_feature_dict, save_path=f\"best_model_fold{fold+1}.pth\")\n",
    "        del model, optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"\\nCross-validation complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T03:38:14.118867Z",
     "iopub.status.busy": "2025-03-31T03:38:14.118347Z",
     "iopub.status.idle": "2025-03-31T13:05:37.434574Z",
     "shell.execute_reply": "2025-03-31T13:05:37.433542Z",
     "shell.execute_reply.started": "2025-03-31T03:38:14.118820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles: 80654, Customers: 88647, Transactions: 2092109\n",
      "Starting cross-validation on training data...\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:178: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor([\n",
      "<ipython-input-9-f65c1d2ad2a7>:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.4435, NDCG@10=0.0519, Recall@10=0.0812, AUC=0.5980, MAP@12=0.0285, Recall@12=0.0949\n",
      "Best model saved at epoch 1\n",
      "\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.0200, NDCG@10=0.0580, Recall@10=0.0925, AUC=0.5771, MAP@12=0.0320, Recall@12=0.1086\n",
      "Best model saved at epoch 2\n",
      "\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.0178, NDCG@10=0.0805, Recall@10=0.1293, AUC=0.5723, MAP@12=0.0465, Recall@12=0.1506\n",
      "Best model saved at epoch 3\n",
      "\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.0162, NDCG@10=0.0812, Recall@10=0.1304, AUC=0.5756, MAP@12=0.0471, Recall@12=0.1515\n",
      "Best model saved at epoch 4\n",
      "\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.0147, NDCG@10=0.0812, Recall@10=0.1302, AUC=0.5722, MAP@12=0.0473, Recall@12=0.1513\n",
      "Best model saved at epoch 5\n",
      "\n",
      "Starting epoch 6\n",
      "No product features in this batch; skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.0135, NDCG@10=0.0864, Recall@10=0.1373, AUC=0.5700, MAP@12=0.0505, Recall@12=0.1589\n",
      "Best model saved at epoch 6\n",
      "\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation of batch 433: selected index k out of range\n",
      "Epoch 7: Loss=0.0130, NDCG@10=0.0861, Recall@10=0.1363, AUC=0.5816, MAP@12=0.0499, Recall@12=0.1581\n",
      "\n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0126, NDCG@10=0.0839, Recall@10=0.1340, AUC=0.5799, MAP@12=0.0488, Recall@12=0.1555\n",
      "\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation of batch 433: selected index k out of range\n",
      "Epoch 9: Loss=0.0121, NDCG@10=0.0882, Recall@10=0.1402, AUC=0.5870, MAP@12=0.0511, Recall@12=0.1630\n",
      "Best model saved at epoch 9\n",
      "\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation of batch 433: selected index k out of range\n",
      "Epoch 10: Loss=0.0117, NDCG@10=0.0968, Recall@10=0.1538, AUC=0.5870, MAP@12=0.0566, Recall@12=0.1786\n",
      "Best model saved at epoch 10\n",
      "\n",
      "Starting epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0114, NDCG@10=0.0925, Recall@10=0.1482, AUC=0.5900, MAP@12=0.0541, Recall@12=0.1720\n",
      "\n",
      "Starting epoch 12\n",
      "No product features in this batch; skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0111, NDCG@10=0.0946, Recall@10=0.1514, AUC=0.5915, MAP@12=0.0556, Recall@12=0.1756\n",
      "\n",
      "Starting epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0111, NDCG@10=0.0968, Recall@10=0.1544, AUC=0.5994, MAP@12=0.0569, Recall@12=0.1789\n",
      "Best model saved at epoch 13\n",
      "\n",
      "Starting epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0107, NDCG@10=0.0974, Recall@10=0.1554, AUC=0.5894, MAP@12=0.0578, Recall@12=0.1801\n",
      "Best model saved at epoch 14\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.4151, NDCG@10=0.0504, Recall@10=0.0773, AUC=0.6050, MAP@12=0.0273, Recall@12=0.0900\n",
      "Best model saved at epoch 1\n",
      "\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.0260, NDCG@10=0.0588, Recall@10=0.0915, AUC=0.5795, MAP@12=0.0324, Recall@12=0.1061\n",
      "Best model saved at epoch 2\n",
      "\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.0215, NDCG@10=0.0662, Recall@10=0.1023, AUC=0.5635, MAP@12=0.0367, Recall@12=0.1194\n",
      "Best model saved at epoch 3\n",
      "\n",
      "Starting epoch 4\n",
      "No product features in this batch; skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.0194, NDCG@10=0.0710, Recall@10=0.1098, AUC=0.5716, MAP@12=0.0396, Recall@12=0.1275\n",
      "Best model saved at epoch 4\n",
      "\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.0174, NDCG@10=0.0697, Recall@10=0.1072, AUC=0.5793, MAP@12=0.0387, Recall@12=0.1249\n",
      "\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.0161, NDCG@10=0.0697, Recall@10=0.1071, AUC=0.5765, MAP@12=0.0387, Recall@12=0.1250\n",
      "\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.0155, NDCG@10=0.0725, Recall@10=0.1117, AUC=0.5681, MAP@12=0.0405, Recall@12=0.1301\n",
      "Best model saved at epoch 7\n",
      "\n",
      "Starting epoch 8\n",
      "No product features in this batch; skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0148, NDCG@10=0.0727, Recall@10=0.1121, AUC=0.5755, MAP@12=0.0405, Recall@12=0.1307\n",
      "Best model saved at epoch 8\n",
      "\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.0143, NDCG@10=0.0743, Recall@10=0.1142, AUC=0.5861, MAP@12=0.0416, Recall@12=0.1331\n",
      "Best model saved at epoch 9\n",
      "\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.0138, NDCG@10=0.0747, Recall@10=0.1149, AUC=0.5910, MAP@12=0.0417, Recall@12=0.1343\n",
      "Best model saved at epoch 10\n",
      "\n",
      "Starting epoch 11\n",
      "No product features in this batch; skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0133, NDCG@10=0.0756, Recall@10=0.1160, AUC=0.5889, MAP@12=0.0424, Recall@12=0.1349\n",
      "Best model saved at epoch 11\n",
      "\n",
      "Starting epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0130, NDCG@10=0.0764, Recall@10=0.1167, AUC=0.5955, MAP@12=0.0429, Recall@12=0.1355\n",
      "Best model saved at epoch 12\n",
      "\n",
      "Starting epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0132, NDCG@10=0.0768, Recall@10=0.1177, AUC=0.6022, MAP@12=0.0431, Recall@12=0.1370\n",
      "Best model saved at epoch 13\n",
      "\n",
      "Starting epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0127, NDCG@10=0.0767, Recall@10=0.1181, AUC=0.6010, MAP@12=0.0429, Recall@12=0.1376\n",
      "\n",
      "Cross-validation complete.\n",
      "Cross-validation complete.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# --------------------------\n",
    "# 10. Main Execution: Step-by-Step\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Preprocess data\n",
    "    # sampled_customers, transactions, articles = load_and_preprocess()\n",
    "    PREPROCESSED_DIR = \"/kaggle/input/preprocessed-data-7/\"\n",
    "\n",
    "    # Load dataframes\n",
    "    articles = pd.read_pickle(os.path.join(PREPROCESSED_DIR, \"articles.pkl\"))\n",
    "    customers = pd.read_pickle(os.path.join(PREPROCESSED_DIR, \"customers.pkl\"))\n",
    "    transactions = pd.read_pickle(os.path.join(PREPROCESSED_DIR, \"transactions.pkl\"))\n",
    "    \n",
    "    print(f\"Articles: {len(articles)}, Customers: {len(customers)}, Transactions: {len(transactions)}\")\n",
    "    \n",
    "    # Make this dictionary globally accessible for evaluate() and train()\n",
    "    global prod_feature_dict\n",
    "\n",
    "    with open(\"/kaggle/input/prod-feature-dict/prod_feature_dict.pkl\", \"rb\") as f:\n",
    "        prod_feature_dict = pickle.load(f)\n",
    "\n",
    "    # # Step 2: Build product feature dictionary from precomputed features\n",
    "    # prod_feature_dict = build_product_feature_dict(articles)\n",
    "    # # Define a directory to save preprocessed files\n",
    "    # PROD_FEATUREDICT_DIR = \"/kaggle/working/prod_feature_dict\"\n",
    "    # ZIP_FILE = \"/kaggle/working/prod_feature_dict.zip\"\n",
    "    # os.makedirs(PROD_FEATUREDICT_DIR, exist_ok=True)\n",
    "    \n",
    "    # # Save dataframes (articles, customers, transactions)\n",
    "    # with open(os.path.join(PROD_FEATUREDICT_DIR, \"prod_feature_dict.pkl\"), 'wb') as f:\n",
    "    #     pickle.dump(prod_feature_dict, f)\n",
    "    \n",
    "    # # ✅ Create a ZIP archive containing all preprocessed files\n",
    "    # with zipfile.ZipFile(ZIP_FILE, 'w') as zipf:\n",
    "    #     for file in os.listdir(PROD_FEATUREDICT_DIR):\n",
    "    #         zipf.write(os.path.join(PROD_FEATUREDICT_DIR, file), arcname=file)\n",
    "    \n",
    "    # print(f\"✅ MULTI completed! Saved as {ZIP_FILE}\")\n",
    "    \n",
    "    # Step 3: Create train/val/test splits\n",
    "    train_trans, val_trans, test_trans = create_splits(transactions)\n",
    "    # Step 4: Cross-validate on training subset (optional)\n",
    "    print(\"Starting cross-validation on training data...\")\n",
    "    cross_validate(articles, customers, train_trans)\n",
    "    print(\"Cross-validation complete.\")\n",
    "     # ✅ Step A: Combine train + val transactions\n",
    "    trainval_trans = pd.concat([train_trans, val_trans], ignore_index=True)\n",
    "    \n",
    "    # ✅ Step B: Rebuild graph using train + val data\n",
    "    trainval_data = build_graph(trainval_trans, articles, customers)\n",
    "    \n",
    "    # ✅ Step C: Initialize model with same architecture\n",
    "    model = MultiModalGNN(trainval_data.metadata(), customers['customer_mapped_id'].max() + 1, articles['article_mapped_id'].max() + 1).to(device)\n",
    "    #  # ✅ Step D: Load best weights from CV\n",
    "    # model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    # print(\"✅ Loaded best model from CV.\")\n",
    "    # # Step 5: Build graphs for train, validation, and test splits\n",
    "    # # train_data = build_graph(train_trans, articles, customers)\n",
    "    # # val_data   = build_graph(val_trans, articles, customers)\n",
    "    # test_data  = build_graph(test_trans, articles, customers)    \n",
    "    # # Step 6: Initialize model with metadata from train_data and incorporate user age\n",
    "    # # num_users = sampled_customers['customer_mapped_id'].max() + 1\n",
    "    # # num_products = articles['article_mapped_id'].max() + 1\n",
    "    # # model = MultiModalGNN(train_data.metadata(), num_users, num_products).to(device)\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    # # Step 7: Train model using the improved training loop (with in-batch negatives, LR scheduler, and early stopping)\n",
    "    # best_ndcg = train(model, train_data, val_data, optimizer, articles, prod_feature_dict)\n",
    "    \n",
    "    # # Step 8: Evaluate on test set\n",
    "    # ndcg, recall, auc = evaluate(model, test_data, articles)\n",
    "    # print(f\"\\nFinal Test Performance:\")\n",
    "    # print(f\"NDCG@10: {ndcg:.4f}\")\n",
    "    # print(f\"Recall@10: {recall:.4f}\")\n",
    "    # print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    # # Step 9: Save final model\n",
    "    # torch.save({\n",
    "    #     'state_dict': model.state_dict(),\n",
    "    #     'metadata': train_data.metadata(),\n",
    "    #     'config': config.__dict__\n",
    "    # }, \"final_model.pth\")\n",
    "    # print(\"Training and evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fold 1\n",
    "Epoch 14: Loss=0.0107, NDCG@10=0.0974, Recall@10=0.1554, AUC=0.5894, MAP@12=0.0578, Recall@12=0.1801\n",
    "fold 2\n",
    "Epoch 14: Loss=0.0127, NDCG@10=0.0767, Recall@10=0.1181, AUC=0.6010, MAP@12=0.0429, Recall@12=0.1376\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T18:22:07.166551Z",
     "iopub.status.busy": "2025-03-31T18:22:07.166247Z",
     "iopub.status.idle": "2025-04-01T02:04:51.140952Z",
     "shell.execute_reply": "2025-04-01T02:04:51.140129Z",
     "shell.execute_reply.started": "2025-03-31T18:22:07.166510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded best model from CV.\n",
      "\n",
      "🔁 Retraining on Train + Validation set...\n",
      "\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.0144 (No evaluation)\n",
      "\n",
      "Starting epoch 2\n",
      "Epoch 2: Loss=0.0139 (No evaluation)\n",
      "\n",
      "Starting epoch 3\n",
      "Epoch 3: Loss=0.0135 (No evaluation)\n",
      "\n",
      "Starting epoch 4\n",
      "Epoch 4: Loss=0.0132 (No evaluation)\n",
      "\n",
      "Starting epoch 5\n",
      "Epoch 5: Loss=0.0130 (No evaluation)\n",
      "\n",
      "Starting epoch 6\n",
      "Epoch 6: Loss=0.0127 (No evaluation)\n",
      "\n",
      "Starting epoch 7\n",
      "Epoch 7: Loss=0.0125 (No evaluation)\n",
      "\n",
      "Starting epoch 8\n",
      "Epoch 8: Loss=0.0123 (No evaluation)\n",
      "\n",
      "Starting epoch 9\n",
      "Epoch 9: Loss=0.0122 (No evaluation)\n",
      "\n",
      "Starting epoch 10\n",
      "Epoch 10: Loss=0.0121 (No evaluation)\n",
      "\n",
      "Starting epoch 11\n",
      "Epoch 11: Loss=0.0122 (No evaluation)\n",
      "\n",
      "Starting epoch 12\n",
      "Epoch 12: Loss=0.0120 (No evaluation)\n",
      "\n",
      "Starting epoch 13\n",
      "Epoch 13: Loss=0.0120 (No evaluation)\n",
      "\n",
      "Starting epoch 14\n",
      "Epoch 14: Loss=0.0120 (No evaluation)\n",
      "best_ndcg -1\n",
      "\n",
      "🧪 Final Evaluation on Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f65c1d2ad2a7>:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Test Performance:\n",
      "NDCG@10: 0.2743\n",
      "Recall@10: 0.4915\n",
      "AUC: 0.6716\n",
      "MAP@12: 0.1916\n",
      "Recall@12: 0.5502\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "global prod_feature_dict\n",
    "\n",
    "with open(\"/kaggle/input/prod-feature-dict/prod_feature_dict.pkl\", \"rb\") as f:\n",
    "    prod_feature_dict = pickle.load(f)\n",
    "\n",
    "PREPROCESSED_DIR = \"/kaggle/input/preprocessed-data-7/\"\n",
    "articles = pd.read_pickle(os.path.join(PREPROCESSED_DIR, \"articles.pkl\"))\n",
    "customers = pd.read_pickle(os.path.join(PREPROCESSED_DIR, \"customers.pkl\"))\n",
    "transactions = pd.read_pickle(os.path.join(PREPROCESSED_DIR, \"transactions.pkl\"))\n",
    "train_trans, val_trans, test_trans = create_splits(transactions)\n",
    "\n",
    "trainval_trans = pd.concat([train_trans, val_trans], ignore_index=True)\n",
    "\n",
    "# ✅ Step B: Rebuild graph using train + val data\n",
    "trainval_data = build_graph(trainval_trans, articles, customers)\n",
    "\n",
    "# ✅ Step C: Initialize model with same architecture\n",
    "model = MultiModalGNN(trainval_data.metadata(), customers['customer_mapped_id'].max() + 1, articles['article_mapped_id'].max() + 1).to(device)\n",
    "\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/kaggle/input/cold-start-rec-modal/other/default/1/best_model_fold1.pth\", weights_only=True)\n",
    "state_dict = checkpoint\n",
    "\n",
    "# Get the checkpoint user embeddings and current model's user embeddings\n",
    "old_user_emb = state_dict[\"user_emb.weight\"]   # Shape: [55299, 256]\n",
    "new_user_emb = model.user_emb.weight             # Shape: [88647, 256]\n",
    "\n",
    "# Check how many rows to copy\n",
    "num_overlap = old_user_emb.size(0)\n",
    "\n",
    "# Replace the first num_overlap rows of the new model's embedding with the loaded weights\n",
    "new_user_emb.data[:num_overlap] = old_user_emb\n",
    "\n",
    "# Optionally, leave the remaining rows as is (randomly initialized) or initialize them as desired\n",
    "state_dict[\"user_emb.weight\"] = new_user_emb\n",
    "\n",
    "# Now load state_dict with strict=False to avoid errors on the rest of the mismatched keys\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"✅ Loaded best model from CV.\")\n",
    "\n",
    "# ✅ Step E: Reinitialize optimizer (for retraining)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "# ✅ Step F: Retrain model on train + val (no val_data used now)\n",
    "print(\"\\n🔁 Retraining on Train + Validation set...\")\n",
    "best_ndcg = train(model, trainval_data, val_data=None, optimizer=optimizer, articles=articles, prod_feature_dict=prod_feature_dict)\n",
    "print(\"best_ndcg\", best_ndcg)\n",
    "# ✅ Step G: Evaluate on the test set\n",
    "print(\"\\n🧪 Final Evaluation on Test Set\")\n",
    "test_data  = build_graph(test_trans, articles, customers)    \n",
    "\n",
    "ndcg, recall, auc, map12, recall12 = evaluate(model, test_data, articles)\n",
    "print(f\"\\n✅ Final Test Performance:\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")\n",
    "print(f\"Recall@10: {recall:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"MAP@12: {map12:.4f}\")\n",
    "print(f\"Recall@12: {recall12:.4f}\")\n",
    "\n",
    "# ✅ Step H: Save the retrained final model\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'metadata': trainval_data.metadata(),\n",
    "    'config': config.__dict__\n",
    "}, \"final_model_retrained.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "emb_dim = 256\n",
    "fold 1\n",
    "Epoch 1: Loss=0.3129, NDCG=0.0806, Recall=0.1324, AUC=0.5680\n",
    "Epoch 2: Loss=0.0271, NDCG=0.0975, Recall=0.1651, AUC=0.5530\n",
    "Epoch 3: Loss=0.0230, NDCG=0.1106, Recall=0.1876, AUC=0.5580\n",
    "fold 2\n",
    "Epoch 1: Loss=0.3602, NDCG=0.0699, Recall=0.1122, AUC=0.5223\n",
    "Epoch 2: Loss=0.0305, NDCG=0.0835, Recall=0.1372, AUC=0.5217   \n",
    "Epoch 3: Loss=0.0231, NDCG=0.0859, Recall=0.1403, AUC=0.5170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "14"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6992705,
     "sourceId": 11199799,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6999334,
     "sourceId": 11209290,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 285403,
     "modelInstanceId": 264310,
     "sourceId": 311639,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
